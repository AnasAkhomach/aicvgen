Updated Software Requirements Specification for CV Tailoring AI Agent MVP
I. Introduction
This report serves to formally update the Software Requirements Specification (SRS) for the CV Tailoring AI Agent Minimum Viable Product (MVP). The primary objective is to integrate new functional and non-functional requirements and clarifications derived from recent user feedback and a detailed analysis of the existing prototype. This ensures the SRS remains a clear, consistent, and actionable guide for the development team, reflecting the latest product vision and technical considerations. The comprehensive analysis within this document provides the necessary background, architectural implications, and technical considerations to facilitate informed design and implementation decisions.
A. Purpose of this Report
The core purpose of this document is to formalize the evolution of the CV Tailoring AI Agent MVP's requirements. It incorporates critical refinements identified through ongoing development and user input. By meticulously detailing these updates, the report aims to provide a unified and unambiguous reference for all stakeholders. This structured approach ensures that development efforts are precisely aligned with the refined product vision and address key technical challenges, thereby minimizing ambiguity and enhancing project efficiency.
B. Overview of Key Updates
The updated SRS incorporates several significant enhancements that refine the system's capabilities and address critical operational considerations. These updates include:
* Shift to "Big 10" Skills: The system will now formalize the generation of 10 key qualifications, moving beyond the previously implied 6, to offer a more comprehensive skill representation.
* Enhanced Output Capabilities: The primary output format will transition to PDF, with capabilities for dynamic content appending and a mechanism for displaying raw Large Language Model (LLM) output for user transparency.
* User Interface Transparency: The user interface will now offer the option to display raw LLM output, providing direct visual feedback on the AI's unformatted responses.
* LLM Rate Limit Mitigation: A strategic shift to a granular, item-by-item processing strategy for professional experience roles and side projects will be implemented to bolster system reliability and improve perceived performance.
II. Summary of Major SRS Revisions
This section provides a concise, high-level overview of the most significant changes to the SRS, highlighting the previous versus updated requirements and their primary justifications. This summary serves as a quick-reference guide for stakeholders and development teams to rapidly grasp the scope and nature of the SRS updates, facilitating communication and alignment across the project.
A. Enhanced Content Generation: "Big 10" Skills
The initial design, as indicated by the extract_big_6_skills function name in the prototype 1, suggested the generation of 6 key qualifications. However, the key_qualifications_prompt_template within the prototype explicitly instructs the LLM to "Generate 10 Skills".1 The system's actual output, as demonstrated in the prototype's execution, consistently produces 10 skills.1
The updated requirement formalizes this existing behavior: the system shall generate 10 key qualifications for the "Key Qualifications" section. This change reflects an evolution in the product's understanding of optimal CV tailoring, aiming to provide a more comprehensive set of skills aligned with job descriptions. The functional capability for generating 10 skills is already present in the codebase. This situation points to a naming inconsistency within the existing codebase, where the function extract_big_6_skills does not accurately reflect its current output. Such discrepancies can lead to confusion among developers, potentially hindering future maintenance or modifications if the code's naming conventions do not align with its actual behavior. Addressing this inconsistency through a minor code cleanup (e.g., renaming the function to extract_big_10_skills) would improve clarity and reduce technical debt, ensuring the codebase precisely reflects the updated specification.
B. Advanced Output & Display Capabilities
Previously, the output format supported Markdown or LaTeX/PDF, with the final decision pending.1 There was no explicit requirement for displaying raw LLM output.
The revised requirement specifies that the final output shall be a PDF file, establishing it as the primary chosen format. The system is also required to display static CV sections as a well-formatted PDF and dynamically integrate newly generated content into this existing PDF. Furthermore, a new capability will be introduced to display the raw, unformatted LLM output directly within the Streamlit UI. This enhancement provides visual feedback to the user and aids in debugging. The justification for these changes is rooted in providing a professional, ready-to-use CV output, enhancing user transparency, and offering advanced debugging capabilities.
C. LLM Rate Limit Mitigation Strategy
The previous REQ-NONFUNC-RELIABILITY-1 stated that the system should handle transient errors gracefully.1 While the prototype included a run_batch_experience_pipeline 1, implying a batch processing approach for roles, this method presented limitations.
The updated requirement mandates that content generation for "Professional Experience" and "Side Projects" shall be performed on an individual role-by-role and project-by-project basis, respectively. This granular processing strategy is a direct response to observed LLM API rate limits, such as Groq's 30 Requests Per Minute (RPM) and 6000 Tokens Per Minute (TPM) for the deepseek-r1-distill-llama-70b model.1 By breaking down large LLM calls into smaller, distributed requests, this approach significantly reduces the system's vulnerability to hitting API rate limits. This architectural shift is not merely a performance optimization but a critical enhancement to system reliability and user experience. It transforms a potentially brittle, monolithic LLM interaction into a more resilient, interactive, and user-friendly workflow. The individual processing approach improves system resilience by isolating potential failures to single items, meaning an issue with one role's generation does not impact the entire professional experience section. It also enhances perceived performance by providing a "faster time-to-first-content" and enabling iterative feedback, allowing users to review and refine content progressively rather than waiting for a complete batch to finish.1
Table 1: Summary of Key SRS Changes


Requirement Area
	Old Requirement (SRS v1.0/1.1/1.2)
	New Requirement
	Justification/Impact
	Key Qualifications
	Generation of 6 key skills (implied by function name) 1
	Generation of 10 key skills 1
	Provides a more comprehensive skill set; formalizes existing prototype behavior.
	Output Format
	Markdown or LaTeX/PDF (decision required) 1
	PDF as primary format, Markdown optional; dynamic content appending to PDF.
	Delivers professional, ready-to-use CV; enhances transparency and debugging.
	Raw LLM Output
	Not explicitly required.
	Display raw, unformatted LLM output in UI.
	Increases transparency into AI's raw responses; aids power users and debugging.
	LLM Interaction Granularity (Experience/Projects)
	Implied batch processing for roles/projects 1
	Individual role-by-role and project-by-project processing.
	Mitigates LLM API rate limits (RPM, TPM) 1; improves perceived performance and system resilience; enables iterative user feedback.
	UI Feedback Granularity (Experience/Projects)
	Section-level review and feedback
	Individual item-level (role-by-role/project-by-project) review and feedback.
	Provides finer-grained control over dynamic content; aligns with LLM rate limit mitigation.
	III. Detailed Functional Requirements Updates
This section provides a granular breakdown of each updated functional requirement, detailing the changes, their rationale, and how they impact the system's behavior.
A. Input Handling (REQ-FUNC-INPUT-X)
The current REQ-FUNC-INPUT-1 specifies that the system shall accept base CV content as text, initially supporting Markdown.1 REQ-FUNC-PARSE-1 further details parsing this text into a StructuredCV data model.1
A new user requirement expresses the desire to "display the static section of the CV well formatted as a PDF file, and then any new generated content shall be correctly formatted and rendered and then added to the existing PDF file" [User Query]. This statement implies a need to handle existing PDF CVs as input for static sections. Therefore, REQ-FUNC-INPUT-1 is clarified and expanded: The system shall provide a mechanism for the user to input or upload their base CV content, supporting Markdown, plain text, and potentially PDF files for static content extraction and display. Concurrently, REQ-FUNC-PARSE-1 is updated: The system shall parse input CV text (Markdown or plain text) and convert it into the internal StructuredCV data model. If a PDF is provided as the base CV, the system shall be able to extract static sections for display and identify appropriate insertion points for dynamic content. The justification for this update is to enable the seamless integration of static pre-formatted CV sections with dynamically generated content, fulfilling the user's vision for the final output.
The request to display static sections "well formatted as a PDF file" and to "add to the existing PDF file" introduces a significant implicit requirement. Previously, PDF parsing was considered a future capability, outside the MVP scope. This new requirement necessitates PDF processing capabilities within the system, potentially within the Parser Agent or a new dedicated component. This is a complex undertaking for an MVP, as it moves beyond simple text parsing to potentially involve Optical Character Recognition (OCR) or direct PDF manipulation. While Python libraries like PyMuPDF, PyPDF2, and pdfrw exist for PDF text extraction and merging 2, maintaining layout integrity and precisely identifying "static sections" within an arbitrary PDF for later content insertion and reflow is a substantial technical challenge. Seamlessly integrating variable-length, AI-generated text into specific, pre-defined locations within an existing PDF, while preserving professional layout and pagination, is notoriously difficult. This is not a simple append operation but often requires deep understanding of PDF structure or complex content reflow mechanisms rarely found in simple PDF generation tools. This requirement carries a high technical risk for an MVP and could disproportionately consume development resources. A more pragmatic approach for the MVP would be for the system to always generate the entire CV as a new PDF from scratch, combining static content (parsed from user input or a template) with dynamically generated content. This approach aligns better with the capabilities of existing HTML-to-PDF converters like weasyprint 1 and ensures consistent formatting throughout the document.
B. Content Generation (REQ-FUNC-GEN-X)
This section details how the AI, specifically the Content Writer Agent, will create and modify CV content, incorporating the new requirements.
1. Key Qualifications Generation (from 6 to 10 skills)
REQ-FUNC-GEN-2 is updated: The system shall generate content for the "Key Qualifications" section, producing a list of 10 highly relevant and concise skills. This generation will utilize relevant CV data, job description requirements, and the key_qualifications_prompt.md.1 The justification for this change is to provide a more comprehensive and impactful representation of the user's core competencies, aligning with evolving best practices in CV tailoring. This update primarily impacts the prompt engineering within the ContentWriterAgent (content_writer_agent.py 1) and requires validation that the LLM reliably produces 10 distinct skills. As confirmed by prototype analysis, the system already achieves this.1 This means the change is largely a documentation alignment task, with a minor code cleanup opportunity (renaming the extract_big_6_skills function for consistency).
2. Professional Experience & Side Projects (Individual Processing)
REQ-FUNC-GEN-3 is updated: The system shall tailor or generate bullet points for each entry in the "Professional Experience" section one by one (role by role). This process shall use relevant CV experience details, job description requirements, Key Qualifications, and the resume_role_prompt.md.1 The output for each role shall be generated as a complete subsection, allowing for individual review and feedback. Similarly, REQ-FUNC-GEN-4 is updated: The system shall tailor or generate descriptions for each entry in the "Side Projects" section one by one (project by project). This process shall use relevant CV project details, job description requirements, Key Qualifications, and the side_project_prompt.md.1 The output for each project shall be generated as a complete subsection, allowing for individual review and feedback.
This granular processing directly addresses LLM API rate limits and improves perceived performance by breaking down large LLM calls into smaller, distributed requests.1 It also enhances usability by enabling iterative, item-level review. This represents a significant architectural change impacting both the Orchestrator Agent and the ContentWriterAgent. The Orchestrator (orchestrator.py) will need to update its workflow state machine to iterate through roles and projects, triggering individual LLM calls and pausing for user feedback after each item's generation. This requires more complex state management and conditional transitions. The ContentWriterAgent (content_writer_agent.py 1) will need to adapt its execute() method and internal prompt-building logic to receive instructions for one role or project at a time, build context specific to that single item, and return content for only that item. The existing _generate_all_dynamic_content function 1 will require refactoring or supplementation to support this granular approach.
C. UI & User Interaction (REQ-FUNC-UI-X)
This section details how users will interact with and provide feedback on the generated CV content, incorporating new display and control requirements.
1. Display of Raw LLM Output
A new requirement, REQ-FUNC-UI-6, is introduced: The UI shall provide a mechanism to display the raw, unformatted output received directly from the Large Language Model (LLM) for generated content, prior to any post-processing or cleaning. This serves as visual feedback and aids in debugging. The justification is to provide transparency into the AI's raw output, aiding power users in understanding the generation process and facilitating more precise feedback for regeneration. This implies that the ContentWriterAgent (content_writer_agent.py 1) must be modified to store or pass the raw LLM response (before cleaning) to the StructuredCV data model, perhaps via a new raw_llm_content field in the Item dataclass. The UI (main.py 1) would then need a new component (e.g., a collapsible st.expander or a dedicated st.text_area) to display this raw content, potentially as an opt-in "developer mode" feature to avoid overwhelming general users with unformatted text that may contain internal LLM thoughts or tags.1 This approach balances the need for transparency with maintaining an intuitive user experience for the primary audience.
2. Section-by-Section Review & Iterative Feedback (Hybrid Approach)
REQ-FUNC-UI-2 is updated: The UI shall present the generated content in a card-based structure, allowing users to focus on and review each section. Specifically, "Key Qualifications" and "Executive Summary" shall be reviewed at the section level, while "Professional Experience" and "Side Projects" shall allow for individual item-level (role-by-role/project-by-project) review and feedback. REQ-FUNC-UI-3 is updated: The UI shall allow the user to directly edit the text content within any generated section or individual item (for Professional Experience and Side Projects). User edits shall update the corresponding content in the StructuredCV state and implicitly indicate modification (e.g., via status or metadata). REQ-FUNC-UI-4 is updated: The UI shall provide interactive elements (e.g., buttons) associated with each generated section (for Key Quals, Summary) or individual generated item (for Professional Experience roles and Side Projects) to allow the user to explicitly "Accept" the content or request its "Regeneration".1
This hybrid approach provides optimal user control, balancing simplified review for cohesive sections with granular control for multi-item sections, while directly supporting the LLM rate limit mitigation strategy. The user's request for "role by role and project by project" generation and review directly conflicts with the current SRS Version 1.2, which explicitly states "section by section" and "section-level control" for UI review and feedback. A purely section-level control for Professional Experience and Side Projects would negate the benefits of the individual processing strategy (LLM rate limit mitigation, iterative feedback). Conversely, a purely item-level control for all sections might introduce unnecessary complexity for shorter, more cohesive sections like Key Qualifications and Executive Summary. Therefore, a hybrid model is the most pragmatic solution. This model introduces increased complexity to the StateManager and Orchestrator (which must manage both section-level and item-level states and transitions) and the UI (which must dynamically render controls based on section type). However, it offers the best balance of usability, control, and technical feasibility for the MVP, directly addressing the user's core needs.
D. Output Generation (REQ-FUNC-OUTPUT-X)
This section specifies the requirements for generating the final tailored CV document, focusing on the new PDF output and dynamic appending.
1. PDF Output as Primary Format
REQ-FUNC-OUTPUT-2 is updated: The system shall support generating the output in PDF format as the primary chosen format, with Markdown as an optional alternative.1 REQ-FUNC-OUTPUT-3 is updated: The system shall compile the generated content into a PDF document. This includes handling necessary formatting, layout, and potentially LaTeX escaping if a LaTeX-based approach is chosen.1 The justification is to deliver a professional, universally viewable, and print-ready CV document. The FormatterAgent (formatter_agent.py) will need to prioritize PDF generation. The prototype already utilizes weasyprint for HTML-to-PDF conversion 1, which is a viable path, implying that the internal representation for rendering should be HTML/CSS or LaTeX.
2. Dynamic PDF Appending for Static & Generated Content
A new requirement, REQ-FUNC-OUTPUT-5, is introduced: The system shall display the static sections of the CV (e.g., Education, Certifications, Languages) as a well-formatted PDF, and dynamically integrate newly generated content (Key Qualifications, Professional Experience, Side Projects, Executive Summary) into this PDF, ensuring correct formatting and rendering. The justification is to provide a seamless, integrated final CV document that combines pre-existing static content with tailored dynamic sections.
This is a highly complex requirement for the FormatterAgent. Seamlessly merging newly generated, variable-length content into an existing PDF while preserving layout, pagination, and professional aesthetics is a significant technical challenge. Simple PDF merging operations, as supported by libraries like PyMuPDF, PyPDF2, or pdfrw 2, typically append pages rather than inserting content into specific sections of existing pages and reflowing text. While LaTeX can include existing PDF pages 4, dynamically inserting and reflowing variable-length text into a pre-rendered PDF is generally beyond the scope of typical PDF generation tools. This requirement carries a high technical risk for an MVP and could easily consume disproportionate development resources for a feature that might not deliver the desired quality without extensive custom work. A more realistic and recommended approach for the MVP would be for the system to always generate the entire CV PDF from scratch, combining parsed static content (from user input or a template) with dynamically generated content. This approach ensures consistent formatting and is more aligned with the capabilities of existing HTML-to-PDF converters.
IV. Detailed Non-Functional Requirements Updates
This section outlines the quality attributes and constraints that the MVP must satisfy, reflecting the new requirements and their impact on system qualities.
A. Performance (REQ-NONFUNC-PERF-X)
REQ-NONFUNC-PERF-1 is updated: The system shall display the first generated dynamic section (e.g., Key Qualifications) within 30 seconds. Each subsequent dynamic section (or individual role/project) shall be generated and displayed for review within 15-30 seconds after user interaction, assuming standard LLM response times and no active rate limiting. The justification is to provide a responsive and interactive user experience by delivering "faster time-to-first-content" and iterative updates, directly addressing the benefits of individual processing.1 The shift to individual processing for roles and projects fundamentally redefines the perception of performance. While the total time to generate the entire CV might not drastically decrease, the time-to-first-content and perceived responsiveness will dramatically improve. This rephrasing of the performance requirement emphasizes iterative generation speed rather than just overall generation time, making the non-functional requirement more precise and aligned with the interactive workflow. This also implicitly ties into the reliability non-functional requirement, as avoiding rate limits (via individual processing) is crucial for consistently meeting these per-item performance targets.
B. Reliability (REQ-NONFUNC-RELIABILITY-X)
REQ-NONFUNC-RELIABILITY-1 is updated: The system shall handle transient errors from external services (e.g., LLM API rate limits, network issues, or content generation failures for individual items) gracefully. It shall provide informative messages to the user and offer retry mechanisms where appropriate, particularly for individual role/project generation.1 The justification is to ensure system stability and a consistent user experience, especially given the increased number of granular LLM calls and the inherent variability of LLM outputs. The individual processing strategy 1 is a direct, proactive measure to enhance system reliability. By breaking down tasks into smaller, independent LLM calls, the system becomes inherently more robust against API rate limits and individual content generation failures. This shifts the error handling from reactive (retries after hitting a limit) to preventive (structuring calls to avoid limits), and localizes the impact of any single failure. This approach enables fault isolation: if the LLM produces an unsatisfactory output or encounters an internal error for one specific role or project, the failure is contained to that single item. The user can then regenerate only that item, rather than requiring a restart or regeneration of an entire batch. This significantly improves the system's ability to recover gracefully and reduces user frustration, making the application feel more robust and dependable.
C. Usability (REQ-NONFUNC-USABILITY-X)
REQ-NONFUNC-USABILITY-2 is updated: The card-based review system shall clearly present each generated section (for Key Quals, Summary) and each individual generated role/project (for Professional Experience and Side Projects), along with the available options for feedback (Accept/Regenerate).1 A new requirement, REQ-NONFUNC-USABILITY-3, is introduced: The user interface shall provide a clear and intuitive way to access and view the raw, unformatted LLM output for generated content, without disrupting the primary review flow. This feature shall be accessible to users who desire deeper insight into the AI's processing. The justification is to provide granular control over dynamic content, enhance transparency, and cater to both general users and those desiring deeper insight into the AI's output. The introduction of raw LLM output display presents a usability consideration. While it offers valuable transparency and debugging capabilities for technical users, it can be overwhelming and confusing for general users due to the unformatted nature of LLM output, which may contain internal tags or intermediate processing information.1 To resolve this, the UI design must balance accessibility with complexity. The raw output should likely be an opt-in feature, perhaps behind a toggle or an expandable section, allowing power users to leverage the transparency without cluttering the main interface for others. This approach ensures that REQ-NONFUNC-USABILITY-1 (intuitive and easy to navigate) is maintained for all user classes.
V. Architectural and Design Considerations
This section delves into the specific architectural and component-level adjustments required to implement the updated requirements.
A. Orchestrator Agent Workflow Adjustments
The Manager Agent (orchestrator.py) is responsible for defining and traversing the workflow state machine, transitioning between major steps like GENERATE_KEY_QUALS and REVIEW_KEY_QUALS. It primarily managed state at the section level.
To support the hybrid section/item-level control and the granular processing of professional experience roles and side projects, the _workflow_state_machine() within the Orchestrator must be significantly enhanced. The workflow will introduce explicit loops for iterating through individual roles within "Professional Experience" and projects within "Side Projects." This will necessitate new states, such as GENERATE_NEXT_ROLE, REVIEW_CURRENT_ROLE, USER_ACCEPTED_ROLE, USER_REGENERATED_ROLE, and similar states for projects. Transitions will become more complex, dependent on user feedback for individual items (e.g., "If user accepts current role, move to next role; if no more roles, move to next section"). The Orchestrator will need to interact with the StateManager (state_manager.py) to update statuses and retrieve content for individual items, not just entire sections. This shift from a largely sequential, section-based workflow to a dynamic, iterative, and hybrid item/section-based flow significantly increases the complexity of the Orchestrator's state machine. It requires careful design of states, transitions, and error handling within the workflow to ensure robustness and correct behavior. The current conceptual state machine is a high-level sequence. For granular control, it needs to incorporate nested loops or sub-workflows. For example, after GENERATE_EXPERIENCE, it would enter a LOOP_THROUGH_ROLES sub-state. Within this loop, states like GENERATE_SINGLE_ROLE_BULLETS, REVIEW_SINGLE_ROLE, USER_ACCEPTS_ROLE, USER_REGENERATES_ROLE would manage the flow for each individual role. This significantly increases the number of states and transitions, requiring a more robust state machine implementation.
B. Content Writer Agent Adaptations
The ContentWriterAgent (content_writer_agent.py) is responsible for generating content for entire sections, as seen in its _generate_all_dynamic_content method.1 It loads prompts, calls the LLM, and applies cleaning logic via _clean_generated_content.1
The execute() method of the ContentWriterAgent must be adapted to receive specific instructions for generating content for a single item (e.g., a specific role's bullet points or a project's description), rather than an entire section. The prompt-building methods (e.g., _build_bullet_point_prompt_enhanced, _build_project_bullet_point_prompt 1) must be refined to create highly focused prompts that provide context relevant only to the single item being generated, ensuring conciseness and managing token consumption to mitigate TPM limits.1 The _clean_generated_content method 1 or a preceding step shall capture and store the raw, uncleaned LLM output. This raw content shall be made available in the StructuredCV data model (e.g., via a new raw_llm_content field in the Item dataclass) for UI display. Given the increased number of individual LLM calls, robust error handling, including retries and informative messages, becomes even more critical within this agent. The shift to granular generation for roles and projects means the ContentWriterAgent becomes more specialized in handling individual content items. This requires a significant refactoring of its internal logic, particularly around prompt engineering and output processing. The agent must be capable of generating highly targeted content while also managing the raw LLM output for transparency. The input signature for its execute method should be able to receive a StructuredCV object and a specific item_id (for a role or project bullet point) to generate content for, rather than just a section.
Table 2: LLM Content Generation Strategy Comparison


Strategy
	LLM Calls per Task (e.g., for 5 roles)
	Token Consumption per Call
	Rate Limit Impact (RPM)
	Rate Limit Impact (TPM)
	System Resilience
	User Experience
	Batch Processing (Current Prototype Implied)
	1 (for all 5 roles)
	High (large aggregated prompt)
	High risk of hitting limits (30 RPM, 6000 TPM for deepseek-r1-distill-llama-70b) 1
	High risk of hitting limits 1
	Low (batch failure impacts all)
	Long wait, all-or-nothing
	Individual Processing (New Requirement)
	5 (one per role)
	Low (small, focused prompt per role)
	Lower risk (requests spread out by user review time) 1
	Lower risk (smaller prompts, lower tokens per request) 1
	High (isolated failures)
	Faster time-to-first-content, iterative feedback
	VI. Conclusions & Recommendations
The analysis of the CV Tailoring AI Agent MVP's requirements reveals a clear direction towards a more robust, user-centric, and technically resilient system. The proposed updates, driven by user feedback and a deeper understanding of LLM operational constraints, represent a strategic evolution from the initial prototype.
The formalization of "Big 10" skills generation, while largely a documentation alignment, underscores the importance of maintaining consistency between product vision, technical specifications, and code implementation. The drive towards PDF as the primary output format and the desire for dynamic content appending highlight a commitment to delivering a professional, polished final product. However, the technical complexities associated with seamlessly inserting variable-length content into existing PDF structures pose a significant challenge for an MVP and require careful management of expectations.
The most substantial architectural shift lies in the adoption of an individual item-by-item processing strategy for professional experience roles and side projects. This approach directly addresses critical LLM API rate limits and profoundly enhances system reliability by enabling fault isolation. It also transforms the user experience, offering faster initial content delivery and granular, iterative feedback loops. This hybrid control model (section-level for cohesive sections, item-level for multi-entry sections) balances usability with technical feasibility, albeit introducing increased complexity in the Orchestrator and State Manager. Furthermore, the introduction of raw LLM output display, while valuable for transparency and debugging, necessitates thoughtful UI design to avoid overwhelming general users.
Based on this comprehensive analysis, the following recommendations are proposed for the continued development of the CV Tailoring AI Agent MVP:
1. Prioritize Full PDF Generation (from scratch) for MVP Output: Instead of attempting to dynamically append content to an existing PDF, focus the Formatter Agent's development on generating the entire tailored CV as a new PDF from a unified internal representation. This approach significantly reduces technical risk for the MVP while still delivering a professional PDF output. Subsequent iterations can explore more advanced PDF manipulation if justified by user need and technical feasibility.
2. Implement Granular Workflow for Dynamic Sections: Invest significant development effort into refactoring the Orchestrator and Content Writer Agents to fully support the role-by-role and project-by-project generation and review. This is crucial for mitigating LLM rate limits and delivering the desired interactive user experience.
3. Refine UI for Hybrid Control and Transparency: Design the Streamlit UI to clearly differentiate between section-level and item-level review. Implement the raw LLM output display as an opt-in feature, perhaps through a toggle or expandable section, to cater to technical users without compromising the primary user experience.
4. Address Codebase Inconsistencies: Conduct a minor refactoring effort to align function and variable names (e.g., renaming extract_big_6_skills to extract_big_10_skills) to reflect the updated "Big 10" skills requirement. This will improve code clarity and maintainability.
5. Strengthen Error Handling for Granular Calls: Enhance the Content Writer Agent's error handling mechanisms to specifically address transient LLM failures or unsatisfactory outputs at the individual item level, providing clear user feedback and retry options.
By adhering to these recommendations, the MVP can successfully deliver its core value proposition, providing a robust, responsive, and user-controlled CV tailoring experience while managing technical complexities effectively.
Works cited
1. anasakhomach-aicvgen.txt
2. How to add text to a PDF using Python? - LambdaTest Community, accessed on June 8, 2025, https://community.lambdatest.com/t/how-to-add-text-to-a-pdf-using-python/34878
3. How to Add Text and Image to a PDF Using PDF.co Web API in Python, accessed on June 8, 2025, https://pdf.co/tutorials/add-text-link-and-image-to-pdf-in-python
4. How to insert PDF into LaTeX – PDFConverters Official Website, accessed on June 8, 2025, https://www.pdfconverters.net/how-to/insert-pdf-to-latex/
5. Inserting a PDF file in LaTeX - Stack Overflow, accessed on June 8, 2025, https://stackoverflow.com/questions/2739159/inserting-a-pdf-file-in-latex