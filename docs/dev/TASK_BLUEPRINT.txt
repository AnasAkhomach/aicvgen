# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 3.1 - Implement Granular, Item-by-Item Processing Workflow in LangGraph**

This technical blueprint outlines the implementation of the core iterative workflow using LangGraph. It focuses on enabling the system to process individual CV items (like a single job role or project), present them to the user for review, and handle feedback (accept/regenerate) before moving to the next item or section. This is the foundational task for Phase 2 of the MVP development.

---

### **Overall Technical Strategy**

The core of this task is to build the state machine defined in the "Unified MVP Refactoring & Development Plan" using LangGraph. We will create a graph where nodes represent agent actions (parsing, content writing, QA) and edges control the flow. A central conditional edge, the `feedback_router`, will be implemented to handle the iterative nature of the user review process. The `ContentWriterAgent` will be refactored to operate on a single `current_item_id` provided in the state, ensuring granular processing. The Streamlit UI will be updated to trigger and display the results of this stateful, item-by-item workflow.

---

### **1. State Management Refinement (`AgentState`)**

The `AgentState` model is the single source of truth for our workflow. It must be updated to manage the granular, item-by-item processing queue.

*   **Affected Component(s):**
    *   `src/orchestration/state.py`

*   **Pydantic Model Changes:**
    The `AgentState` model in `src/orchestration/state.py` must be updated to the following definitive structure. This ensures we can track which item is being processed and what remains in the queue for the current section.

    ```python
    # src/orchestration/state.py

    from pydantic import BaseModel, Field
    from typing import List, Optional, Dict, Any

    from src.models.data_models import StructuredCV, JobDescriptionData

    class AgentState(BaseModel):
        """
        Represents the complete, centralized state of the CV generation workflow
        for LangGraph orchestration.
        """
        # Core Data Models
        structured_cv: StructuredCV
        job_description_data: JobDescriptionData

        # Workflow Control & Granular Processing
        # The key of the section currently being processed (e.g., "professional_experience")
        current_section_key: Optional[str] = None
        # A queue of item IDs (subsections) for the current section to be processed one by one.
        items_to_process_queue: List[str] = Field(default_factory=list)
        # The ID of the specific role, project, or item currently being processed by an agent.
        current_item_id: Optional[str] = None
        # Flag to indicate if this is the first pass or a user-driven regeneration.
        is_initial_generation: bool = True

        # User Feedback for Regeneration
        # Stores feedback from the UI to guide the next generation cycle.
        # e.g., {"action": "regenerate", "item_id": "...", "feedback_text": "Make it more technical."}
        user_feedback: Optional[Dict[str, Any]] = None

        # Agent Outputs & Finalization
        # Path to the final generated PDF file.
        final_output_path: Optional[str] = None
        # Accumulated error messages from the workflow.
        error_messages: List[str] = Field(default_factory=list)

        class Config:
            arbitrary_types_allowed = True

    ```

*   **Rationale for Changes:**
    *   `current_section_key`: Explicitly tracks which CV section (e.g., "Professional Experience") is active.
    *   `items_to_process_queue`: Manages the list of subsections (roles, projects) within the current section that need to be processed. This is the core of the iterative workflow.
    *   `current_item_id`: Isolates the generation process to a single item, preventing unintended modifications and enabling the granular UI controls.
    *   `is_initial_generation`: Allows agents to differentiate between the first pass and subsequent user-requested regenerations.

---

### **2. LangGraph Workflow Implementation**

This involves defining the nodes and edges of our state machine in `cv_workflow_graph.py`.

*   **Affected Component(s):**
    *   `src/orchestration/cv_workflow_graph.py`

*   **Detailed Implementation Steps:**

    1.  **Instantiate Agents:** At the top of the file, instantiate the necessary agents.

        ```python
        # src/orchestration/cv_workflow_graph.py
        from src.agents import ParserAgent, EnhancedContentWriterAgent, QualityAssuranceAgent
        from src.services.llm import get_llm_service

        llm_service = get_llm_service()
        parser_agent = ParserAgent(name="ParserAgent", description="Parses CV and JD.", llm=llm_service)
        content_writer_agent = EnhancedContentWriterAgent()
        qa_agent = QualityAssuranceAgent(name="QAAgent", description="Performs quality checks.", llm=llm_service)
        ```

    2.  **Define Node Functions:** Create a Python function for each node in the graph. These functions will wrap the agent's logic.

        ```python
        # src/orchestration/cv_workflow_graph.py

        from typing import Dict
        from .state import AgentState

        def parser_node(state: AgentState) -> Dict[str, any]:
            # This node is triggered once at the start.
            # It should parse the raw JD and CV text from the initial state.
            # It populates structured_cv and job_description_data.
            # It also populates the first items_to_process_queue (e.g., for "Key Qualifications").
            # ... implementation using parser_agent ...
            # Return a dictionary with the updated state fields.
            return {"structured_cv": updated_cv, "job_description_data": parsed_jd, "items_to_process_queue": qual_ids, "current_section_key": "key_qualifications"}

        def content_writer_node(state: AgentState) -> Dict[str, any]:
            # This is the core generative node. It will be called for each item.
            # It MUST use state.current_item_id to focus its work.
            result = content_writer_agent.run_as_node(state) # The agent logic will be updated in the next step
            return result

        def qa_generated_node(state: AgentState) -> Dict[str, any]:
            # This node checks the quality of the content just generated by the content_writer_node.
            # It can add warnings or suggestions to the item's metadata.
            result = qa_agent.run_as_node(state)
            return result

        # ... other nodes like formatter_node ...
        ```

    3.  **Define the Conditional Routing Function:** This function is the decision-making core of the iterative workflow.

        ```python
        # src/orchestration/cv_workflow_graph.py

        from src.models.data_models import ItemStatus

        # Define the sequence of sections to be processed
        WORKFLOW_SEQUENCE = ["key_qualifications", "professional_experience", "projects", "executive_summary"]

        def route_after_review(state: AgentState) -> str:
            """
            Determines the next step after the UI presents the content and gets user feedback.
            """
            feedback = state.user_feedback or {}
            action = feedback.get("action")

            if action == "regenerate":
                # User wants to regenerate the same item. Loop back to the content writer.
                return "content_writer_node"

            if action == "accept":
                if state.items_to_process_queue:
                    # More items in the current section queue. Process the next one.
                    return "process_next_item_node" # A small utility node to pop from the queue
                else:
                    # Current section is finished, find the next section.
                    try:
                        current_index = WORKFLOW_SEQUENCE.index(state.current_section_key)
                        if current_index + 1 < len(WORKFLOW_SEQUENCE):
                            # There is a next section in the sequence.
                            return "prepare_next_section_node" # Utility node to set up the next section
                        else:
                            # All sections are completed.
                            return "formatter_node"
                    except ValueError:
                        # Should not happen if current_section_key is valid
                        return END

            # Default case if no specific action is provided (e.g., initial run)
            if state.items_to_process_queue:
                return "process_next_item_node"
            else:
                return "formatter_node"

        ```

    4.  **Build the Graph:** Assemble the nodes and edges in `cv_workflow_graph.py`.

        ```python
        # src/orchestration/cv_workflow_graph.py
        from langgraph.graph import StateGraph, END

        # ... (node and routing functions from above) ...

        workflow = StateGraph(AgentState)

        # Add Nodes
        workflow.add_node("parser_node", parser_node)
        workflow.add_node("content_writer_node", content_writer_node)
        workflow.add_node("qa_generated_node", qa_generated_node)
        # Add utility nodes to manage state transitions cleanly
        workflow.add_node("process_next_item_node", lambda state: {"current_item_id": state.items_to_process_queue.pop(0)})
        workflow.add_node("prepare_next_section_node", ...) # Logic to set new section_key and populate queue
        workflow.add_node("formatter_node", ...) # The final formatting step

        # Define Edges
        workflow.set_entry_point("parser_node")
        workflow.add_edge("parser_node", "process_next_item_node") # Start processing the first section
        workflow.add_edge("process_next_item_node", "content_writer_node")
        workflow.add_edge("content_writer_node", "qa_generated_node")

        # Add the conditional edge for the iterative loop
        workflow.add_conditional_edges(
            "qa_generated_node",
            route_after_review,
            {
                "content_writer_node": "content_writer_node",
                "process_next_item_node": "process_next_item_node",
                "prepare_next_section_node": "prepare_next_section_node",
                "formatter_node": "formatter_node",
                END: END
            }
        )
        workflow.add_edge("prepare_next_section_node", "process_next_item_node")
        workflow.add_edge("formatter_node", END)

        # Compile the graph
        cv_graph_app = workflow.compile()
        ```

---

### **3. Agent Logic Modification (`EnhancedContentWriterAgent`)**

The `ContentWriterAgent` must be refactored to support generating content for a single, specific item ID from the state.

*   **Affected Component(s):**
    *   `src/agents/enhanced_content_writer.py`

*   **Agent Logic Modifications:**

    1.  **Refactor `run_as_node`:** This method is the entry point for the agent when called from LangGraph. It must now inspect the state to determine its exact task.

    ```python
    # src/agents/enhanced_content_writer.py

    from src.orchestration.state import AgentState
    from src.models.data_models import StructuredCV

    class EnhancedContentWriterAgent(...):
        # ... (existing methods) ...

        def run_as_node(self, state: AgentState) -> dict:
            """
            Executes the content generation logic as a LangGraph node.
            This method will process a single item if `current_item_id` is set in the state.
            """
            logger.info(f"ContentWriterAgent node running for item: {state.current_item_id}")

            # Defensive validation of inputs from the state
            if not state.current_item_id:
                logger.error("ContentWriterAgent called without a current_item_id in the state.")
                return {"error_messages": state.error_messages + ["ContentWriter failed: No item ID."]}

            if not state.structured_cv:
                logger.error("ContentWriterAgent called without a structured_cv in the state.")
                return {"error_messages": state.error_messages + ["ContentWriter failed: No CV data."]}

            # Deep copy the CV to avoid mutation issues in the graph state
            updated_cv = state.structured_cv.model_copy(deep=True)

            # Find the specific item (role, project, etc.) to process
            target_item, section, subsection = updated_cv.find_item_by_id(state.current_item_id)

            if not target_item:
                error_msg = f"ContentWriter failed: Item with ID {state.current_item_id} not found."
                logger.error(error_msg)
                return {"error_messages": state.error_messages + [error_msg]}

            # Build a focused prompt for this single item
            # This logic should be encapsulated in a helper method
            prompt = self._build_single_item_prompt(target_item, section, subsection, state.job_description_data, state.user_feedback)

            # Call the LLM to generate content
            # This should include robust error handling and fallbacks
            generated_content = self._generate_content_with_llm(prompt) # Simplified for blueprint

            # Update the specific item in the copied CV object
            target_item.content = generated_content
            target_item.status = ItemStatus.GENERATED
            target_item.raw_llm_output = "..." # Store the raw response

            # Return the updated CV to the state
            return {"structured_cv": updated_cv}

        def _build_single_item_prompt(self, item, section, subsection, job_data, feedback):
            # This helper method will contain the logic to create a highly specific prompt
            # for the given item, using context from its section, subsection, and the job data.
            # This is where the agent's "intelligence" is focused.
            # ...
            return "Generated prompt for " + item.id

    ```

---

### **4. Testing Considerations**

*   **Unit Tests:**
    *   Test the `route_after_review` function with various `AgentState` configurations (e.g., `action='regenerate'`, `action='accept'` with and without items left in the queue) to ensure it returns the correct next node name.
    *   Test the `EnhancedContentWriterAgent._build_single_item_prompt` method to verify it generates a correctly focused prompt.
    *   Test the `EnhancedContentWriterAgent.run_as_node` method by passing a state with a `current_item_id` and asserting that *only* the corresponding item in the returned `structured_cv` is modified.

*   **Integration Tests:**
    *   Create a minimal LangGraph with `process_next_item_node` -> `content_writer_node` -> `qa_generated_node` and invoke it with a state containing a queue of two items. Verify that the `content_writer_node` is called twice.

*   **E2E Testing:**
    *   Write a Streamlit test script that simulates a user:
        1.  Generating a full CV.
        2.  Clicking "Regenerate" on the second experience item.
        3.  Asserting that the second item's text changes in the UI.
        4.  Asserting that the first and third items' text remains unchanged.

---

### **5. Potential Challenges & Critical Considerations**

*   **State Management in Streamlit:** Streamlit's execution model re-runs the script on every interaction. The `AgentState` object must be reliably stored in `st.session_state` and passed to/from the LangGraph orchestrator on each interaction to maintain the workflow's state.
*   **Preventing State Mutation:** LangGraph nodes should not mutate the input state directly. They must return a dictionary of the fields they have changed. The blueprint's example of creating `updated_cv = state.structured_cv.model_copy(deep=True)` is the correct pattern to follow.
*   **Idempotency:** Agent actions, especially content generation, should be as idempotent as possible. Re-running the `content_writer_node` on the same item with the same state should ideally produce a similar (if not identical) result.

---

### **Critical Gaps & Questions**

*   **Conceptual Clarification on "Pausing" the Graph:** The term "pause" in the workflow description can be misleading. LangGraph does not pause execution. The `render_ui_node` conceptually represents the end of one invocation. The Streamlit app renders the new state, and a subsequent user action (like clicking "Accept") triggers a *new* invocation of the graph with an updated state. This distinction is crucial for the implementation team.
*   **User Feedback Data Structure:** The exact structure of the `user_feedback` dictionary needs to be standardized between the UI and the orchestrator. **Proposal:** A simple, flat dictionary: `{"action": "regenerate" | "accept", "item_id": "...", "feedback_text": "..."}`. This should be defined in `src/models/data_models.py`.
*   **Section Sequencing Logic:** The logic for moving from one section to the next (e.g., from "Key Qualifications" to "Professional Experience") needs to be explicitly defined. **Proposal:** For the MVP, implement a hardcoded sequence in the `route_after_review` function or a `prepare_next_section_node`. A `WORKFLOW_SEQUENCE` list as shown in the example is a good approach.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 3.1 - Implement Granular, Item-by-Item Processing Workflow in LangGraph**

This technical blueprint outlines the implementation of the core iterative workflow using LangGraph. It focuses on enabling the system to process individual CV items (like a single job role or project), present them to the user for review, and handle feedback (accept/regenerate) before moving to the next item or section. This is the foundational task for Phase 2 of the MVP development.

---

### **Overall Technical Strategy**

The core of this task is to build the state machine defined in the "Unified MVP Refactoring & Development Plan" using LangGraph. We will create a graph where nodes represent agent actions (parsing, content writing, QA) and edges control the flow. A central conditional edge, `route_after_review`, will be implemented to handle the iterative nature of the user review process.

A critical concept for implementation is understanding the interaction model between Streamlit and LangGraph. The graph does **not** "pause" awaiting user input. Instead, each user action in the UI (e.g., clicking "Accept") triggers a *complete, new invocation* of the LangGraph application with an updated state object. The UI is simply a view of the current state, and its actions create the input for the next state transition in the graph.

The `ContentWriterAgent` will be refactored to operate on a single `current_item_id` provided in the `AgentState`, ensuring granular processing. The Streamlit UI will be updated to trigger and display the results of this stateful, item-by-item workflow.

---

### **1. State and Feedback Model Definition**

The `AgentState` is the single source of truth. We will refine it and introduce a dedicated model for user feedback to ensure a clear contract between the UI and the backend.

*   **Affected Component(s):**
    *   `src/orchestration/state.py`
    *   `src/models/data_models.py` (new model)

*   **Pydantic Model Changes:**

    1.  **Define `UserFeedback` model** in `src/models/data_models.py`. This standardizes the data coming from the UI.

        ```python
        # src/models/data_models.py

        class UserAction(str, Enum):
            ACCEPT = "accept"
            REGENERATE = "regenerate"

        class UserFeedback(BaseModel):
            action: UserAction
            item_id: str
            feedback_text: Optional[str] = None
        ```

    2.  **Update `AgentState` model** in `src/orchestration/state.py` to its definitive structure.

        ```python
        # src/orchestration/state.py

        from pydantic import BaseModel, Field
        from typing import List, Optional, Dict, Any
        from src.models.data_models import StructuredCV, JobDescriptionData, UserFeedback

        class AgentState(BaseModel):
            # Core Data Models
            structured_cv: StructuredCV
            job_description_data: JobDescriptionData

            # Workflow Control for Granular Processing
            current_section_key: Optional[str] = None
            items_to_process_queue: List[str] = Field(default_factory=list)
            current_item_id: Optional[str] = None
            is_initial_generation: bool = True

            # User Feedback for Regeneration
            user_feedback: Optional[UserFeedback] = None

            # Final Output & Error Handling
            final_output_path: Optional[str] = None
            error_messages: List[str] = Field(default_factory=list)

            class Config:
                arbitrary_types_allowed = True
        ```

*   **Rationale for Changes:**
    *   The new `UserFeedback` model creates a strict API contract, preventing ambiguity.
    *   The refined `AgentState` now perfectly models the state needed for an item-by-item, section-by-section workflow, controlled by a processing queue and the current item pointer.

---

### **2. LangGraph Workflow Implementation**

This involves defining the nodes and edges of our state machine in `cv_workflow_graph.py`.

*   **Affected Component(s):**
    *   `src/orchestration/cv_workflow_graph.py`

*   **Detailed Implementation Steps:**

    1.  **Define Workflow Sequence:** At the top of the file, define the hardcoded sequence of sections for the MVP.

        ```python
        # src/orchestration/cv_workflow_graph.py
        WORKFLOW_SEQUENCE = ["key_qualifications", "professional_experience", "project_experience", "executive_summary"]
        ```

    2.  **Define Node Functions:** Implement Python functions for each node. Each function must accept `state: AgentState` and return a dictionary of the state fields it has modified.

        *   **CRITICAL NOTE:** Nodes **must not** mutate the input `state` object. Always work on a copy (e.g., `updated_cv = state.structured_cv.model_copy(deep=True)`) and return only the changed fields.

        ```python
        # src/orchestration/cv_workflow_graph.py

        def parser_node(state: AgentState) -> Dict:
            # ... (logic to parse JD and CV) ...
            # This node populates the first section's queue
            first_section_key = WORKFLOW_SEQUENCE[0]
            first_section = updated_cv.get_section_by_name(first_section_key)
            item_queue = [item.id for item in first_section.items] if first_section else []
            return {
                "structured_cv": updated_cv,
                "job_description_data": parsed_jd,
                "items_to_process_queue": item_queue,
                "current_section_key": first_section_key
            }

        def content_writer_node(state: AgentState) -> Dict:
            # Invokes the refactored agent, which uses state.current_item_id
            return content_writer_agent.run_as_node(state)

        def qa_generated_node(state: AgentState) -> Dict:
            # Runs QA on the newly generated content
            return qa_agent.run_as_node(state)

        def process_next_item_node(state: AgentState) -> Dict:
            # Pops the next item from the queue and sets it as current.
            if not state.items_to_process_queue:
                return {} # Should be routed to prepare_next_section
            next_item_id = state.items_to_process_queue.pop(0)
            return {"current_item_id": next_item_id, "items_to_process_queue": state.items_to_process_queue}

        def prepare_next_section_node(state: AgentState) -> Dict:
            # Finds the next section in WORKFLOW_SEQUENCE and populates the queue
            current_index = WORKFLOW_SEQUENCE.index(state.current_section_key)
            next_section_key = WORKFLOW_SEQUENCE[current_index + 1]
            next_section = state.structured_cv.get_section_by_name(next_section_key)
            item_queue = [item.id for item in next_section.items] if next_section else [] # Or subsections
            return {"current_section_key": next_section_key, "items_to_process_queue": item_queue}

        def formatter_node(state: AgentState) -> Dict:
             # ... (logic to generate PDF) ...
             return {"final_output_path": path_to_pdf}
        ```

    3.  **Define Conditional Routing Logic:** This function directs the workflow based on user feedback and queue status.

        ```python
        # src/orchestration/cv_workflow_graph.py

        def route_after_review(state: AgentState) -> str:
            feedback = state.user_feedback
            if feedback and feedback.action == "regenerate":
                return "content_writer_node"  # Loop back for regeneration

            if state.items_to_process_queue:
                return "process_next_item_node"  # More items in this section
            else:
                try:
                    current_index = WORKFLOW_SEQUENCE.index(state.current_section_key)
                    if current_index + 1 < len(WORKFLOW_SEQUENCE):
                        return "prepare_next_section_node"  # Move to next section
                    else:
                        return "formatter_node"  # All sections done
                except (ValueError, IndexError):
                    return END # Safety exit
        ```

    4.  **Assemble and Compile the Graph:**

        ```python
        # src/orchestration/cv_workflow_graph.py

        workflow = StateGraph(AgentState)
        # Add all nodes...
        workflow.add_node("parser_node", parser_node)
        workflow.add_node("content_writer_node", content_writer_node)
        # ...etc...

        # Define Edges
        workflow.set_entry_point("parser_node")
        workflow.add_edge("parser_node", "process_next_item_node")
        workflow.add_edge("process_next_item_node", "content_writer_node")
        workflow.add_edge("prepare_next_section_node", "process_next_item_node")
        workflow.add_edge("content_writer_node", "qa_generated_node")
        workflow.add_edge("formatter_node", END)

        workflow.add_conditional_edges(
            "qa_generated_node",
            route_after_review,
            {
                "content_writer_node": "content_writer_node",
                "process_next_item_node": "process_next_item_node",
                "prepare_next_section_node": "prepare_next_section_node",
                "formatter_node": "formatter_node",
                END: END
            }
        )

        cv_graph_app = workflow.compile()
        ```

---

### **3. Agent Logic Modification (`EnhancedContentWriterAgent`)**

The `ContentWriterAgent` must be refactored to support generating content for a single, specific item ID from the state.

*   **Affected Component(s):**
    *   `src/agents/enhanced_content_writer.py`

*   **Agent Logic Modifications:**
    *   Refactor the `run_as_node` method to be the primary entry point from LangGraph. It must read `state.current_item_id` and perform its generation logic *only* on that specific item.
    *   Create a helper method `_build_single_item_prompt` that constructs a highly focused prompt using the item's context (section, subsection, job data, user feedback) to guide the LLM.
    *   The method must return a dictionary containing the updated `structured_cv` object, like `{"structured_cv": updated_cv}`.

---

### **4. Streamlit UI Integration**

The UI must be updated to manage the stateful interaction with the LangGraph orchestrator.

*   **Affected Component(s):**
    *   `src/core/main.py` (or the main Streamlit UI file)

*   **Detailed Implementation Steps:**

    1.  **Initialize State:** At the top of the main UI function, initialize the state.

        ```python
        # src/core/main.py
        if 'agent_state' not in st.session_state:
            st.session_state.agent_state = None
        ```

    2.  **"Generate" Button Logic:** When the user clicks "Generate Tailored CV":
        *   Create the initial `AgentState` with raw JD and CV text.
        *   Invoke the graph: `new_state = cv_graph_app.invoke(initial_state)`.
        *   Save the result: `st.session_state.agent_state = AgentState.model_validate(new_state)`.
        *   Call `st.rerun()` to render the first piece of generated content.

    3.  **"Accept" / "Regenerate" Button Logic:** These buttons will be part of a callback function.
        *   **CRITICAL:** The callback must not invoke the graph directly. Its only job is to update `st.session_state.agent_state`.

        ```python
        # src/core/main.py
        def handle_user_action(action: str, item_id: str, feedback_text: str = ""):
            # This function is called by the on_click parameter of st.button
            if st.session_state.agent_state:
                st.session_state.agent_state.user_feedback = UserFeedback(
                    action=UserAction(action),
                    item_id=item_id,
                    feedback_text=feedback_text
                )

        # In the UI rendering loop:
        st.button("Accept", on_click=handle_user_action, args=("accept", item.id))
        st.button("Regenerate", on_click=handle_user_action, args=("regenerate", item.id))
        ```

    4.  **Main UI Loop:** After the callbacks have set the user feedback, the script continues. If `user_feedback` is present in the state, invoke the graph again with the updated state.

        ```python
        # src/core/main.py
        if st.session_state.agent_state and st.session_state.agent_state.user_feedback:
            current_state = st.session_state.agent_state
            # Clear feedback to prevent re-triggering
            current_state.user_feedback = None
            # Invoke the graph with the current state to continue the workflow
            new_state_dict = cv_graph_app.invoke(current_state.model_dump())
            st.session_state.agent_state = AgentState.model_validate(new_state_dict)
            st.rerun() # Rerender the UI with the latest state
        ```

---

### **5. Testing Considerations**

*   **Unit Tests:**
    *   Test `route_after_review` with various `AgentState` configurations to ensure correct routing.
    *   Test `EnhancedContentWriterAgent.run_as_node` to assert that *only* the `current_item_id` is modified in the returned `structured_cv`.
*   **Integration Tests:**
    *   Test the `parser_node` -> `process_next_item_node` -> `content_writer_node` sequence to ensure the queue is correctly initialized and the first item is processed.
*   **E2E Testing:**
    *   Create a test to simulate a user clicking "Regenerate" twice on the same item, then "Accept", and verify the state transitions correctly at each step.
    *   Test for idempotency: Simulate a double-click on "Accept" and ensure the workflow doesn't advance twice.

---

### **6. Potential Challenges & Critical Considerations**

*   **State Immutability:** This is the most critical point. The engineer must be vigilant in not mutating the state object passed into a node. Always use `model_copy(deep=True)` for complex objects like `StructuredCV` before modification.
*   **Workflow Logic Complexity:** The `route_after_review` function can become complex. It must handle all possible end-of-queue and end-of-workflow scenarios gracefully.
*   **UI State Synchronization:** Ensure that any user interaction that triggers a workflow step correctly updates `st.session_state` *before* the `st.rerun()` is called. Using `on_click` callbacks is the standard, reliable pattern for this.

---

### **Critical Gaps & Questions**

*   **Resolved:** The previous blueprint's questions have been addressed by the audit and incorporated into this refined plan. The state management model, UI interaction flow, and sequencing logic are now explicitly defined, providing a clear path for implementation. No further critical gaps are identified for this specific task.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 3.2 - Implement "Big 10" Skills Generation**

This blueprint details the implementation of the "Big 10" skills generation feature. The system will analyze the job description and the user's base CV to identify and present the top 10 most relevant skills or qualifications. This feature directly addresses a core value proposition of the MVP: automatically highlighting a candidate's key strengths in relation to a specific job.

---

### **Overall Technical Strategy**

The implementation will be centered within the `EnhancedContentWriterAgent`. A new public method, `generate_big_10_skills`, will be created to encapsulate this logic. This method will follow a two-step LLM chain:

1.  **Generation:** Use the `key_qualifications_prompt.md` to generate a raw, and potentially verbose, list of key qualifications based on the job description and the user's existing skills (extracted from their CV).
2.  **Cleaning & Extraction:** Use the `clean_big_6_prompt.md` (conceptually, the "skills cleaning prompt") to parse the raw LLM output from step 1 into a clean, newline-separated list of exactly 10 skills.

The resulting clean list of skills and the initial raw LLM output will be stored in new, dedicated fields within the `StructuredCV` Pydantic model. This entire process will be integrated into the LangGraph workflow as a new, dedicated node that runs immediately after the initial parsing stage.

---

### **1. Pydantic Model Changes**

To store the generated skills and maintain transparency, the `StructuredCV` model must be extended.

*   **Affected Component(s):**
    *   `src/models/data_models.py`

*   **Pydantic Model Changes:**
    Modify the `StructuredCV` model to include fields for the processed skills list and the raw LLM output.

    ```python
    # src/models/data_models.py

    class StructuredCV(BaseModel):
        """The main data model representing the entire CV structure."""
        id: UUID = Field(default_factory=uuid4)
        sections: List[Section] = Field(default_factory=list)
        metadata: Dict[str, Any] = Field(default_factory=dict)

        # New fields for "Big 10" Skills
        big_10_skills: List[str] = Field(
            default_factory=list,
            description="A clean list of the top 10 generated key qualifications."
        )
        big_10_skills_raw_output: Optional[str] = Field(
            None,
            description="The raw, uncleaned output from the LLM for the key qualifications generation."
        )

        # ... existing methods (to_dict, from_dict, etc.) ...
    ```

*   **Rationale for Changes:**
    *   `big_10_skills`: Provides a structured, clean list of strings that can be easily rendered in the UI and used by other agents.
    *   `big_10_skills_raw_output`: Fulfills the requirement (Task 3.4) to store raw LLM output for user transparency and debugging purposes.

---

### **2. LLM Prompt Usage**

This feature will utilize two existing prompts in a chained sequence.

*   **Affected Component(s):**
    *   `data/prompts/key_qualifications_prompt.md`
    *   `data/prompts/clean_big_6_prompt.md`

*   **LLM Prompt Definitions:**

    1.  **Generation Prompt (`key_qualifications_prompt.md`):** This prompt will be used to generate the initial set of skills.

        ```markdown
        [System Instruction]
        You are an expert CV and LinkedIn profile skill generator. Your goal is to analyze the provided job description and generate a list of the 10 most relevant and impactful skills for a candidate's "Key Qualifications" section.

        [Instructions for Skill Generation]
        1. **Analyze Job Description:** Carefully read the main job description below. Pay close attention to sections like "Required Qualifications," "Responsibilities," "Ideal Candidate," and "Skills." Prioritize skills mentioned frequently and those listed as essential requirements.
        2. **Identify Key Skills:** Extract the 10 most critical core skills and competencies sought by the employer.
        3. **Synthesize and Condense:** Rephrase the skills to be concise and impactful. Aim for action-oriented phrases that highlight capabilities. Each skill phrase should be **no longer than 30 characters**.
        4. **Format Output:** Return the 10 skills as a simple, plain text, newline-separated list. Do not use bullet points, numbers, or any other formatting.
        5. **Generate the "Big 10" Skills:** Create exactly 10 skills that are:
            * Highly relevant to the job description.
            * Concise (under 30 characters).
            * Action-oriented and impactful.
            * Directly aligned with employer requirements.

        [Job Description]
        {{main_job_description_raw}}

        [Additional Context & Talents to Consider]
        {{my_talents}}
        ```
        *   **Context Variables:**
            *   `{{main_job_description_raw}}`: The full text of the job description.
            *   `{{my_talents}}`: A string containing a summary of the user's existing skills, extracted from their parsed CV.

    2.  **Cleaning Prompt (`clean_big_6_prompt.md`):** This prompt will parse the potentially messy output of the first call. *Note: The filename "Big 6" is a legacy name; the prompt logic is suitable for extracting any number of skills.*

        ```markdown
        You are a highly precise resume assistant. Your task is to carefully extract a list of **meaningful skill phrases** from a verbose and messy output generated by another AI.  You are NOT supposed to simplify or shorten the skills drastically; instead, you should **identify and extract the complete skill phrases as they are listed in the final output** of the messy input.

        You MUST ignore all reasoning, explanations, tags (like `<think>`), introductory or concluding sentences, and any other extraneous text.  Your output should contain **ONLY the originally generated skill phrases**, cleaned and formatted as a list.

        Each **extracted skill phrase** MUST adhere to these rules:
        - Be under 27 characters.
        - Represent a clear and **meaningful professional skill or expertise**.
        - Be extracted **directly from the final list of skills** in the messy input. Do not re-write, summarize, or generate new skills.

        Your output MUST be formatted as a simple Markdown bulleted list, with each skill phrase on a new line, starting with a hyphen and a space...

        Here is the messy input you need to clean:
        """
        {raw_response}
        """
        ```
        *   **Context Variables:**
            *   `{raw_response}`: The full, raw output from the `key_qualifications_prompt.md` LLM call.

---

### **3. Agent Logic Modifications**

A new method will be added to `EnhancedContentWriterAgent` to handle this specific task.

*   **Affected Component(s):**
    *   `src/agents/enhanced_content_writer.py`

*   **Agent Logic Modifications:**

    Implement the `generate_big_10_skills` method. This method is **not** part of the `run_as_node` item-by-item loop but is a standalone utility function to be called by a specific graph node.

    ```python
    # src/agents/enhanced_content_writer.py

    class EnhancedContentWriterAgent(EnhancedAgentBase):
        # ... existing __init__ and other methods ...

        def generate_big_10_skills(self, job_description: str, my_talents: str = "") -> Dict[str, Any]:
            """
            Generates the "Big 10" skills using a two-step LLM chain (generate then clean).
            Returns a dictionary with the clean skills list and the raw LLM output.
            """
            try:
                # === Step 1: Generate Raw Skills ===
                generation_template = self._load_prompt_template("key_qualifications_prompt")
                generation_prompt = generation_template.format(
                    main_job_description_raw=job_description,
                    my_talents=my_talents or "Professional with diverse technical and analytical skills"
                )

                logger.info("Generating raw 'Big 10' skills...")
                # Note: Assuming self.llm_service.generate_content is awaitable in a real async context
                # For this blueprint, we'll show a synchronous-style call for clarity.
                raw_response = self.llm_service.generate_content(prompt=generation_prompt)
                raw_skills_output = raw_response.content if raw_response.success else ""

                if not raw_skills_output.strip():
                    raise ValueError("LLM returned an empty response for skills generation.")

                # === Step 2: Clean the Raw Output ===
                cleaning_template = self._load_prompt_template("clean_big_6_prompt")
                cleaning_prompt = cleaning_template.format(raw_response=raw_skills_output)

                logger.info("Cleaning generated skills...")
                cleaned_response = self.llm_service.generate_content(prompt=cleaning_prompt)
                cleaned_skills_text = cleaned_response.content if cleaned_response.success else ""

                # === Step 3: Parse and Finalize ===
                # Parse the cleaned text into a list of strings
                skills_list = [line.strip().lstrip('- ').strip() for line in cleaned_skills_text.split('\n') if line.strip()]

                # Ensure exactly 10 skills, truncating if necessary
                final_skills = skills_list[:10]

                logger.info(f"Successfully generated and cleaned {len(final_skills)} skills.")

                return {
                    "skills": final_skills,
                    "raw_llm_output": raw_skills_output,
                    "success": True,
                    "error": None
                }

            except Exception as e:
                logger.error(f"Error generating Big 10 skills: {e}", exc_info=True)
                return {
                    "skills": [],
                    "raw_llm_output": "",
                    "success": False,
                    "error": str(e)
                }
    ```

---

### **4. LangGraph Workflow Integration**

A new node will be added to the graph to orchestrate the "Big 10" skills generation.

*   **Affected Component(s):**
    *   `src/orchestration/cv_workflow_graph.py`

*   **Orchestrator/Workflow Changes:**

    1.  **Define `generate_skills_node`:** Create a new node function that calls the agent method.

        ```python
        # src/orchestration/cv_workflow_graph.py

        def generate_skills_node(state: AgentState) -> Dict:
            """Generates the 'Big 10' skills and updates the CV state."""
            logger.info("Executing generate_skills_node")

            # Extract skills from user's original CV to provide as context
            my_talents = ", ".join(state.structured_cv.get_section_by_name("Key Qualifications").items) if state.structured_cv.get_section_by_name("Key Qualifications") else ""

            result = content_writer_agent.generate_big_10_skills(
                job_description=state.job_description_data.raw_text,
                my_talents=my_talents
            )

            if result["success"]:
                updated_cv = state.structured_cv.model_copy(deep=True)
                updated_cv.big_10_skills = result["skills"]
                updated_cv.big_10_skills_raw_output = result["raw_llm_output"]

                # Also populate the Key Qualifications section with these skills as Items
                qual_section = updated_cv.get_section_by_name("Key Qualifications")
                if qual_section:
                    qual_section.items = [Item(content=skill, status=ItemStatus.GENERATED, item_type=ItemType.KEY_QUALIFICATION) for skill in result["skills"]]

                return {"structured_cv": updated_cv}
            else:
                return {"error_messages": state.error_messages + [f"Skills generation failed: {result['error']}"]}
        ```

    2.  **Update Graph Edges:** Insert the new node into the workflow sequence.

        ```python
        # In build_cv_workflow_graph() function:

        # ... (add other nodes) ...
        workflow.add_node("generate_skills_node", generate_skills_node)

        # Update edges
        workflow.set_entry_point("parser_node")
        workflow.add_edge("parser_node", "generate_skills_node") # <-- New Edge
        workflow.add_edge("generate_skills_node", "process_next_item_node") # <-- Updated Edge
        # ... (rest of the graph definition) ...
        ```

---

### **5. UI Integration**

The Streamlit UI needs to be updated to display the generated skills.

*   **Affected Component(s):**
    *   `src/core/main.py`

*   **UI Changes:**
    *   In the UI rendering logic, locate the "Key Qualifications" section within the `st.session_state.agent_state.structured_cv`.
    *   Iterate through the `items` list of that section and display each `Item.content` as a skill.
    *   Since these items are generated in one go, they may not need individual "Regenerate" buttons initially, but the section as a whole can have a "Regenerate Key Qualifications" button.

---

### **6. Testing Considerations**

*   **Unit Tests:**
    *   Test `EnhancedContentWriterAgent.generate_big_10_skills` by mocking the two LLM calls. Verify that it correctly handles a clean response, a messy response, and an error response.
    *   Assert that the method returns a dictionary with the correct keys (`skills`, `raw_llm_output`, `success`).
    *   Test the parsing logic to ensure it produces a list of strings, truncated to 10 if necessary.
*   **Integration Tests:**
    *   Test the `parser_node` -> `generate_skills_node` graph sequence. Verify that the `AgentState` passed out of `generate_skills_node` contains a `structured_cv` with the `big_10_skills` and `big_10_skills_raw_output` fields populated.
*   **E2E Testing:**
    *   Run a full workflow and assert that the "Key Qualifications" section in the UI displays a list of 10 skills after the initial processing step.

---

### **Critical Gaps & Questions**

*   No critical gaps are identified for this task. The plan is self-contained and builds directly on the work from Phase 1 and Task 3.1.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 3.4 - Implement Raw LLM Output Display**

This blueprint details the implementation for storing and displaying the raw, unedited output from the Gemini LLM for each generated content item. This feature is critical for user transparency, debugging, and providing context for regeneration feedback, directly fulfilling requirement `REQ-FUNC-UI-6`.

---

### **Overall Technical Strategy**

The strategy is to capture the raw text response from every generative LLM call and persist it alongside the final, processed content. This will be achieved by modifying the data contract of our core `Item` model to include a field for this raw output. The `LLMService` will be updated to return a more structured response object containing both the final content and the raw text. The `EnhancedContentWriterAgent` will then be responsible for populating the new field in the `Item` model. Finally, the Streamlit UI will be enhanced with an expander (`st.expander`) to conditionally display this raw data to the user on demand.

---

### **1. Pydantic Model Changes**

The `Item` model must be updated to include a field for storing the raw LLM output.

*   **Affected Component(s):**
    *   `src/models/data_models.py`

*   **Pydantic Model Changes:**
    Add a new optional string field, `raw_llm_output`, to the `Item` Pydantic model.

    ```python
    # src/models/data_models.py

    class Item(BaseModel):
        """A granular piece of content within the CV (e.g., a bullet point)."""
        id: UUID = Field(default_factory=uuid4)
        content: str
        status: ItemStatus = ItemStatus.INITIAL
        item_type: ItemType = ItemType.BULLET_POINT

        # New Field for Raw LLM Output
        raw_llm_output: Optional[str] = Field(
            None,
            description="The raw, uncleaned output from the LLM for this specific item."
        )

        confidence_score: Optional[float] = None
        metadata: Dict[str, Any] = Field(default_factory=dict)
        user_feedback: Optional[str] = None

        # ... existing methods ...
    ```

*   **Rationale for Changes:**
    *   Storing `raw_llm_output` directly on the `Item` ensures that the raw data is tightly coupled with the generated content it produced. This simplifies data retrieval for the UI and maintains a clear audit trail for each piece of content.

---

### **2. Service Layer Modification (`LLMService`)**

The `LLMService` must be refactored to return a structured object instead of just a raw string. This allows it to pass both the processed content and the raw response back to the calling agent.

*   **Affected Component(s):**
    *   `src/services/llm.py`

*   **LLM Service Changes:**

    1.  **Define `LLMResponse` Dataclass:** Create a new dataclass or Pydantic model at the top of `llm.py` to structure the output.

        ```python
        # src/services/llm.py
        from dataclasses import dataclass, field
        # ... other imports ...

        @dataclass
        class LLMResponse:
            """Structured response from LLM calls."""
            content: str
            raw_response_text: str
            tokens_used: int = 0
            processing_time: float = 0.0
            model_used: str = ""
            success: bool = True
            error_message: Optional[str] = None
            metadata: Dict[str, Any] = field(default_factory=dict)
        ```

    2.  **Update `generate_content` Return Type:** Modify the `generate_content` method in `EnhancedLLMService` to return an `LLMResponse` object.

        ```python
        # src/services/llm.py

        class EnhancedLLMService:
            # ...
            async def generate_content(self, ...) -> LLMResponse:
                # ... (existing logic for retries and API calls) ...

                # Inside the successful response block:
                if response.success:
                    # ...
                    llm_response = LLMResponse(
                        content=processed_content, # The cleaned, final content
                        raw_response_text=raw_llm_text, # The original, unedited text from the API
                        # ... other fields ...
                    )
                    return llm_response

                # Inside the error/fallback block:
                return LLMResponse(
                    content=fallback_content,
                    raw_response_text=f"ERROR: {str(e)}",
                    success=False,
                    error_message=str(e),
                    # ... other fields ...
                )
        ```

*   **Rationale for Changes:**
    *   Returning a structured `LLMResponse` object makes the service's output explicit and extensible. It cleanly separates the final, usable content from the raw diagnostic data without resorting to returning complex tuples.

---

### **3. Agent Logic Modification (`EnhancedContentWriterAgent`)**

The `ContentWriterAgent` must be updated to handle the new `LLMResponse` object and populate the `raw_llm_output` field on the `Item` model.

*   **Affected Component(s):**
    *   `src/agents/enhanced_content_writer.py`

*   **Agent Logic Modifications:**
    *   In the `run_as_node` method (or its helpers), when the agent calls `self.llm_service.generate_content`, it will now receive an `LLMResponse` object.
    *   The agent must be updated to use `response.content` for the item's main content and `response.raw_response_text` for the new field.

    ```python
    # src/agents/enhanced_content_writer.py

    class EnhancedContentWriterAgent(EnhancedAgentBase):
        # ...

        def run_as_node(self, state: AgentState) -> dict:
            # ... (find target_item logic) ...

            # Build the prompt for the single item
            prompt = self._build_single_item_prompt(...)

            # Call the LLM service
            llm_response = self.llm_service.generate_content(prompt) # Simplified for blueprint

            # Update the specific item in the copied CV object
            if llm_response.success:
                target_item.content = llm_response.content
                target_item.raw_llm_output = llm_response.raw_response_text # <-- Store the raw output
                target_item.status = ItemStatus.GENERATED
            else:
                target_item.content = "Error: Failed to generate content."
                target_item.raw_llm_output = llm_response.error_message
                target_item.status = ItemStatus.GENERATION_FAILED

            # ... (return updated CV state) ...
            return {"structured_cv": updated_cv}
    ```

---

### **4. UI Implementation (`Streamlit`)**

The Streamlit UI needs a component to display the stored raw output. An expander is the ideal choice as it keeps the interface clean by default.

*   **Affected Component(s):**
    *   `src/core/main.py` (or wherever the item rendering logic resides)

*   **UI Changes:**

    1.  **Modify the Item Display Logic:** In the function responsible for rendering each CV item, add a conditional block to check for the existence of `raw_llm_output`.

    ```python
    # src/core/main.py

    def display_item(item: dict, ...): # Assuming item is a dict representation of the Item model
        # ... (existing code for displaying item content and buttons) ...

        # Add the expander for Raw LLM Output
        raw_output = item.get("raw_llm_output")
        if raw_output:
            with st.expander("🔍 View Raw LLM Output", expanded=False):
                st.code(raw_output, language="text")

    ```

*   **Rationale for Changes:**
    *   Using `st.expander` provides the transparency feature without cluttering the primary UI. Users can inspect the raw data only when they need to.
    *   Displaying the text within `st.code` provides a monospaced, easy-to-read block that respects formatting like newlines and whitespace from the original LLM response.

---

### **5. Testing Considerations**

*   **Unit Tests:**
    *   Write a unit test for `EnhancedContentWriterAgent.run_as_node` that mocks the `LLMService` to return a specific `LLMResponse`. Assert that the `raw_llm_output` field of the modified `Item` in the returned `StructuredCV` matches the `raw_response_text` from the mocked response.
    *   Write a unit test for the `display_item` function in the Streamlit UI. Pass it a mock item dictionary with and without the `raw_llm_output` key and assert that `st.expander` is called only when the key is present.
*   **E2E Testing:**
    *   Create an end-to-end test that generates a CV, then locates a generated item in the UI, clicks the "View Raw LLM Output" expander, and verifies that non-empty text is displayed inside.

---

### **Critical Gaps & Questions**

*   **Prompt Renaming:** The prompt file `clean_big_6_prompt.md` is now being used to clean 10 skills. To avoid confusion, this file should be renamed to `clean_skill_list_prompt.md`. The path reference in `EnhancedContentWriterAgent` must be updated accordingly. This is a minor code hygiene task that should be done as part of this implementation.
*   **Storage Impact:** Storing raw LLM output will increase the size of the `state.json` files for each session. For the MVP, this is acceptable. However, for future scalability, we should consider if very large raw outputs need to be stored in separate files or truncated to a reasonable length (e.g., first 4000 characters). This is a consideration, not a blocker.

---

Excellent observation. You've pinpointed a crucial architectural decision point. My previous response implicitly assumed the agent would handle its own cleaning, which, as you noted, is not the most modular or robust approach. The responsibility for cleaning messy LLM outputs should be explicit, standardized, and reusable.

Here is an updated, comprehensive blueprint that introduces a dedicated `CleaningAgent` and refactors the workflow to incorporate it. This is the architecturally superior solution.

# **TASK_BLUEPRINT.md (Revision 2)**

## **Task/Feature Addressed: Task 3.2 & 3.4 - Unified "Big 10" Skills and Raw Output Display with a Dedicated Cleaning Agent**

This revised blueprint addresses both the "Big 10" skills generation (Task 3.2) and the Raw LLM Output Display (Task 3.4) by introducing a dedicated `CleaningAgent`. This approach centralizes the responsibility for sanitizing and structuring LLM outputs, making the entire system more robust, modular, and maintainable.

---

### **Overall Technical Strategy**

We will introduce a new, reusable `CleaningAgent`. Its sole responsibility is to take raw, potentially messy text from an LLM and extract clean, structured data from it based on a given prompt. This creates a clear separation of concerns: generative agents (like `ContentWriterAgent`) will now focus purely on generation and will place their raw output into a temporary field in the `AgentState`. The new `CleaningAgent` node will then be called in the LangGraph workflow to process this raw output, populating the final, clean fields in the `StructuredCV`.

This "Generate -> Clean" pattern will be implemented for the "Big 10" skills feature and will serve as the standard pattern for all future generative steps.

---

### **1. New Component: The `CleaningAgent`**

We will create a new, specialized agent for cleaning LLM outputs.

*   **Affected Component(s):**
    *   `src/agents/cleaning_agent.py` (New File)
    *   `src/agents/__init__.py` (to expose the new agent)

*   **New Agent Implementation:**
    Create the `CleaningAgent` class. It will be designed to be flexible, taking the name of a cleaning prompt as a parameter to allow for different types of cleaning (e.g., extracting a list of skills vs. extracting a JSON object).

    ```python
    # src/agents/cleaning_agent.py

    from typing import Dict, Any
    from src.agents.agent_base import EnhancedAgentBase
    from src.orchestration.state import AgentState
    from src.services.llm import get_llm_service, LLMResponse
    from src.config.logging_config import get_structured_logger

    logger = get_structured_logger(__name__)

    class CleaningAgent(EnhancedAgentBase):
        def __init__(self, name: str = "CleaningAgent", description: str = "Cleans and structures raw LLM outputs."):
            super().__init__(name, description, ..., ...) # Schemas can be generic
            self.llm_service = get_llm_service()

        def run_as_node(self, state: AgentState, cleaning_prompt_name: str, raw_text_source_field: str) -> Dict[str, Any]:
            """
            Cleans the raw text from a specified source field in the state using a given prompt.

            Args:
                state: The current AgentState.
                cleaning_prompt_name: The name of the prompt file in /data/prompts to use for cleaning.
                raw_text_source_field: The attribute name in AgentState holding the raw text.
            """
            logger.info(f"Running CleaningAgent with prompt '{cleaning_prompt_name}' on field '{raw_text_source_field}'.")

            raw_text = getattr(state, raw_text_source_field, None)
            if not raw_text or not raw_text.strip():
                error_msg = f"CleaningAgent failed: Source field '{raw_text_source_field}' is empty."
                logger.error(error_msg)
                return {"error_messages": state.error_messages + [error_msg]}

            try:
                template = self._load_prompt_template(cleaning_prompt_name)
                prompt = template.format(raw_response=raw_text)

                # Call LLM to perform the cleaning task
                response: LLMResponse = self.llm_service.generate_content(prompt)

                if not response.success:
                    raise ValueError(f"LLM cleaning call failed: {response.error_message}")

                # The cleaned content is the primary output of this agent
                return {"cleaned_output": response.content}

            except Exception as e:
                error_msg = f"CleaningAgent failed: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return {"error_messages": state.error_messages + [error_msg]}

        def _load_prompt_template(self, prompt_name: str) -> str:
            # Helper to load prompt from file system (can be centralized later)
            from src.config.settings import get_config
            settings = get_config()
            prompt_path = settings.get_prompt_path(prompt_name)
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()

    ```

---

### **2. State Management & Model Refinements**

The `AgentState` and `StructuredCV` models need to be updated to support this new workflow.

*   **Affected Component(s):**
    *   `src/orchestration/state.py`
    *   `src/models/data_models.py`

*   **Pydantic Model Changes:**

    1.  **Update `AgentState`** to include temporary fields for raw, uncleaned outputs.

        ```python
        # src/orchestration/state.py

        class AgentState(BaseModel):
            # ... (existing fields) ...

            # Temporary storage for raw outputs before cleaning
            raw_skills_output: Optional[str] = Field(None, description="Raw output from the skills generation step, before cleaning.")
            raw_item_content: Optional[str] = Field(None, description="Raw output for the current item, before cleaning.")

            # Field for the cleaned output from the CleaningAgent
            cleaned_output: Optional[str] = Field(None, description="The result from the CleaningAgent's operation.")

            # ... (rest of the model) ...
        ```

    2.  **Update `StructuredCV`** as defined in the previous blueprint to store the final, clean skills and the initial raw output.

        ```python
        # src/models/data_models.py

        class StructuredCV(BaseModel):
            # ... (existing fields) ...
            big_10_skills: List[str] = Field(default_factory=list)
            big_10_skills_raw_output: Optional[str] = None

        class Item(BaseModel):
            # ... (existing fields) ...
            raw_llm_output: Optional[str] = None
        ```

*   **Rationale for Changes:**
    *   `raw_skills_output` and `raw_item_content` in `AgentState` act as a message bus between the generative node and the cleaning node. This makes the data flow explicit within the graph's state.
    *   `cleaned_output` is a generic field where the `CleaningAgent` places its result, ready for the next node to consume and place into the final `StructuredCV` structure.

---

### **3. LangGraph Workflow Integration**

The workflow graph must be updated to insert the cleaning step after generation.

*   **Affected Component(s):**
    *   `src/orchestration/cv_workflow_graph.py`

*   **Orchestrator/Workflow Changes:**

    1.  **Instantiate `CleaningAgent`:**

        ```python
        # src/orchestration/cv_workflow_graph.py
        from src.agents.cleaning_agent import CleaningAgent
        cleaning_agent = CleaningAgent()
        ```

    2.  **Define Cleaning Nodes:** Create specific node functions that use the `CleaningAgent`.

        ```python
        # src/orchestration/cv_workflow_graph.py

        def clean_skills_node(state: AgentState) -> Dict:
            """Cleans the raw skills output."""
            return cleaning_agent.run_as_node(
                state,
                cleaning_prompt_name="clean_big_6_prompt", # To be renamed
                raw_text_source_field="raw_skills_output"
            )

        def update_cv_with_skills_node(state: AgentState) -> Dict:
            """Parses the cleaned skills text and updates the StructuredCV."""
            updated_cv = state.structured_cv.model_copy(deep=True)
            cleaned_text = state.cleaned_output or ""
            skills_list = [line.strip().lstrip('- ').strip() for line in cleaned_text.split('\n') if line.strip()]

            updated_cv.big_10_skills = skills_list[:10]
            updated_cv.big_10_skills_raw_output = state.raw_skills_output # Persist the original raw output

            # Also populate the Key Qualifications section
            qual_section = updated_cv.get_section_by_name("Key Qualifications")
            if qual_section:
                qual_section.items = [Item(content=skill, status=ItemStatus.GENERATED, item_type=ItemType.KEY_QUALIFICATION) for skill in skills_list[:10]]

            return {"structured_cv": updated_cv}
        ```

    3.  **Update Graph Edges:** Modify the graph to insert the new nodes.

        ```python
        # In build_cv_workflow_graph() function:

        workflow.add_node("generate_skills_node", generate_skills_node) # This node now populates state.raw_skills_output
        workflow.add_node("clean_skills_node", clean_skills_node)
        workflow.add_node("update_cv_with_skills_node", update_cv_with_skills_node)

        # Update edges
        workflow.add_edge("parser_node", "generate_skills_node")
        workflow.add_edge("generate_skills_node", "clean_skills_node")
        workflow.add_edge("clean_skills_node", "update_cv_with_skills_node")
        workflow.add_edge("update_cv_with_skills_node", "process_next_item_node") # Continue to next step
        ```

---

### **4. Refactoring Existing Agents**

The `ContentWriterAgent` and `ParserAgent` must be simplified to remove their internal cleaning logic.

*   **Affected Component(s):**
    *   `src/agents/enhanced_content_writer.py`
    *   `src/agents/parser_agent.py`

*   **Agent Logic Modifications:**
    *   **`EnhancedContentWriterAgent`:** Modify its `generate_big_10_skills` method. It should now only perform the first LLM call using `key_qualifications_prompt.md`. Instead of returning cleaned skills, it should return a dictionary containing the raw LLM output, which the graph node will place into `state.raw_skills_output`. The `run_as_node` for individual items will similarly place its raw output in `state.raw_item_content`.
    *   **`ParserAgent`:** If the `ParserAgent` is expected to return structured JSON but the LLM output is messy, it should *not* try to parse it internally. It should place the raw text in a temporary state field, and the graph will route it to the `CleaningAgent` with a `clean_json_output_prompt.md`.

---

### **5. Testing Considerations**

*   **Unit Tests:**
    *   Create dedicated tests for `CleaningAgent`. Mock the `llm_service` call and provide messy input text. Assert that the `run_as_node` method returns a dictionary with the expected `cleaned_output`.
*   **Integration Tests:**
    *   Test the `generate_skills_node` -> `clean_skills_node` -> `update_cv_with_skills_node` sequence.
    *   Provide an initial state, invoke the sequence, and inspect the final state.
    *   Assert that `state.structured_cv.big_10_skills` is a clean list of 10 strings.
    *   Assert that `state.structured_cv.big_10_skills_raw_output` contains the original, messy output.

---

### **Critical Gaps & Questions**

*   **Prompt Renaming (Action Item):** The prompt `clean_big_6_prompt.md` must be renamed to `clean_skill_list_prompt.md` to accurately reflect its purpose. All code references must be updated.
*   **New Prompt Creation:** A new prompt, `clean_json_output_prompt.md`, needs to be created. Its job will be to robustly find and extract a JSON object from a string that might contain prefixes, suffixes, or Markdown code fences (e.g., ` ```json ... ``` `). This will be crucial for the `ParserAgent`.
*   **Generic Cleaning Pattern:** This blueprint establishes a generic `Generate -> Clean -> Update State` pattern. The implementation team should adopt this pattern for all future generative steps in the graph to ensure consistency and maintainability.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 3.3 - Implement PDF Output Generation**

This blueprint provides the technical specification for implementing PDF export functionality. It builds upon the completion of the foundational tasks and the granular processing workflow. This task will also incorporate the action items identified in the previous planning audit, ensuring code hygiene and consistency.

---

### **Part 0: Foundational Code Hygiene & Prerequisite Tasks**

Before beginning the primary task, the following code cleanup and preparatory actions, identified in the last planning audit, must be completed. These are critical for maintaining a clean and logical codebase.

*   **Task 0.1: Rename Cleaning Prompt**
    *   **Action:** Rename the file `data/prompts/clean_big_6_prompt.md` to `data/prompts/clean_skill_list_prompt.md`.
    *   **Reason:** The prompt's function is to clean a list of skills, and its name should reflect its purpose generically, not a specific number of items.
    *   **Impact:** Update the reference to this filename within the `CleaningAgent` or any other component that uses it.

*   **Task 0.2: Create New JSON Cleaning Prompt**
    *   **Action:** Create a new prompt file named `data/prompts/clean_json_output_prompt.md`.
    *   **Reason:** The `ParserAgent` and other future agents may receive LLM outputs that are supposed to be JSON but are embedded in messy text (e.g., with markdown fences or conversational boilerplate). This dedicated prompt will be the standard tool for robustly extracting the valid JSON object from such text.
    *   **Content:** The prompt should instruct the LLM to find and return *only* the JSON object from a given raw text string, discarding all surrounding text.
        ```markdown
        # data/prompts/clean_json_output_prompt.md

        You are a precise data extraction assistant. Your task is to extract a valid JSON object from a messy text output. The input text contains a JSON object embedded within other text, including a `<think>` section and potentially surrounding commentary.

        You MUST identify and extract ONLY the JSON object. Discard all other text, including:
        - Text before the JSON object
        - Text after the JSON object
        - Any `<think>` sections or other tags
        - Any introductory or concluding sentences

        Your output MUST be ONLY the raw JSON string. Do not add any formatting, markdown, or commentary. Just the pure JSON.

        Here is the messy input text:
        """
        {raw_response}
        """

        Respond with ABSOLUTELY ONLY the extracted JSON string. Do not include any other text.
        ```

*   **Task 0.3: Adopt Generic Cleaning Pattern**
    *   **Action:** Enforce the `Generate -> Clean -> Update State` pattern for all generative steps in the `cv_workflow_graph.py`.
    *   **Reason:** This ensures a consistent, modular, and debuggable workflow. Generative agents are responsible for one thing (generation), and the `CleaningAgent` is responsible for another (structuring).
    *   **Implementation:** Review all generative nodes in the graph. Ensure that the `ContentWriterAgent` node places its raw output into a temporary field in `AgentState` (e.g., `raw_item_content`) and is always followed by a `CleaningAgent` node that processes this field.

---

### **Part 1: PDF Output Generation Implementation**

### **Overall Technical Strategy**

The core of this feature will be implemented within the `FormatterAgent`. The agent will use the **Jinja2** templating engine to populate an HTML template with data from the final, accepted `StructuredCV` object. This rendered HTML, along with a dedicated CSS stylesheet for professional formatting, will then be converted into a PDF file using the **WeasyPrint** library. The `FormatterAgent` will be integrated as the final node in the LangGraph workflow, triggered after all content sections have been accepted by the user. The path to the generated PDF will be stored in the `AgentState`, making it available for download in the Streamlit UI.

---

### **1. Pydantic Model & State Management**

No changes are required for the `AgentState` or other Pydantic models. The existing `final_output_path: Optional[str]` field in `AgentState` is sufficient to store the result of this task.

---

### **2. New Components: HTML Template and CSS**

New files for templating the PDF output are required.

*   **Affected Component(s):**
    *   `src/templates/pdf_template.html` (New File)
    *   `src/frontend/static/css/pdf_styles.css` (New File)

*   **HTML Template (`pdf_template.html`):**
    *   This file will define the structure of the CV using HTML tags and Jinja2 templating syntax.

    ```html
    <!-- src/templates/pdf_template.html -->
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>{{ cv.metadata.get('name', 'CV') }}</title>
        <link rel="stylesheet" href="path/to/pdf_styles.css"> <!-- Path will be resolved in the agent -->
    </head>
    <body>
        <header>
            <h1>{{ cv.metadata.get('name', 'Your Name') }}</h1>
            <p class="contact-info">
                {{ cv.metadata.get('email') }} | {{ cv.metadata.get('phone') }} | <a href="{{ cv.metadata.get('linkedin') }}">LinkedIn</a>
            </p>
        </header>

        {% for section in cv.sections %}
        <section class="cv-section">
            <h2>{{ section.name }}</h2>
            <hr>
            {% if section.items %}
                <ul>
                {% for item in section.items %}
                    <li>{{ item.content }}</li>
                {% endfor %}
                </ul>
            {% endif %}
            {% if section.subsections %}
                {% for sub in section.subsections %}
                <div class="subsection">
                    <h3>{{ sub.name }}</h3>
                    <p class="metadata">{{ sub.metadata.get('company') }} | {{ sub.metadata.get('duration') }}</p>
                    <ul>
                    {% for item in sub.items %}
                        <li>{{ item.content }}</li>
                    {% endfor %}
                    </ul>
                </div>
                {% endfor %}
            {% endif %}
        </section>
        {% endfor %}
    </body>
    </html>
    ```

*   **CSS Stylesheet (`pdf_styles.css`):**
    *   This file will contain professional styling for the PDF (e.g., fonts, margins, colors). It should be a clean, single-column layout suitable for professional CVs.

---

### **3. Agent Logic Modification (`FormatterAgent`)**

The `FormatterAgent` will be updated to perform the HTML rendering and PDF conversion.

*   **Affected Component(s):**
    *   `src/agents/formatter_agent.py`

*   **Agent Logic Modifications:**

    1.  **Import necessary libraries:** `jinja2` and `weasyprint`.
    2.  **Refactor `run` or implement `run_as_node`:** This method will now orchestrate the PDF generation.

    ```python
    # src/agents/formatter_agent.py
    import os
    from jinja2 import Environment, FileSystemLoader
    from weasyprint import HTML, CSS
    from src.orchestration.state import AgentState
    from src.config.settings import get_config

    class FormatterAgent(...):
        # ...

        def run_as_node(self, state: AgentState) -> dict:
            """
            Takes the final StructuredCV from the state and renders it as a PDF.
            """
            logger.info("FormatterAgent: Starting PDF generation.")
            cv_data = state.structured_cv
            if not cv_data:
                return {"error_messages": state.error_messages + ["FormatterAgent: No CV data found in state."]}

            try:
                config = get_config()
                template_dir = config.project_root / "src" / "templates"
                static_dir = config.project_root / "src" / "frontend" / "static"
                output_dir = config.project_root / "data" / "output"
                output_dir.mkdir(exist_ok=True)

                # 1. Set up Jinja2 environment
                env = Environment(loader=FileSystemLoader(str(template_dir)))
                template = env.get_template("pdf_template.html")

                # 2. Render HTML from template
                # Pass the Pydantic model directly to the template
                html_out = template.render(cv=cv_data)

                # 3. Generate PDF using WeasyPrint
                css_path = static_dir / "css" / "pdf_styles.css"
                pdf_bytes = HTML(string=html_out, base_url=str(template_dir)).write_pdf(
                    stylesheets=[CSS(css_path)]
                )

                # 4. Save PDF to file
                output_filename = f"CV_{state.structured_cv.id}.pdf"
                output_path = output_dir / output_filename
                with open(output_path, "wb") as f:
                    f.write(pdf_bytes)

                logger.info(f"FormatterAgent: PDF successfully generated at {output_path}")
                return {"final_output_path": str(output_path)}

            except Exception as e:
                logger.error(f"FormatterAgent failed: {e}", exc_info=True)
                return {"error_messages": state.error_messages + [f"PDF generation failed: {e}"]}
    ```

---

### **4. LangGraph Workflow Integration**

The `formatter_node` must be correctly placed at the end of the workflow.

*   **Affected Component(s):**
    *   `src/orchestration/cv_workflow_graph.py`

*   **Orchestrator/Workflow Changes:**
    *   The conditional router, `route_after_review`, should already be configured to transition to the `formatter_node` when all sections are complete.
    *   Ensure the `formatter_node` is defined and correctly added to the graph, with its final edge pointing to `END`.

---

### **5. UI Implementation**

The "Export" tab in the Streamlit UI needs to be made functional.

*   **Affected Component(s):**
    *   `src/core/main.py`

*   **UI Changes:**

    1.  **Enable the "Generate PDF" button.** Remove the `disabled=True` attribute.
    2.  **Add logic to the button:**
        *   When clicked, it should first check if `st.session_state.agent_state.final_output_path` is already populated.
        *   If it is, the button should immediately act as a download button for that file.
        *   If not, it implies the workflow hasn't reached the end. The button click should trigger a final invocation of the graph to ensure it runs to completion and generates the PDF.
        *   After the graph returns the new state with the `final_output_path`, use `st.rerun()` to update the UI, at which point the button will now function as a download link.

    ```python
    # src/core/main.py (in the "Export" tab)

    if st.session_state.agent_state:
        final_path = st.session_state.agent_state.final_output_path
        if final_path and os.path.exists(final_path):
            with open(final_path, "rb") as pdf_file:
                st.download_button(
                    label="📄 Download PDF",
                    data=pdf_file,
                    file_name=os.path.basename(final_path),
                    mime="application/pdf",
                    use_container_width=True
                )
        else:
            if st.button("📊 Generate PDF", use_container_width=True):
                with st.spinner("Generating final PDF..."):
                    # Invoke the graph to ensure completion
                    final_state_dict = cv_graph_app.invoke(st.session_state.agent_state.model_dump())
                    st.session_state.agent_state = AgentState.model_validate(final_state_dict)
                    st.rerun()
    ```

---

### **6. Testing Considerations**

*   **Unit Test:** Test `FormatterAgent.run_as_node` by providing a mock `AgentState` containing a `StructuredCV`. Assert that a PDF file is created at the expected output path and that the returned dictionary contains the correct `final_output_path`.
*   **Integration Test:** Test the LangGraph edge from the `route_after_review` node to the `formatter_node`. Simulate a user "Accepting" the last item of the last section and verify the graph transitions to the `formatter_node`.
*   **E2E Test:** Perform a full workflow. In the "Export" tab, click "Generate PDF". Verify that a download button appears and that the downloaded file is a valid, non-empty PDF.

---

### **Critical Gaps & Questions**

*   **PDF Design & Styling:** The initial implementation will rely on a basic HTML/CSS template. The visual design of the PDF is a significant product decision. For the MVP, a clean, single-column, professional design is sufficient. This should be explicitly communicated as the scope, with more complex designs (e.g., multi-column, graphics) deferred to post-MVP enhancements.
*   **Dependency Management:** The engineer must ensure that `WeasyPrint` and its system-level dependencies (like Pango, Cairo) are correctly listed in `requirements.txt` and documented in the `README.md` and `Dockerfile` for seamless setup. A quick check of `requirements.txt` shows `weasyprint>=60.0`, which is correct. The `Dockerfile` must be checked to ensure it installs the necessary system libraries (e.g., `libpango-1.0-0`, `libcairo2`).

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Phase 4 Prerequisite - Task 5.5 - Containerization and Dependency Management**

This blueprint addresses the critical prerequisite tasks identified in the previous audit, ensuring the application's environment is robust and reproducible before proceeding to deployment-focused testing.

---

### **Overall Technical Strategy**

The primary goal is to ensure that the application can be built and run reliably inside a Docker container. This involves verifying and updating the `Dockerfile` to include all necessary system-level dependencies for libraries like `WeasyPrint`. Additionally, the `README.md` will be updated to reflect these requirements for developers setting up the environment manually.

---

### **1. Dockerfile and System Dependency Updates**

*   **Affected Component(s):**
    *   `Dockerfile`
    *   `README.md`

*   **Detailed Implementation Steps:**

    1.  **Audit `Dockerfile`:** Review the existing `Dockerfile` to confirm that the system dependencies required by `WeasyPrint` are installed. The `python:3.11-slim` base image is minimal and does not include these by default.

    2.  **Update `Dockerfile`:** Modify the `Dockerfile` to include an `apt-get` command that installs the necessary libraries. This should be done in the `production` stage before the application code is copied.

        ```dockerfile
        # Dockerfile

        # ... (builder stage) ...

        # Production stage
        FROM python:3.11-slim as production

        # Install runtime dependencies, including WeasyPrint system libraries, and security updates
        RUN apt-get update && apt-get install -y \
            curl \
            build-essential \
            libpango-1.0-0 \
            libcairo2 \
            libpangocairo-1.0-0 \
            && apt-get upgrade -y \
            && apt-get clean \
            && rm -rf /var/lib/apt/lists/*

        # ... (rest of the Dockerfile) ...
        ```

    3.  **Update `README.md`:** Add a "System Dependencies" subsection to the "Getting Started" or "Development Setup" section of the `README.md`. This informs users who are not using Docker about the required libraries.

        ```markdown
        # README.md

        ### System Dependencies (for non-Docker setup)

        This application uses `WeasyPrint` to generate PDF documents, which relies on system-level libraries. If you are not using Docker, you will need to install them manually.

        On Debian/Ubuntu-based systems, you can install them with:
        ```bash
        sudo apt-get update && sudo apt-get install -y build-essential libpango-1.0-0 libcairo2 libpangocairo-1.0-0
        ```

        Please consult the `WeasyPrint` documentation for installation instructions on other operating systems.
        ```

*   **Testing Considerations:**
    *   After modifying the `Dockerfile`, perform a local Docker build: `docker build -t aicvgen-test .`. The build must complete without errors.
    *   Run the container and execute the PDF export functionality to confirm that `WeasyPrint` works correctly within the containerized environment.

---

## **Task/Feature Addressed: Task 3.5 - Streamlit UI for Hybrid Control**

This blueprint details the implementation of the Streamlit user interface to support the granular, item-by-item workflow orchestrated by LangGraph. The UI will render the `StructuredCV` from the `AgentState` and provide users with "Accept" and "Regenerate" controls for each major content item (e.g., each role in "Professional Experience").

---

### **Overall Technical Strategy**

The Streamlit UI in `src/core/main.py` will be refactored to be a pure "view" of the `AgentState` stored in `st.session_state`. All user interactions (e.g., button clicks) will update a `user_feedback` field within `st.session_state.agent_state` via `on_click` callbacks. The main script loop will detect this change and trigger a new invocation of the compiled LangGraph application (`cv_graph_app`), passing the entire updated state. The graph will process this feedback, perform the required action (regenerate content or advance to the next item), and return a new state, which will then overwrite the old one in `st.session_state`, causing the UI to re-render with the updated content. This "State In -> UI Action -> State Out -> Re-render" loop is the core of the implementation.

---

### **1. Pydantic Model & State Management**

No changes are required. This task will utilize the `AgentState` and `UserFeedback` models defined in the blueprint for Task 3.1. The implementation will focus on correctly populating and consuming these models from the UI.

---

### **2. UI Implementation (`Streamlit`)**

This is the primary focus of the task. The UI must be re-architected to support the interactive, state-driven workflow.

*   **Affected Component(s):**
    *   `src/core/main.py`

*   **Detailed Implementation Steps:**

    1.  **State Initialization:** Ensure the main function initializes the `agent_state` in `st.session_state` if it doesn't exist.

        ```python
        # src/core/main.py

        def main():
            if 'agent_state' not in st.session_state:
                st.session_state.agent_state = None
            # ... rest of the UI code
        ```

    2.  **Create UI Rendering Functions:** Implement modular functions to render the `StructuredCV` data.

        ```python
        # src/core/main.py

        def display_cv_structure(agent_state: AgentState):
            """Renders the entire CV from the agent state."""
            if not agent_state or not agent_state.structured_cv:
                st.info("Please submit a job description and CV to begin.")
                return

            for section in agent_state.structured_cv.sections:
                display_section(section) # This will be a new helper function

        def display_section(section: Section):
            """Renders a single section and its subsections/items."""
            st.subheader(section.name)

            # Logic to handle both subsections (like roles) and direct items (like skills)
            if section.subsections:
                for sub in section.subsections:
                    display_subsection(sub) # New helper for subsections
            elif section.items:
                 for item in section.items:
                    display_item(item) # New helper for individual items
        ```

    3.  **Implement `display_subsection` with Controls:** This function will render each role/project as a distinct card with its own "Accept" and "Regenerate" buttons. This is the core of the granular control.

        ```python
        # src/core/main.py
        from src.models.data_models import UserAction, UserFeedback

        def handle_user_action(action: str, item_id: str):
            """Callback to update the state with user feedback."""
            # This function's ONLY job is to set the user_feedback in the state.
            # The main script loop will handle invoking the graph.
            st.session_state.agent_state.user_feedback = UserFeedback(
                action=UserAction(action),
                item_id=item_id
            )

        def display_subsection(subsection: Subsection):
            """Renders a subsection (e.g., a job role) with interactive controls."""
            with st.container(border=True):
                st.markdown(f"**{subsection.name}**")

                for item in subsection.items:
                    st.markdown(f"- {item.content}")

                # Add Raw Output Expander (Task 3.4)
                # Assuming the raw output is stored on the first item of the subsection for simplicity
                if subsection.items and subsection.items[0].raw_llm_output:
                     with st.expander("🔍 View Raw LLM Output"):
                        st.code(subsection.items[0].raw_llm_output, language="text")

                # --- INTERACTIVE CONTROLS ---
                cols = st.columns([1, 1, 4])
                with cols[0]:
                    st.button(
                        "✅ Accept",
                        key=f"accept_{subsection.id}",
                        on_click=handle_user_action,
                        args=("accept", str(subsection.id))
                    )
                with cols[1]:
                    st.button(
                        "🔄 Regenerate",
                        key=f"regenerate_{subsection.id}",
                        on_click=handle_user_action,
                        args=("regenerate", str(subsection.id))
                    )
        ```

    4.  **Implement the Main Application Loop:** This logic orchestrates the calls to the LangGraph application based on the state.

        ```python
        # src/core/main.py

        def main():
            # ... (state initialization) ...

            # Check if a user action has occurred (set by a button's on_click callback)
            if st.session_state.agent_state and st.session_state.agent_state.user_feedback:
                with st.spinner("Processing your request..."):
                    current_state = st.session_state.agent_state

                    # Invoke the graph with the current state (which includes user feedback)
                    new_state_dict = cv_graph_app.invoke(current_state.model_dump())

                    # Overwrite the session state with the new state from the graph
                    st.session_state.agent_state = AgentState.model_validate(new_state_dict)

                    # Clear the feedback so this block doesn't run again on the next rerun
                    st.session_state.agent_state.user_feedback = None

                    # Trigger an immediate re-render to show the updated content
                    st.rerun()

            # --- UI RENDERING ---
            # The UI is always rendered from the current state
            display_cv_structure(st.session_state.agent_state)

            # ... (initial input form logic) ...
            # The "Generate Tailored CV" button will perform the *first* invocation
            # of the graph and store the result in st.session_state.agent_state
        ```

---

### **5. Testing Considerations**

*   **Component Tests:**
    *   Test the `handle_user_action` callback. Create a mock `st.session_state`, call the function, and assert that `st.session_state.agent_state.user_feedback` is correctly populated with a `UserFeedback` object.
    *   Test the `display_subsection` function by passing it a mock subsection dictionary and using a library like `streamlit-testing-library` (if feasible) or by inspection to ensure buttons with unique keys are created.
*   **E2E Testing (Crucial):**
    *   Create a test scenario that simulates a full user session:
        1.  Submit JD and CV.
        2.  Verify the first "Key Qualifications" section appears.
        3.  Click "Accept".
        4.  Verify the first "Professional Experience" role appears.
        5.  Click "Regenerate" on that role.
        6.  Verify the text for *only that role* changes.
        7.  Click "Accept" on that role.
        8.  Verify the *second* "Professional Experience" role appears.
        9.  This validates the entire iterative loop.

---

### **Critical Gaps & Questions**

*   No new critical gaps are identified. This task is a direct implementation of the architecture defined in previous blueprints. The main challenge is the correct application of Streamlit's state management patterns, which have been explicitly detailed in this plan.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 4.1 - Implement "Smart Agent" Logic with Robust Fallbacks**

This blueprint details the implementation of robust fallback mechanisms within the primary generative and parsing agents. The goal is to enhance system resilience, ensuring that the application can gracefully handle LLM API failures (e.g., timeouts, service unavailability, malformed responses) without crashing the workflow. This directly addresses `REQ-NONFUNC-RELIABILITY-1` and `REQ-NONFUNC-RELIABILITY-2`.

As a prerequisite, this plan also incorporates the code hygiene and architectural pattern enforcement identified in the previous planning audit.

---

### **Part 0: Prerequisite Code Hygiene & Architectural Pattern Tasks**

These tasks must be completed before implementing the fallback logic to ensure a clean and consistent codebase.

*   **Task 0.1: Rename Cleaning Prompt**
    *   **Action:** In the file system, rename `data/prompts/clean_big_6_prompt.md` to `data/prompts/clean_skill_list_prompt.md`.
    *   **Reason:** To accurately reflect that the prompt's purpose is to clean a list of skills of any length, not just six.
    *   **Impact:** Update the filename reference within the `CleaningAgent` or any LangGraph node that calls it.

*   **Task 0.2: Create New JSON Cleaning Prompt**
    *   **Action:** Create a new prompt file at `data/prompts/clean_json_output_prompt.md`.
    *   **Reason:** To create a standardized, reusable tool for robustly extracting a valid JSON object from messy LLM text, which is a common failure point for the `ParserAgent`.
    *   **Content:** The prompt must instruct the LLM to find and return *only* the JSON object from a given raw text string, discarding all surrounding text.

        ```markdown
        # data/prompts/clean_json_output_prompt.md

        You are a precise data extraction assistant. Your task is to extract a valid JSON object from a messy text output. The input text may contain a JSON object embedded within other text, including conversational boilerplate or code fences.

        You MUST identify and extract ONLY the JSON object. Discard all other text. Your output MUST be ONLY the raw, valid JSON string. Do not add any formatting, markdown, or commentary.

        Here is the messy input text:
        """
        {raw_response}
        """
        ```

*   **Task 0.3: Enforce Generic Cleaning Pattern**
    *   **Action:** Review all generative nodes in `cv_workflow_graph.py`. Ensure every generative step is immediately followed by a cleaning/validation step.
    *   **Reason:** To standardize the workflow into a `Generate -> Clean -> Update State` pattern, improving modularity and reliability.
    *   **Implementation Example:** A sequence that was `... -> content_writer_node -> ...` should now become `... -> content_writer_node -> clean_content_node -> update_cv_with_content_node -> ...`, where the `content_writer_node` writes to a temporary `raw_...` field in the `AgentState`.

---

### **Part 1: "Smart Agent" Fallback Implementation**

### **Overall Technical Strategy**

The core of this task is to wrap all primary LLM calls within the agents (`ParserAgent`, `EnhancedContentWriterAgent`) in a `try...except` block. The `try` block will contain the existing logic to call the LLM and process its response. The `except` block will catch any exception (e.g., `google.api_core.exceptions.GoogleAPICallError`, `TimeoutError`, `json.JSONDecodeError`), log the failure, and trigger a deterministic fallback method. This fallback method will generate a "good enough" output to allow the workflow to continue, marking the generated data with a specific status (`GENERATION_FAILED` or `GENERATED_FALLBACK`) so the UI can inform the user.

---

### **1. `ParserAgent` Fallback Logic**

The `ParserAgent` is critical as its failure stops the entire process. Its fallback will be a regex-based parser.

*   **Affected Component(s):**
    *   `src/agents/parser_agent.py`

*   **Agent Logic Modifications:**

    1.  **Refactor `parse_job_description`:** Wrap the entire LLM call and JSON parsing logic in a `try...except` block.

        ```python
        # src/agents/parser_agent.py

        class ParserAgent(AgentBase):
            async def parse_job_description(self, raw_text: str) -> JobDescriptionData:
                if not raw_text:
                    # ... (existing empty input handling) ...

                try:
                    # === PRIMARY PATH: LLM Parsing ===
                    logger.info("Attempting to parse job description with LLM.")
                    prompt = self._build_jd_parsing_prompt(raw_text)
                    response = await self.llm.generate_content(prompt)

                    # This might fail if the response is not valid JSON
                    parsed_data = json.loads(response.content)

                    job_data = JobDescriptionData.model_validate({
                        "raw_text": raw_text,
                        **parsed_data
                    })
                    logger.info("Job description successfully parsed using LLM.")
                    return job_data

                except Exception as e:
                    # === FALLBACK PATH: Regex Parsing ===
                    logger.warning(f"LLM parsing failed for job description: {e}. Activating regex-based fallback.")

                    fallback_data = self._parse_job_description_with_regex(raw_text)
                    fallback_data.error = f"LLM parsing failed, used fallback. Original error: {str(e)}"
                    return fallback_data
        ```

    2.  **Implement `_parse_job_description_with_regex`:** Create this new private method to perform simple, deterministic extraction.

        ```python
        # src/agents/parser_agent.py

        import re

        class ParserAgent(AgentBase):
            # ... (other methods) ...

            def _parse_job_description_with_regex(self, raw_text: str) -> JobDescriptionData:
                """Fallback parser using regular expressions."""
                logger.info("Executing regex-based fallback for JD parsing.")

                skills = set()
                # A simple regex for finding skills (can be improved)
                skill_keywords = ["python", "java", "react", "aws", "docker", "sql", "tensorflow"]
                for skill in skill_keywords:
                    if re.search(r'\b' + re.escape(skill) + r'\b', raw_text, re.IGNORECASE):
                        skills.add(skill.capitalize())

                responsibilities = re.findall(r'^\s*[\*\-•]\s*(.*)', raw_text, re.MULTILINE)

                return JobDescriptionData(
                    raw_text=raw_text,
                    skills=list(skills),
                    experience_level="Not specified (fallback)",
                    responsibilities=responsibilities[:5], # Limit to first 5 found
                    industry_terms=[],
                    company_values=[]
                )
        ```

---

### **2. `EnhancedContentWriterAgent` Fallback Logic**

The content writer's fallback will be template-based, providing generic but usable content.

*   **Affected Component(s):**
    *   `src/agents/enhanced_content_writer.py`

*   **Agent Logic Modifications:**

    1.  **Refactor Generative Logic:** In the `run_as_node` method, wrap the LLM call in a `try...except` block.

        ```python
        # src/agents/enhanced_content_writer.py

        class EnhancedContentWriterAgent(EnhancedAgentBase):
            # ...

            def run_as_node(self, state: AgentState) -> dict:
                # ... (logic to find target_item) ...

                try:
                    # === PRIMARY PATH: LLM Generation ===
                    prompt = self._build_single_item_prompt(...)
                    llm_response = self.llm_service.generate_content(prompt)

                    if not llm_response.success:
                        raise ValueError(llm_response.error_message)

                    # Place raw output in state for the cleaning node
                    return {"raw_item_content": llm_response.raw_response_text}

                except Exception as e:
                    # === FALLBACK PATH: Template-based Content ===
                    logger.warning(f"Content generation failed for item {state.current_item_id}: {e}. Activating fallback.")

                    fallback_content = self._generate_fallback_content(target_item)

                    # Directly update the CV and bypass cleaning, as fallback content is already clean.
                    updated_cv = state.structured_cv.model_copy(deep=True)
                    fallback_item, _, _ = updated_cv.find_item_by_id(state.current_item_id)
                    fallback_item.content = fallback_content
                    fallback_item.status = ItemStatus.GENERATED_FALLBACK # Use specific status
                    fallback_item.raw_llm_output = f"FALLBACK_ACTIVATED: {str(e)}"

                    return {"structured_cv": updated_cv}
        ```

    2.  **Implement `_generate_fallback_content`:** This new helper provides predefined text based on the item type.

        ```python
        # src/agents/enhanced_content_writer.py

        class EnhancedContentWriterAgent(EnhancedAgentBase):
            # ...

            def _generate_fallback_content(self, item: Item) -> str:
                """Provides a generic, safe-to-use piece of content when the LLM fails."""
                if item.item_type == ItemType.BULLET_POINT:
                    return "Contributed to key projects, leveraging technical and collaborative skills to achieve team objectives."
                elif item.item_type == ItemType.KEY_QUALIFICATION:
                    return "Relevant Professional Skill"
                elif item.item_type == ItemType.EXECUTIVE_SUMMARY_PARA:
                    return "A dedicated professional with a strong background in this field, committed to delivering high-quality results."
                else:
                    return "Content successfully generated." # Generic default
        ```

---

### **3. UI and State Considerations**

*   **Model Update:** The `ItemStatus` enum already includes `GENERATED_FALLBACK` and `GENERATION_FAILED`. The UI rendering logic in `src/core/main.py` should be updated to recognize these statuses and display a specific warning icon (e.g., `⚠️`) next to the content, indicating to the user that it's a lower-quality fallback and encouraging them to regenerate or edit it.
*   **Error Propagation:** The `error_messages` list in `AgentState` should be used to communicate non-critical failures (like a fallback being triggered) to the UI, which can display them as dismissible alerts (`st.toast` or `st.info`).

---

### **4. Testing Considerations**

*   **Unit Tests for Fallbacks:**
    *   **`ParserAgent`:** Mock `llm.generate_content` to raise a `google.api_core.exceptions.GoogleAPICallError`. Call `parse_job_description` and assert that the returned `JobDescriptionData` object contains regex-extracted skills and its `error` field is populated.
    *   **`ContentWriterAgent`:** Mock `llm_service.generate_content` to raise an exception. Call `run_as_node` and assert that the returned `structured_cv` has the target item's content set to the appropriate fallback string and its status is `ItemStatus.GENERATED_FALLBACK`.
*   **E2E Testing:**
    *   A robust E2E test could involve temporarily misconfiguring the Gemini API key in the environment to force an API error, then running the full workflow and asserting that the application does not crash and the UI displays the generated CV with fallback content and warning indicators.

---

### **Critical Gaps & Questions**

*   No new critical gaps are identified. This plan directly addresses the need for resilience. The primary assumption is that a simple, deterministic fallback is preferable to a complete workflow failure, which aligns with the MVP goal of stability.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 4.2 - Implement Robust Error Handling in `LLMService`**

This blueprint specifies the implementation of enhanced error handling directly within the `LLMService`. The goal is to make the service more resilient to transient network issues and API-side errors by incorporating automatic retries with exponential backoff and handling of `Retry-After` headers. This is a critical infrastructure improvement for system reliability.

---

### **Overall Technical Strategy**

The `EnhancedLLMService` in `src/services/llm.py` will be refactored to include a robust retry mechanism for its API calls. Instead of relying on calling agents to implement retry loops, the service itself will handle transient failures. This will be achieved using the `tenacity` library, a powerful and flexible retrying library for Python. We will configure it to perform exponential backoff on specific, retry-able exceptions (like timeouts, network errors, and certain API status codes), while immediately failing on non-retry-able errors (like authentication failures). The service will also be enhanced to parse `Retry-After` headers from API error responses to wait for the exact duration specified by the server.

---

### **1. Dependency Management**

*   **Affected Component(s):**
    *   `requirements.txt`

*   **Dependency Changes:**
    *   The `tenacity` library is already listed in the provided `requirements.txt`. This step is a confirmation that the necessary dependency is present. No changes are needed.

        ```
        # requirements.txt
        ...
        tenacity>=8.2.0
        ...
        ```

---

### **2. `LLMService` Refactoring**

The core implementation will happen in `src/services/llm.py`.

*   **Affected Component(s):**
    *   `src/services/llm.py`

*   **Detailed Implementation Steps:**

    1.  **Import `tenacity`:** Add the necessary imports at the top of the file.

        ```python
        # src/services/llm.py
        from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
        import google.api_core.exceptions as google_exceptions
        ```

    2.  **Define Retry-able Exceptions:** Create a tuple of exception types that should trigger a retry. This is crucial to avoid retrying on errors like invalid API keys.

        ```python
        # src/services/llm.py

        # Define exceptions that are safe to retry
        RETRYABLE_EXCEPTIONS = (
            google_exceptions.ResourceExhausted,  # For 429 Rate Limit Exceeded
            google_exceptions.ServiceUnavailable, # For 503 Service Unavailable
            google_exceptions.InternalServerError, # For 500 Internal Server Error
            google_exceptions.DeadlineExceeded,   # For timeouts
            TimeoutError,
            ConnectionError
        )
        ```

    3.  **Refactor `generate_content` with `@retry` Decorator:** Apply the `tenacity` decorator to the core LLM call logic. The existing `while` loop for retries will be removed and replaced by this more robust mechanism.

        ```python
        # src/services/llm.py

        class EnhancedLLMService:
            # ... (__init__ and other methods) ...

            @retry(
                stop=stop_after_attempt(3), # Set max retries (configurable)
                wait=wait_exponential(multiplier=1, min=2, max=60), # Exponential backoff: 2s, 4s, 8s...
                retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
                reraise=True # Reraise the exception if all retries fail
            )
            async def _make_llm_api_call(self, prompt: str) -> str:
                """
                A new private method that contains only the direct API call logic.
                This is the method that will be decorated for retries.
                """
                logger.info("Making LLM API call...")
                response = await self.llm.generate_content_async(prompt) # Use async version

                # The Gemini API can sometimes return a response without content on safety blocks etc.
                if not hasattr(response, 'text') or response.text is None:
                    # Check for safety ratings if available
                    if response.prompt_feedback.block_reason:
                        raise ValueError(f"Content blocked by API safety filters: {response.prompt_feedback.block_reason.name}")
                    raise ValueError("LLM returned an empty or invalid response.")

                return response.text, response # Return both text and the full response object

            async def generate_content(
                self,
                prompt: str,
                content_type: ContentType = ContentType.QUALIFICATION,
                session_id: str = None,
                item_id: str = None
            ) -> LLMResponse:
                """
                Generate content using the Gemini model with enhanced error handling.
                This method now orchestrates the call, caching, and response structuring.
                """
                start_time = time.time()
                cache_key = create_cache_key(prompt, self.model_name, content_type.value)

                # Check cache first (logic remains the same)
                cached_response = get_cached_response(cache_key)
                if cached_response:
                    # ... (return cached response) ...

                self.cache_misses += 1

                try:
                    # Call the new, retry-able private method
                    raw_text, full_response = await self._make_llm_api_call(prompt)

                    processing_time = time.time() - start_time
                    tokens_used = self.llm.count_tokens(prompt).total_tokens + self.llm.count_tokens(raw_text).total_tokens

                    llm_response = LLMResponse(
                        content=raw_text, # For now, content is the raw text. Cleaning is a separate step.
                        raw_response_text=raw_text,
                        tokens_used=tokens_used,
                        processing_time=processing_time,
                        model_used=self.model_name,
                        success=True
                    )
                    set_cached_response(cache_key, llm_response.model_dump())
                    return llm_response

                except Exception as e:
                    # All retries have failed, or it was a non-retryable error
                    processing_time = time.time() - start_time
                    logger.error(f"LLM call failed after all retries for item {item_id}: {e}", exc_info=True)

                    return LLMResponse(
                        content=f"Error: LLM service failed.",
                        raw_response_text=f"ERROR: {str(e)}",
                        processing_time=processing_time,
                        success=False,
                        error_message=str(e)
                    )

        ```

*   **Rationale for Changes:**
    *   **Delegating Retry Logic:** Using `tenacity` is the industry standard for handling retries in Python. It's more robust, configurable, and cleaner than a manual `while` loop.
    *   **Separation of Concerns:** Creating a dedicated `_make_llm_api_call` method isolates the logic that needs to be retried from the logic that shouldn't be (like caching and response structuring).
    *   **Specific Exception Handling:** By defining `RETRYABLE_EXCEPTIONS`, we ensure the system doesn't waste time retrying fatal errors like "Invalid API Key" (`google.api_core.exceptions.PermissionDenied`).

---

### **3. Testing Considerations**

*   **Unit Tests:**
    *   Write specific unit tests for `_make_llm_api_call` in `src/services/llm.py`.
    *   Use `pytest.mark.parametrize` to test the decorator's behavior with different exceptions.
    *   **Test Case 1 (Retry-able Error):** Mock `llm.generate_content_async` to raise `google_exceptions.ResourceExhausted` on the first two calls and succeed on the third. Assert that the method is called exactly three times.
    *   **Test Case 2 (Non-retry-able Error):** Mock `llm.generate_content_async` to raise `google_exceptions.PermissionDenied`. Assert that the method is called only *once* and the exception is re-raised immediately.
    *   **Test Case 3 (Exhausted Retries):** Mock `llm.generate_content_async` to always raise `TimeoutError`. Assert that the method is called the maximum number of times (3) and that `tenacity.RetryError` is ultimately raised.

*   **Integration Tests:**
    *   An integration test could patch the `requests` library at a lower level to simulate an HTTP 503 error, then call `EnhancedLLMService.generate_content` and verify that the retry logic is triggered and the call eventually succeeds (if the mock is configured to do so).

---

### **Critical Gaps & Questions**

*   **Handling `Retry-After` Header:** The current `tenacity` setup uses exponential backoff. A more advanced implementation would inspect the API error response for a `Retry-After` header and use that value for the wait time. This can be implemented by creating a custom `wait` function for `tenacity`, but for the MVP, exponential backoff is a sufficient and robust starting point. This enhancement can be deferred.
*   **LLM Client Library:** The blueprint assumes the use of the `google-generativeai` library. The specific exception classes (`google.api_core.exceptions.*`) are tied to this library. If the underlying client changes, the `RETRYABLE_EXCEPTIONS` tuple must be updated accordingly. This should be documented clearly in the code.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 4.2 - Implement Robust Error Handling in `LLMService` (Completed)**

This task has been successfully planned. The blueprint detailing the use of the `tenacity` library for exponential backoff on retry-able exceptions has been finalized. As per the audit, the implementation will proceed with the robust exponential backoff strategy, deferring the more complex `Retry-After` header parsing for post-MVP enhancements.

*   **Action Item from Audit:** A key documentation task from this blueprint is to add a clear comment block in `src/services/llm.py` above the `RETRYABLE_EXCEPTIONS` tuple, explicitly stating its dependency on the `google-generativeai` library's exception classes and the need to update it if the underlying LLM client changes.

---

## **Task/Feature Addressed: Task 4.3 - Integrate Remaining MVP Agents (QA, Research) into LangGraph Workflow**

This blueprint outlines the integration of the `ResearchAgent` and `QualityAssuranceAgent` into the main LangGraph workflow. This step enhances the intelligence of the system by enriching the context before content generation and validating the output after generation.

### **Overall Technical Strategy**

The integration will happen by defining new nodes and updating the graph's topology in `src/orchestration/cv_workflow_graph.py`.

1.  **Research Integration:** A `research_node` will be inserted immediately after the initial `parser_node`. It will use the parsed job description and CV to perform its analysis and populate the `research_findings` field in the `AgentState`. This ensures all subsequent generative agents have access to this enriched context.
2.  **Quality Assurance Integration:** A `qa_node` will be inserted after every generative step (e.g., after `content_writer_node`). It will inspect the newly generated content (identified by `current_item_id`), check it against quality criteria, and add its findings to the metadata of the corresponding `Item` in the `StructuredCV`. This provides immediate, granular feedback without altering the content directly.

The `run_as_node` methods for both agents will be implemented to conform to the LangGraph standard of accepting the `AgentState` and returning a dictionary of the updated state fields.

---

### **1. `ResearchAgent` Integration**

*   **Affected Component(s):**
    *   `src/agents/research_agent.py`
    *   `src/orchestration/cv_workflow_graph.py`

*   **Agent Logic Modifications (`research_agent.py`):**
    Implement the `run_as_node` method. This method will be the agent's entry point from the graph.

    ```python
    # src/agents/research_agent.py

    class ResearchAgent(AgentBase):
        # ... (existing methods) ...

        def run_as_node(self, state: AgentState) -> dict:
            """
            Executes the research logic as a LangGraph node.
            """
            logger.info("ResearchAgent node running.")

            if not state.job_description_data:
                logger.warning("ResearchAgent: No job description data to process.")
                return {}

            try:
                # The agent's core 'run' logic performs the research.
                # It requires both job data and the initial CV structure for context.
                findings = self.run({
                    "job_description_data": state.job_description_data.model_dump(),
                    "structured_cv": state.structured_cv.model_dump()
                })

                # Return the findings to be merged into the main state.
                return {"research_findings": findings}
            except Exception as e:
                logger.error(f"Error in ResearchAgent node: {e}", exc_info=True)
                return {"error_messages": state.error_messages + [f"Research failed: {e}"]}
    ```

---

### **2. `QualityAssuranceAgent` Integration**

*   **Affected Component(s):**
    *   `src/agents/quality_assurance_agent.py`
    *   `src/orchestration/cv_workflow_graph.py`

*   **Agent Logic Modifications (`quality_assurance_agent.py`):**
    Implement the `run_as_node` method. It will focus its analysis on the `current_item_id`.

    ```python
    # src/agents/quality_assurance_agent.py

    class QualityAssuranceAgent(AgentBase):
        # ... (existing methods) ...

        def run_as_node(self, state: AgentState) -> dict:
            """
            Executes quality checks on the most recently generated item.
            """
            logger.info(f"QAAgent node running for item: {state.current_item_id}")

            if not state.current_item_id:
                logger.warning("QAAgent: No current_item_id to check.")
                return {}

            try:
                # Perform a targeted check on the specific item
                qa_results = self.run({
                    "structured_cv": state.structured_cv,
                    "job_description_data": state.job_description_data
                }) # This agent's `run` logic should be refactored to focus on one item if needed

                # For this blueprint, we assume the QA agent can inspect the whole CV
                # but we will conceptually link its findings to the current item.
                # A more advanced QA agent would get the item_id as direct input.

                # Let's simulate adding QA metadata to the specific item
                updated_cv = state.structured_cv.model_copy(deep=True)
                item, _, _ = updated_cv.find_item_by_id(state.current_item_id)
                if item:
                    # In a real implementation, you'd extract item-specific results from qa_results
                    item.metadata["qa_check_passed"] = True
                    item.metadata["qa_timestamp"] = datetime.now().isoformat()

                return {"structured_cv": updated_cv}

            except Exception as e:
                logger.error(f"Error in QAAgent node: {e}", exc_info=True)
                return {"error_messages": state.error_messages + [f"QA failed: {e}"]}

    ```

---

### **3. LangGraph Workflow Refactoring**

Update the graph topology to include the new `research_node` and `qa_node` at the appropriate points.

*   **Affected Component(s):**
    *   `src/orchestration/cv_workflow_graph.py`

*   **Orchestrator/Workflow Changes:**

    1.  **Instantiate All Agents:** Ensure `ResearchAgent` and `QualityAssuranceAgent` are instantiated at the top of the file.
    2.  **Define New Node Functions:** Create the wrapper functions that call the agents' `run_as_node` methods.

        ```python
        # src/orchestration/cv_workflow_graph.py

        def research_node(state: AgentState) -> Dict:
            return research_agent.run_as_node(state)

        def qa_node(state: AgentState) -> Dict:
            # This is the same node function used after each generative step
            return qa_agent.run_as_node(state)
        ```
    3.  **Update Graph Edges:** Modify the graph definition to reflect the new, more intelligent flow.

        ```python
        # In build_cv_workflow_graph() function:

        # Add new nodes to the graph
        workflow.add_node("research_node", research_node)
        workflow.add_node("qa_node", qa_node) # QA node is generic

        # --- Define the new workflow topology ---

        # Entry point is still the parser
        workflow.set_entry_point("parser_node")

        # 1. Parse -> Research
        workflow.add_edge("parser_node", "research_node")

        # 2. Research -> Start content generation sequence
        workflow.add_edge("research_node", "generate_skills_node") # Or whatever the first generative step is

        # 3. Generate -> QA -> Review
        # This pattern repeats for each content type.
        # Example for the first item:
        workflow.add_edge("update_cv_with_skills_node", "process_next_item_node")
        workflow.add_edge("process_next_item_node", "content_writer_node")
        workflow.add_edge("content_writer_node", "qa_node") # <-- Insert QA after generation

        # The conditional edge now starts from the QA node, not the writer
        workflow.add_conditional_edges(
            "qa_node", # <-- Edge starts from QA
            route_after_review,
            {...}
        )
        ```

---

### **4. Testing Considerations**

*   **Unit Tests:**
    *   Test `ResearchAgent.run_as_node` to ensure it correctly calls the internal `run` method and returns a dictionary with the `research_findings` key.
    *   Test `QualityAssuranceAgent.run_as_node` to verify it modifies the `metadata` of the correct item within the `structured_cv` and returns the updated object.
*   **Integration Tests:**
    *   Test the `parser_node` -> `research_node` edge to ensure `research_findings` is populated in the state.
    *   Test the `content_writer_node` -> `qa_node` edge. Verify that after the `qa_node` runs, the `structured_cv` in the state contains new QA-related metadata on the `current_item_id`.
*   **E2E Testing:**
    *   A full workflow run should be performed. The state after the `research_node` should be inspected (if possible via logging or debugging) to confirm `research_findings` is populated. The final `StructuredCV` should have QA metadata on its generated items.

---

### **Critical Gaps & Questions**

*   **Consuming Research Data:** The `ContentWriterAgent` needs to be updated to actually *use* the `research_findings` from the `AgentState`. Its `_build_single_item_prompt` method should be refactored to accept `research_findings` as an argument and incorporate relevant insights (e.g., company values, industry terms) into the prompts it generates. This is a critical dependency for this task to have any real impact.
*   **Scope of QA for MVP:** For the MVP, the `QualityAssuranceAgent`'s role should be limited to *annotating* items with metadata (e.g., `item.metadata['qa_passed'] = False`, `item.metadata['qa_issues'] = ['Too short']`). It should **not** trigger an automatic regeneration loop. This prevents adding significant complexity. The UI can then use this metadata to display warnings to the user, who can then choose to regenerate the item manually.

---

The audit provides an exceptionally clear and strategically sound set of clarifications. It correctly identifies the potential pitfalls of a naive implementation and recommends an architectural approach that prioritizes robustness, user control, and MVP scope. The distinction between an "active" QA agent that forces loops and an "advisory" QA agent that annotates state is particularly crucial.

This audit doesn't require a change in the *tasks* of the blueprint, but it provides vital implementation details and constraints that must be integrated into the existing plan. I will now incorporate these clarifications directly into the blueprint for **Task 4.3**, ensuring the engineer has this guidance when building the feature.

# **TASK_BLUEPRINT.md (Revision 3)**

## **Task/Feature Addressed: Task 4.3 - Integrate Remaining MVP Agents (QA, Research) into LangGraph Workflow (with Audited Implementation Guidance)**

This blueprint outlines the integration of the `ResearchAgent` and `QualityAssuranceAgent` into the main LangGraph workflow. This revision incorporates critical feedback from a planning audit, ensuring that the integration is robust, adheres to MVP principles, and avoids common pitfalls like "runaway" AI loops.

---

### **Part 1: `ResearchAgent` Integration & Consumption**

The `ResearchAgent` provides deep context. This context must be consumed by the `ContentWriterAgent` to have a tangible impact on CV quality.

*   **Affected Component(s):**
    *   `src/agents/research_agent.py`
    *   `src/agents/enhanced_content_writer.py`
    *   `src/orchestration/cv_workflow_graph.py`
    *   `data/prompts/resume_role_prompt.md` (and other content generation prompts)

*   **Detailed Implementation Steps:**

    1.  **Implement `research_agent.run_as_node`:** As previously planned, this node will execute after the `parser_node` and populate the `research_findings` field in the `AgentState`.

    2.  **Modify `ContentWriterAgent` Prompting Logic:** This is the key implementation step for this feature. The agent's prompt-building methods (e.g., `_build_single_item_prompt`) must be refactored to accept the `research_findings` dictionary from the `AgentState`.

        ```python
        # src/agents/enhanced_content_writer.py

        class EnhancedContentWriterAgent(...):
            # ...
            def _build_single_item_prompt(self, item, section, subsection, job_data, research_findings, user_feedback):
                # This method now receives the research findings.

                # Extract specific insights from research_findings
                company_values = research_findings.get("company_values", [])
                industry_terms = research_findings.get("industry_terms", [])

                # ... existing prompt building logic ...

                # Add a new section to the prompt for CRITICAL CONTEXT
                prompt += "\n\n--- CRITICAL CONTEXT TO INCORPORATE ---\n"
                prompt += f"Company Values to Emphasize: {', '.join(company_values)}\n"
                prompt += f"Key Industry Terms to Use: {', '.join(industry_terms)}\n"
                prompt += "Instructions: Rewrite the content to subtly weave in these values and terms."

                return prompt
        ```

    3.  **Update Prompt Templates:** Modify the relevant prompt files in `data/prompts/` to include placeholders for this new context. For example, `resume_role_prompt.md` should be updated.

        ```markdown
        # data/prompts/resume_role_prompt.md

        ... (existing prompt content) ...

        **CRITICAL CONTEXT TO INCORPORATE:**
        Use the following research findings to refine the tone and content.

        **Company Values to Emphasize:**
        {{ company_values }}

        **Key Industry Terms to Use:**
        {{ industry_terms }}

        **Instructions:**
        1. Rewrite the original bullet point.
        2. Ensure it aligns with the job responsibilities.
        3. Subtly weave in the company's values and use the relevant industry terms.
        4. The output should be a single, refined bullet point.
        ```

    4.  **Update Graph Topology:** Ensure the `research_node` is placed correctly in `cv_workflow_graph.py` *before* any content writing nodes.

        ```python
        # src/orchestration/cv_workflow_graph.py

        # ... (node definitions) ...
        workflow.add_edge("parser_node", "research_node")
        workflow.add_edge("research_node", "generate_skills_node") # First generative step
        ```

*   **Testing Considerations:**
    *   **Unit Test:** Test the `ContentWriterAgent`'s `_build_single_item_prompt` method. Pass a mock `research_findings` dictionary and assert that the returned prompt string contains the "CRITICAL CONTEXT" section with the correct values populated.

---

### **Part 2: "Advisory" `QualityAssuranceAgent` Integration**

The `QualityAssuranceAgent` will be integrated as an "advisor" that annotates content with metadata rather than forcing regeneration loops. This is a critical MVP design decision to ensure stability and user control.

*   **Affected Component(s):**
    *   `src/agents/quality_assurance_agent.py`
    *   `src/orchestration/cv_workflow_graph.py`
    *   `src/core/main.py` (UI rendering)

*   **Detailed Implementation Steps:**

    1.  **Modify `Item` Model Metadata:** While no Pydantic change is needed, the *convention* for the `metadata` dictionary on the `Item` model will be established. The `QAAgent` will add the following keys:
        *   `qa_status`: "passed" | "warning" | "failed"
        *   `qa_issues`: `List[str]` (e.g., `["Content may be too short.", "Lacks quantifiable metrics."]` )

    2.  **Implement `qa_agent.run_as_node` as an Annotator:** Refactor the agent's node function. It **must not** alter the item's `content`. Its only job is to inspect the content of the `current_item_id` and add the `qa_status` and `qa_issues` keys to that item's `metadata` dictionary within the `structured_cv`.

        ```python
        # src/agents/quality_assurance_agent.py

        class QualityAssuranceAgent(AgentBase):
            # ...
            def run_as_node(self, state: AgentState) -> dict:
                logger.info(f"QAAgent node running for item: {state.current_item_id}")
                updated_cv = state.structured_cv.model_copy(deep=True)
                item, _, _ = updated_cv.find_item_by_id(state.current_item_id)

                if not item:
                    # ... (error handling) ...
                    return {"error_messages": ...}

                issues = []
                # Example Check 1: Length
                if len(item.content.split()) < 10:
                    issues.append("Content may be too short for impact.")
                # Example Check 2: Action Verbs
                if not item.content.lstrip().lower().startswith(("developed", "led", "managed", "optimized")):
                    issues.append("Consider starting with a strong action verb.")

                # Annotate the item's metadata
                if issues:
                    item.metadata['qa_status'] = 'warning'
                    item.metadata['qa_issues'] = issues
                else:
                    item.metadata['qa_status'] = 'passed'

                return {"structured_cv": updated_cv}
        ```

    3.  **Update Graph Topology:** Place the `qa_node` immediately after the `content_writer_node`. The conditional routing edge `route_after_review` will now originate from `qa_node`. This ensures QA runs before the UI is updated and presented to the user.

        ```python
        # src/orchestration/cv_workflow_graph.py
        # ...
        workflow.add_edge("content_writer_node", "qa_node")

        workflow.add_conditional_edges(
            "qa_node", # <-- The router now starts AFTER the QA check
            route_after_review,
            {...}
        )
        ```

    4.  **Update Streamlit UI:** The `display_subsection` or `display_item` function in `src/core/main.py` must be updated to read and display the QA metadata.

        ```python
        # src/core/main.py

        def display_subsection(subsection: Subsection):
            # ...
            # Inside the loop for rendering items:
            for item in subsection.items:
                # ...
                qa_status = item.metadata.get('qa_status')
                if qa_status == 'warning':
                    qa_issues = "\n- ".join(item.metadata.get('qa_issues', []))
                    st.warning(f"⚠️ **Quality Alert:**\n- {qa_issues}", icon="⚠️")
            # ...
        ```

*   **Testing Considerations:**
    *   **Unit Test `QAAgent`:** Pass a state object with a specific `current_item_id` and content. Assert that the returned `structured_cv` has the correct `qa_status` and `qa_issues` populated in the target item's metadata, and that the item's `content` remains unchanged.
    *   **E2E Test:** Run a full workflow. After a piece of content is generated, verify that a "Quality Alert" warning message appears in the UI if the content violates a QA rule (e.g., is too short).

---

### **Critical Gaps & Questions**

*   No new critical gaps identified. The audit has successfully clarified the implementation path, de-risking the feature by scoping the QA agent's role appropriately for the MVP and ensuring the research data is consumed effectively. The plan is now robust and ready for implementation.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 4.4 - Finalize LangGraph-Compatible Agent Interfaces**

This task is a final, crucial refactoring and standardization step. The goal is to ensure that every agent in the system rigorously adheres to the standard LangGraph-compatible interface. This will involve reviewing and modifying each agent's primary execution method (`run_as_node`) to guarantee it exclusively interacts with the rest of the system via the `AgentState` object. This creates a clean, predictable, and maintainable architecture.

---

### **Overall Technical Strategy**

The strategy involves a comprehensive audit and refactoring of all agent classes in `src/agents/`. Each agent's `run_as_node` method will be standardized to follow a strict pattern:

1.  **Input:** The method signature must be `run_as_node(self, state: AgentState) -> dict`. It will receive the *entire* current state of the workflow.
2.  **Processing:** The agent will read all necessary data directly from the input `state` object (e.g., `state.structured_cv`, `state.current_item_id`, `state.research_findings`). It will perform its core logic based on this data.
3.  **State Immutability:** The agent **must not** modify the input `state` object directly. It must work on copies of any complex objects it needs to change (e.g., `updated_cv = state.structured_cv.model_copy(deep=True)`).
4.  **Output:** The method must return a dictionary containing *only* the fields of the `AgentState` that it has created or modified. LangGraph will be responsible for merging this dictionary back into the main state.

This task also involves removing any legacy `run` methods or ensuring they are clearly marked as deprecated and not used by the core LangGraph workflow.

---

### **1. Agent Interface Standardization**

*   **Affected Component(s):**
    *   `src/agents/parser_agent.py`
    *   `src/agents/research_agent.py`
    *   `src/agents/enhanced_content_writer.py`
    *   `src/agents/quality_assurance_agent.py`
    *   `src/agents/cleaning_agent.py` (if created)
    *   `src/agents/formatter_agent.py`

*   **Detailed Implementation Steps (for each agent):**

    1.  **Review `run_as_node` Signature:** Ensure the method signature is exactly `def run_as_node(self, state: AgentState) -> dict:`. Remove any other parameters.

    2.  **Refactor Input Handling:** Go through the method and replace any direct parameter access with state access.
        *   **Before (Incorrect):** `def run_as_node(self, structured_cv, job_data):`
        *   **After (Correct):** `def run_as_node(self, state: AgentState): \n    structured_cv = state.structured_cv \n    job_data = state.job_description_data`

    3.  **Refactor Output Handling:** Ensure the `return` statement is a dictionary where keys are valid attribute names of the `AgentState` model.
        *   **Before (Incorrect):** `return updated_cv`
        *   **After (Correct):** `return {"structured_cv": updated_cv}`

    4.  **Enforce Immutability:** Search the code for any direct modifications to the input state (e.g., `state.structured_cv.sections.append(...)`). Replace all such instances with operations on a deep copy.
        *   **Implementation:**
            ```python
            updated_cv = state.structured_cv.model_copy(deep=True)
            # ... perform all modifications on updated_cv ...
            return {"structured_cv": updated_cv}
            ```

    5.  **Deprecate Old `run` Methods:** Locate any old `run` methods that are no longer used by the LangGraph workflow. Add a `DeprecationWarning` and a docstring indicating that `run_as_node` should be used instead.

        ```python
        import warnings

        def run(self, input_data: Any) -> Any:
            """
            DEPRECATED: This method is for legacy compatibility only.
            The LangGraph workflow uses run_as_node.
            """
            warnings.warn("The 'run' method is deprecated.", DeprecationWarning)
            # ... (optional: keep old logic for backward compatibility in tests) ...
            # In the final version, this can be removed entirely.
            pass
        ```

---

### **2. Example: Refactoring `ParserAgent`**

This example illustrates the required changes for the `ParserAgent`.

*   **Affected Component(s):** `src/agents/parser_agent.py`

*   **Refactoring Logic:**

    ```python
    # src/agents/parser_agent.py
    from src.orchestration.state import AgentState
    from typing import Dict

    class ParserAgent(AgentBase):
        # ...

        def run_as_node(self, state: AgentState) -> Dict:
            """
            Parses raw text from the state and populates structured data fields.
            This is the new, standardized entry point for the LangGraph workflow.
            """
            logger.info("ParserAgent node running.")

            try:
                # 1. Read input directly from the state object
                raw_jd = state.job_description_data.raw_text
                raw_cv = state.raw_user_cv_text # Assuming this field is populated at the start

                # 2. Perform core logic (this part remains the same)
                parsed_jd = self.parse_job_description(raw_jd)
                parsed_cv = self.parse_cv_text(raw_cv, parsed_jd)

                # 3. Return a dictionary of the updated state fields
                # This is the ONLY output of the function.
                return {
                    "job_description_data": parsed_jd,
                    "structured_cv": parsed_cv,
                    # Optionally, this node can populate the first processing queue
                    "items_to_process_queue": [item.id for item in parsed_cv.get_section_by_name("Key Qualifications").items],
                    "current_section_key": "key_qualifications"
                }
            except Exception as e:
                logger.error(f"Error in ParserAgent node: {e}", exc_info=True)
                # Append to error messages and return
                return {"error_messages": state.error_messages + [f"Parsing failed: {e}"]}

        # The old `run` method is now deprecated
        def run(self, input_data: dict) -> Dict:
            """
            DEPRECATED: Use run_as_node(state) instead.
            """
            import warnings
            warnings.warn("The ParserAgent 'run' method is deprecated.", DeprecationWarning)
            # For backward compatibility, you could simulate the old behavior
            # but for the MVP, we can assume it's no longer called by the main workflow.
            return {}

    ```

---

### **3. Testing Considerations**

*   **Refactor Existing Unit Tests:** All existing agent unit tests must be refactored. Instead of calling `agent.run(mock_input)`, they should now:
    1.  Create a mock `AgentState` object containing the necessary input data.
    2.  Call `agent.run_as_node(mock_state)`.
    3.  Assert that the returned dictionary contains the expected keys and that the values are correct.

*   **New Unit Tests for Immutability:**
    *   For each agent, write a test that passes a state object to `run_as_node`.
    *   After the call, assert that the *original* state object passed into the function has **not** been changed. This verifies that the agent is correctly working on a copy.

*   **Integration Tests:** The existing LangGraph integration tests are already aligned with this pattern and will serve as the primary validation that the refactored agents are working correctly within the graph. No major changes are needed here, but they must all pass after the agent refactoring.

---

### **Critical Gaps & Questions**

*   No new critical gaps are identified. This task is a pure refactoring effort to enforce architectural consistency. The primary risk is breaking existing tests, which is why a thorough refactoring of the test suite is a core part of this task. The successful completion of this task will significantly improve the long-term maintainability and debuggability of the agent system.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Phase 4 - Task 5.1 & 5.2 - Comprehensive Testing & NFR Validation**

This blueprint outlines the strategy for conducting comprehensive unit, integration, and End-to-End (E2E) testing for the AI CV Generator MVP. The primary goals are to achieve high test coverage, validate all functional requirements, and verify that critical Non-Functional Requirements (NFRs) such as performance and reliability are met.

---

### **Overall Technical Strategy**

The testing strategy is multi-layered:

1.  **Unit Testing:** Focus on isolating and testing individual components, especially agent logic, helper functions, and data models. Mocking will be used extensively to isolate dependencies (e.g., LLM API calls, file system access).
2.  **Integration Testing:** Test the interactions between components. The primary focus will be on testing short sequences of the LangGraph workflow to ensure that state transitions and data handoffs between nodes are correct.
3.  **End-to-End (E2E) Testing:** Use a framework like `pytest` with `asyncio` support to simulate the full user workflow, from submitting a job description to downloading a final PDF. These tests will run against a mocked LLM service to ensure predictable and fast execution in the CI/CD pipeline.

The `pytest` framework will be used for all test types, with clear directory separation (`tests/unit/`, `tests/integration/`, `tests/e2e/`) and appropriate markers (`@pytest.mark.unit`, `@pytest.mark.integration`, `@pytest.mark.e2e`).

---

### **Part 1: Unit Testing**

*   **Affected Component(s):**
    *   `tests/unit/` (New and existing files)
    *   All agent classes in `src/agents/`
    *   All service classes in `src/services/`
    *   All utility functions in `src/utils/`

*   **Detailed Implementation Steps:**

    1.  **Agent Logic Tests:**
        *   **`ParserAgent`:** Test `_parse_job_description_with_regex` with various malformed text inputs to ensure it extracts data gracefully. Test the main `run_as_node` by mocking the LLM call and asserting the output state is correct.
        *   **`EnhancedContentWriterAgent`:** Test the `_build_..._prompt` helper methods to ensure they correctly incorporate context from the `AgentState` (job data, research findings, user feedback). Mock the `llm_service` to test the fallback content generation logic.
        *   **`QualityAssuranceAgent`:** Test the `run_as_node` method with various `Item` content examples (e.g., too short, no action verb) and assert that the correct `qa_status` and `qa_issues` are added to the item's metadata.
        *   **`CleaningAgent`:** Test with various messy inputs (with/without JSON, with/without markdown fences) and assert that the `cleaned_output` is correct.

    2.  **Service Logic Tests:**
        *   **`LLMService`:** Write extensive tests for the `tenacity` retry logic as defined in the previous blueprint (Task 4.2). Mock the underlying `generate_content_async` call to throw different `google.api_core.exceptions` and assert that the retry behavior is correct.
        *   **`StateManager`:** Test saving and loading of `StructuredCV` objects to/from the file system. Test item update methods (`update_item_content`, `update_item_status`) and verify the changes are correctly reflected after a save/load cycle.

    3.  **Data Model Tests:**
        *   Test the `to_dict` and `from_dict` methods of the Pydantic models (`StructuredCV`, `Section`, etc.) to ensure successful serialization and deserialization, especially with enum fields.

---

### **Part 2: Integration Testing**

*   **Affected Component(s):**
    *   `tests/integration/` (New and existing files)
    *   `src/orchestration/cv_workflow_graph.py`

*   **Detailed Implementation Steps:**

    1.  **Test Graph Node Sequences:** Write tests for critical sequences in the LangGraph workflow. These are not full E2E tests but verify the data flow between a few connected nodes.
        *   **Test Parser -> Research Flow:** Create an initial state with raw text. Invoke the graph up to the `research_node`. Assert that the `AgentState` after the `research_node` has the `research_findings` field populated.
        *   **Test Generate -> Clean -> Update Flow:** Create a state where a generative node (e.g., `generate_skills_node`) has just run, populating a `raw_...` field. Invoke the graph for the next two steps (`clean_skills_node` and `update_cv_with_skills_node`). Assert that the final state's `structured_cv` has the correct, clean data populated in the final destination field (e.g., `big_10_skills`).
        *   **Test Feedback Loop:** Create a state representing the output of the `qa_node`. Update the state's `user_feedback` to simulate a "regenerate" action. Invoke the graph and assert that the next node executed is the `content_writer_node`.

---

### **Part 3: End-to-End (E2E) Testing**

*   **Affected Component(s):**
    *   `tests/e2e/` (New and existing files)
    *   `tests/e2e/conftest.py`

*   **Detailed Implementation Steps:**

    1.  **Create Mock LLM Service for E2E Tests:** In `tests/e2e/conftest.py`, create a fixture that provides a mocked `EnhancedLLMService`. This mock should return deterministic, pre-defined responses based on the input prompt content. This is crucial for making E2E tests fast and reliable.

        ```python
        # tests/e2e/conftest.py
        @pytest.fixture
        def mock_e2e_llm_service():
            mock_service = MagicMock(spec=LLMService)
            async def mock_generate(prompt: str, **kwargs) -> LLMResponse:
                # Logic to return a specific response based on keywords in the prompt
                if "generate a list of the 10 most relevant" in prompt.lower():
                    # Return a messy response for the cleaning agent to handle
                    raw = "Here are the skills:\n\n* Skill 1\n* Skill 2\n..."
                    return LLMResponse(content=raw, raw_response_text=raw, success=True)
                elif "clean the following" in prompt.lower():
                    # Return a clean response
                    raw = prompt.split('"""')[1]
                    return LLMResponse(content=raw, raw_response_text=raw, success=True)
                # ... other prompt conditions ...
                else:
                    return LLMResponse(content="Default generated content.", raw_response_text="Default generated content.", success=True)
            mock_service.generate_content = mock_generate
            return mock_service
        ```

    2.  **Write Full Workflow Test (`test_complete_cv_generation.py`):**
        *   This test will simulate the "happy path".
        *   **Setup:** Use the `mock_e2e_llm_service` fixture. Create an initial `AgentState` with a sample job description and CV text.
        *   **Execution:** Invoke the compiled LangGraph application (`cv_graph_app`) with the initial state.
        *   **Assertion:**
            *   Assert that the final state's `error_messages` list is empty.
            *   Assert that the `final_output_path` is populated and points to an existing PDF file.
            *   Assert that the `structured_cv` object contains generated content in all dynamic sections.
            *   Assert that the `big_10_skills` list contains 10 strings.

    3.  **Write Error Recovery E2E Test (`test_error_recovery.py`):**
        *   **Setup:** Configure the `mock_e2e_llm_service` to throw an exception (e.g., `google_exceptions.ResourceExhausted`) on the first call for a specific item, and succeed on the second call.
        *   **Execution:** Invoke the full workflow.
        *   **Assertion:** Assert that the workflow completes successfully (no errors in the final state) and that the logs show a retry attempt was made by `tenacity`. Verify that the final CV content is correct, proving the recovery was successful.

    4.  **Write NFR Validation Tests:**
        *   **Performance (REQ-NONFUNC-PERF-1):** Create a test that runs the full E2E workflow and measures the execution time. Assert that `execution_time < 30` seconds (using the mocked LLM, this should be very fast, validating the application logic's performance).
        *   **Reliability (REQ-NONFUNC-RELIABILITY-1):** The error recovery test above serves as a primary validation for this requirement.

---

### **Critical Gaps & Questions**

*   **E2E Test Determinism:** The biggest challenge in E2E testing AI systems is the non-deterministic nature of LLMs. The strategy outlined here, using a **fully mocked LLM service that returns predefined responses**, is the correct approach for ensuring reliable, repeatable tests in a CI/CD environment. Tests that call the actual Gemini API should be separated into a different suite (e.g., `tests/live_api/`) and run manually or on a schedule, as they are expected to be slower and may fail due to external factors.
*   **Test Data Management:** The `tests/e2e/test_data/` directory should be organized with sample inputs and corresponding expected outputs. This makes it easier to write validation logic. For example, for a given `job_description_1.txt` and `cv_1.txt`, there might be an `expected_summary_1.txt` to compare against.


---

The audit provides an excellent, industry-standard clarification of the testing strategy. It correctly emphasizes the critical distinction between deterministic logic testing (for CI/CD) and non-deterministic live API monitoring. The proposed test data directory structure is a best practice for managing test fixtures.

This feedback doesn't change the *what* of the testing plan, but it significantly refines the *how*. I will now integrate this detailed guidance into the blueprints for Tasks 5.1 and 5.2 to provide a more prescriptive and actionable plan for the engineer.

# **TASK_BLUEPRINT.md (Revision 4)**

## **Task/Feature Addressed: Task 5.1 & 5.2 - Comprehensive Testing & NFR Validation (with Audited Implementation Guidance)**

This blueprint provides a unified and detailed strategy for executing the full testing suite for the MVP. It incorporates key architectural decisions from the recent audit regarding test determinism and data management, ensuring a professional and maintainable testing framework.

---

### **Part 1: Test Data Fixture Management**

Before writing the tests, a clean and scalable structure for test data ("fixtures") must be established.

*   **Affected Component(s):**
    *   `tests/e2e/test_data/` (Directory Structure)

*   **Detailed Implementation Steps:**

    1.  **Create Scenario-Based Directories:** Inside `tests/e2e/test_data/`, create subdirectories for each distinct E2E test scenario. This approach isolates all data related to a single test case.

        ```
        tests/e2e/test_data/
        ├── scenario_1_happy_path/
        │   ├── input_cv.txt
        │   ├── input_jd.txt
        │   ├── mock_llm_parser_response.json
        │   ├── mock_llm_skills_response.txt
        │   └── mock_llm_experience_response.txt
        │
        ├── scenario_2_llm_fails_with_fallback/
        │   ├── input_cv.txt
        │   ├── input_jd.txt
        │   └── mock_llm_writer_error_response.json  # A response that will trigger a fallback
        │
        └── scenario_3_user_regenerates_item/
            ├── input_cv.txt
            ├── input_jd.txt
            ├── mock_llm_initial_response.txt
            └── mock_llm_regenerated_response.txt
        ```

    2.  **Populate Initial Scenarios:** Create the files for `scenario_1_happy_path` with representative content. The mock LLM responses should be examples of what a real LLM might return, including some conversational boilerplate for the cleaning agents to handle.

---

### **Part 2: E2E Test Implementation (`tests/e2e/`)**

The E2E suite tests the application's logic from start to finish using a **fully mocked LLM service**. This ensures the tests are deterministic, fast, and suitable for a CI/CD pipeline.

*   **Affected Component(s):**
    *   `tests/e2e/conftest.py`
    *   `tests/e2e/test_complete_cv_generation.py`
    *   `tests/e2e/test_error_recovery.py`

*   **Detailed Implementation Steps:**

    1.  **Create the Mock LLM Fixture:** In `tests/e2e/conftest.py`, create a `pytest` fixture that provides a mocked `EnhancedLLMService`. This mock will load its responses from the test data files based on the prompt it receives.

        ```python
        # tests/e2e/conftest.py
        import pytest
        from unittest.mock import MagicMock
        from src.services.llm import LLMService, LLMResponse

        @pytest.fixture
        def mock_e2e_llm_service(request):
            """A sophisticated mock that can load responses based on the test scenario."""
            mock_service = MagicMock(spec=LLMService)
            scenario_name = request.node.callspec.params.get("scenario") # Get scenario from pytest params

            async def mock_generate(prompt: str, **kwargs) -> LLMResponse:
                # Determine which mock file to load based on keywords in the prompt
                if "parse the following job description" in prompt.lower():
                    response_file = "mock_llm_parser_response.json"
                elif "generate a list of the 10 most relevant" in prompt.lower():
                    response_file = "mock_llm_skills_response.txt"
                # ... other conditions for different prompts ...
                else:
                    response_file = "mock_llm_experience_response.txt"

                # Load the mock response from the correct scenario directory
                file_path = Path("tests/e2e/test_data") / scenario_name / response_file
                with open(file_path, 'r') as f:
                    content = f.read()

                return LLMResponse(content=content, raw_response_text=content, success=True)

            mock_service.generate_content = mock_generate
            return mock_service
        ```

    2.  **Write the "Happy Path" E2E Test:** In `test_complete_cv_generation.py`, write a test that uses the mock service to validate the entire workflow.

        ```python
        # tests/e2e/test_complete_cv_generation.py

        @pytest.mark.parametrize("scenario", ["scenario_1_happy_path"])
        def test_full_workflow_happy_path(self, mock_e2e_llm_service, scenario):
            # 1. Setup: Patch the get_llm_service() function to return our mock
            with patch('src.agents.get_llm_service', return_value=mock_e2e_llm_service):
                # 2. Load input data from the scenario directory
                input_jd = Path(f"tests/e2e/test_data/{scenario}/input_jd.txt").read_text()

                # 3. Create initial AgentState
                initial_state = AgentState(job_description_data=JobDescriptionData(raw_text=input_jd), ...)

                # 4. Execute the graph
                final_state = cv_graph_app.invoke(initial_state)

                # 5. Assertions
                assert not final_state.get("error_messages")
                assert final_state["final_output_path"] is not None
                assert len(final_state["structured_cv"].big_10_skills) == 10
        ```

    3.  **Write Error Recovery E2E Test:** In `test_error_recovery.py`, create a test where the mock LLM is configured to fail initially.

        ```python
        # tests/e2e/test_error_recovery.py

        # In this test, the mock service would be configured to throw a retry-able exception
        # on its first call, then succeed on the second.
        def test_workflow_recovers_from_transient_api_error(...):
            # ... setup with a mock that fails once ...

            # Execute the graph
            final_state = cv_graph_app.invoke(...)

            # Assert that the final state is successful and the log contains a warning about the retry.
            assert not final_state.get("error_messages")
            # ... check logs for retry warning ...
        ```

---

### **Part 3: Live API Test Suite**

This suite is for monitoring, not for CI/CD. It will use the real Gemini API key.

*   **Affected Component(s):**
    *   `tests/live_api/test_live_llm_quality.py` (New Directory and File)
    *   `.github/workflows/daily_quality_check.yml` (New File - conceptual)

*   **Detailed Implementation Steps:**

    1.  **Create `tests/live_api/` directory.**
    2.  **Create `test_live_llm_quality.py`:** This file will contain tests that are marked with a custom pytest marker to separate them from the main test suite.

        ```python
        # tests/live_api/test_live_llm_quality.py
        import pytest
        import os

        # This test will only run if a specific environment variable is set
        # and the --run-live-api flag is passed to pytest.
        @pytest.mark.live_api
        @pytest.mark.skipif(not os.getenv("GEMINI_API_KEY"), reason="Requires live API key")
        def test_key_qualifications_prompt_quality(self):
            # This test does NOT use a mock. It initializes the real LLMService.
            llm_service = LLMService()
            prompt = ... # Build a real prompt
            response = llm_service.generate_content(prompt)

            # Assertions are "softer" - they check for quality, not exact matches.
            assert response.success is True
            assert len(response.content) > 50
            # A simple heuristic: check if it contains list-like structures
            assert response.content.count('\n') > 5
        ```

    3.  **Configure `pytest.ini`:** Add the custom marker.

        ```ini
        # pytest.ini
        [pytest]
        markers =
            live_api: marks tests that call the live Gemini API (slow, non-deterministic)
        ```

    4.  **Update CI/CD Script:** Ensure the main test command in the CI/CD pipeline *excludes* these tests.
        *   `pytest -m "not live_api"`

    5.  **Create Scheduled Workflow (Conceptual):** A separate GitHub Actions workflow can be created to run these tests on a schedule (e.g., daily).
        *   `pytest -m live_api`

---

### **Critical Gaps & Questions**

*   No critical gaps remain. This blueprint provides a clear, robust, and professional testing strategy that balances the need for deterministic logic validation with the need for live integration monitoring. The separation of concerns between mocked E2E tests and live API tests is the key to a stable and reliable development pipeline.

---

# TASK_BLUEPRINT.md

## **Task/Feature Addressed: Task 5.3 - Performance Tuning and Optimization**

This blueprint details the plan for profiling, analyzing, and optimizing the performance of the AI CV Generator. The goal is to ensure the application meets its non-functional performance requirements, specifically focusing on reducing latency and resource consumption in the end-to-end workflow.

---

### **Overall Technical Strategy**

The optimization strategy will be data-driven, focusing on three main areas:

1.  **Profiling:** Use Python's built-in `cProfile` and `pstats` modules to identify performance bottlenecks in the application logic, particularly within agent processing and data model transformations.
2.  **LLM Call Optimization:** Analyze the number, sequence, and payload of LLM calls. The primary goal is to reduce redundant calls and optimize prompts for faster responses and lower token usage. This includes implementing a caching layer for LLM responses.
3.  **Asynchronous Execution:** Ensure all I/O-bound operations, especially LLM API calls, are executed asynchronously using `asyncio` to prevent the main thread from blocking, thereby improving overall throughput.

---

### **1. Profiling and Bottleneck Identification**

*   **Affected Component(s):**
    *   `scripts/profiling_runner.py` (New File)
    *   `src/core/enhanced_orchestrator.py`

*   **Detailed Implementation Steps:**

    1.  **Create a Profiling Script:** Create a new script, `scripts/profiling_runner.py`, that can run a full E2E workflow and generate a performance profile. This script will programmatically execute the `EnhancedOrchestrator`'s workflow.

        ```python
        # scripts/profiling_runner.py
        import cProfile
        import pstats
        import asyncio
        from src.orchestration.cv_workflow_graph import cv_graph_app
        from src.orchestration.state import AgentState
        # ... import sample data ...

        def run_profiled_workflow():
            # Setup initial state with sample data
            initial_state = AgentState(...)

            # Run the graph invocation under the profiler
            cv_graph_app.invoke(initial_state.model_dump())

        if __name__ == "__main__":
            profiler = cProfile.Profile()
            profiler.enable()

            run_profiled_workflow()

            profiler.disable()
            stats = pstats.Stats(profiler).sort_stats('cumulative')
            stats.print_stats(30) # Print the top 30 cumulative time offenders
            stats.dump_stats('logs/performance/workflow_profile.pstat')
        ```

    2.  **Analyze Profile Results:** Execute the profiling script and analyze the output (`workflow_profile.pstat`) using a visualizer like `snakeviz` (`snakeviz logs/performance/workflow_profile.pstat`). Identify functions with high cumulative time (`cumtime`). Common suspects will be:
        *   LLM API call execution (`requests` or `httpx` methods).
        *   Pydantic model validation (`model_validate`).
        *   JSON serialization/deserialization.
        *   File I/O in `StateManager`.

---

### **2. LLM Call and Prompt Optimization**

This is the most critical area for performance improvement.

*   **Affected Component(s):**
    *   `src/services/llm.py`
    *   `src/agents/*` (all agents making LLM calls)

*   **Detailed Implementation Steps:**

    1.  **Implement Caching in `LLMService`:** Introduce a caching layer to store and retrieve LLM responses for identical prompts. This dramatically reduces latency and cost for repeated actions like regenerating an item multiple times.

        ```python
        # src/services/llm.py
        import functools

        # Simple in-memory cache using functools.lru_cache
        # The key will be a hash of the prompt and model name.
        @functools.lru_cache(maxsize=128)
        async def _cached_llm_call(prompt_hash: str, llm_service_instance, prompt: str):
            # The actual call logic is now wrapped
            return await llm_service_instance._make_llm_api_call(prompt)

        class EnhancedLLMService:
            async def generate_content(self, prompt: str, ...):
                # ...
                prompt_hash = self._create_cache_key(prompt, self.model_name)
                try:
                    # Use the cached wrapper function
                    raw_text, full_response = await _cached_llm_call(prompt_hash, self, prompt)
                    logger.info(f"LLM call cache hit for hash: {prompt_hash}")
                except Exception as e:
                    # ... handle error ...
                # ...
        ```

    2.  **Review and Condense Prompts:** Audit all prompts in `data/prompts/`.
        *   Remove redundant instructions.
        *   Shorten examples where possible.
        *   Ensure instructions are at the top of the prompt for faster processing by the LLM.
        *   **Action:** Reduce the token count of each prompt by an average of 10-15% without sacrificing quality.

    3.  **Batch LLM Calls (Post-MVP consideration):** While likely out of scope for the MVP, a key future optimization is to batch multiple, independent content generation requests into a single API call if the LLM API supports it. This is a note for future architectural improvement, not immediate implementation.

---

### **3. Asynchronous Execution Verification**

Ensure all I/O-bound operations are non-blocking.

*   **Affected Component(s):**
    *   All agent and service classes that perform I/O.

*   **Detailed Implementation Steps:**

    1.  **Audit I/O Calls:** Systematically review the codebase for any blocking I/O calls within `async` functions.
        *   **Correct:** `await self.llm.generate_content_async(...)`
        *   **Incorrect:** `time.sleep(5)` (use `await asyncio.sleep(5)` instead).
        *   **Correct:** `async with aiofiles.open('file.txt', mode='r') as f: ...`
        *   **Incorrect:** `with open('file.txt', mode='r') as f: ...` (inside an `async def` method).

    2.  **Refactor Blocking Calls:** Replace any identified blocking calls with their asynchronous equivalents. If an async version of a library is not available, run the blocking call in a separate thread using `asyncio.to_thread`.

        ```python
        # Example of wrapping a sync file write in an async function
        import aiofiles

        async def save_state_async(data, file_path):
            async with aiofiles.open(file_path, 'w') as f:
                await f.write(json.dumps(data))
        ```

---

### **4. Testing Considerations**

*   **Benchmarking:** Create a benchmark script (e.g., `scripts/benchmark.py`) that runs the E2E workflow multiple times before and after optimizations. This script should record key metrics like total execution time, memory usage, and the number of LLM calls.
*   **Cache Invalidation Test:** Write a unit test to verify that the LLM cache works. Call `generate_content` with the same prompt twice; assert that the mocked API call function is only called once. Then, call it with a different prompt and assert the mock is called again.
*   **Load Testing (Post-MVP):** For future validation, use a tool like `locust` to simulate multiple concurrent users interacting with the API (if an API is exposed) to test the system's scalability and ensure the `asyncio` implementation correctly handles concurrent requests.

---

### **Critical Gaps & Questions**

*   No critical gaps are identified. This plan provides a standard, structured approach to performance optimization. The key is the methodical process: **Profile -> Identify -> Optimize -> Benchmark**. Rushing to optimize without profiling often leads to wasted effort on non-bottlenecks.

The current blueprint for **Task 5.3** is solid and covers the essential pillars of performance tuning: profiling, LLM optimization, and asynchronous execution. However, to make it even more robust and aligned with production-readiness, we can add a crucial dimension: **Data Model and Serialization Optimization**. Pydantic models are central to this application, and their performance can be a significant factor.

I will now add this new section to the blueprint.

# **TASK_BLUEPRINT.md (Revision 5)**

## **Task/Feature Addressed: Task 5.3 - Performance Tuning and Optimization (Extended)**

This blueprint details the plan for profiling, analyzing, and optimizing the performance of the AI CV Generator. This extended version adds a new focus area: optimizing the extensive use of Pydantic models for data validation and serialization, which can become a performance bottleneck in data-heavy applications.

---

### **Part 1: Profiling and Bottleneck Identification (No Change)**

*(The strategy outlined in the previous response remains the same: use `cProfile` and `snakeviz` to identify hotspots.)*

---

### **Part 2: LLM Call and Prompt Optimization (No Change)**

*(The strategy of implementing caching and reviewing prompts remains the same.)*

---

### **Part 3: Asynchronous Execution Verification (No Change)**

*(The strategy of auditing for blocking I/O calls remains the same.)*

---

### **Part 4: Data Model and Serialization Optimization (New Section)**

**Overall Strategy:** Pydantic is used heavily for data validation and contract enforcement, which is excellent for robustness. However, creating and validating complex nested models (like `StructuredCV`) repeatedly within the LangGraph workflow can be computationally expensive. This part of the optimization will focus on minimizing unnecessary validation and serialization overhead.

*   **Affected Component(s):**
    *   `src/models/data_models.py`
    *   All agents and LangGraph nodes that interact with `AgentState`

*   **Detailed Implementation Steps:**

    1.  **Selective Model Validation:**
        *   **Problem:** Every time a node in LangGraph receives the `AgentState`, Pydantic implicitly re-validates the entire state object, including the large `StructuredCV`. This is redundant if only a small part of the state has changed.
        *   **Action:** In agents that only read from the state or make minor changes, avoid full model re-validation. Instead of creating a new `AgentState` object from a dictionary, pass the existing object and work with its attributes directly (while still respecting immutability by copying objects that will be modified).
        *   **Refactor `run_as_node` methods:** Where possible, ensure that nodes which only read data do not trigger a full `AgentState.model_validate()`. LangGraph's internal mechanics can sometimes trigger this; the key is to ensure our own code doesn't do it unnecessarily.

    2.  **Optimize `StructuredCV` Instantiation:**
        *   **Problem:** The `parser_agent` currently parses the raw CV text and then constructs a complex, nested `StructuredCV` object from scratch. This can be slow if the CV is large.
        *   **Action:** Profile the `StructuredCV.from_dict` or `StructuredCV.model_validate` methods. If they are identified as a bottleneck, consider optimizing the parsing logic to build the object more efficiently, perhaps by creating `Item` and `Subsection` objects incrementally rather than building a large dictionary first.

    3.  **Use Optimized Serializers (If Necessary):**
        *   **Problem:** The default `json` library can be slow. For a data-intensive application, serialization between the Streamlit frontend and the backend (or for saving state) can be a bottleneck.
        *   **Action:** If profiling shows significant time spent in `json.dumps` or `json.loads`, consider replacing the standard library with a faster alternative like `orjson`. This requires updating any code that performs JSON operations, particularly in the `StateManager` and API layers.
        *   **Implementation Example:**
            ```python
            # src/core/state_manager.py
            # import json -> import orjson

            def save_state(self):
                # ...
                # json.dump(...) -> f.write(orjson.dumps(data_dict))

            def load_state(self):
                # ...
                # json.load(...) -> orjson.loads(f.read())
            ```

---

### **5. Testing Considerations (Updated)**

*   **Benchmarking (Updated):** The `scripts/benchmark.py` script is now even more critical. It should be run before and after the Pydantic/serialization optimizations to quantify the performance gain. The benchmark should measure not only total time but also memory allocation, as efficient model handling can reduce memory churn.
*   **New Unit Tests:**
    *   Write a benchmark-style unit test specifically for the instantiation of a large, complex `StructuredCV` object to measure its creation time. This can be used to validate optimizations made in **Part 4.2**.
    *   If `orjson` is implemented, write a test to ensure it correctly serializes and deserializes the `AgentState` object, especially handling complex types like `UUID` and `datetime` correctly.

---

### **6. Potential Challenges & Critical Considerations (Updated)**

*   **Validation vs. Performance Trade-off:** The primary reason for using Pydantic is for its robust validation. While optimizing, it's crucial not to accidentally disable validation where it is needed most (e.g., at API boundaries or when receiving data from an LLM). The goal is to avoid *redundant* validation within trusted, internal parts of the workflow.
*   **`orjson` Compatibility:** `orjson` is generally a drop-in replacement but may have different default behaviors for certain data types (like enums or datetimes). The switch requires careful testing to ensure data integrity is maintained.

---

### **7. Critical Gaps & Questions**

*   No new critical gaps identified. This addition makes the performance tuning plan more holistic by addressing the data-layer performance, which is a common oversight. The plan remains focused on measurable improvements guided by profiling data.

---

The performance tuning blueprint is now comprehensive, covering application logic, LLM interactions, asynchronous execution, and data model performance. There are no major gaps. The next logical and final step in the plan is to address documentation and deployment preparation, which leads directly into **Task 5.4 and 5.5**.

This blueprint will cover both, as they are tightly related aspects of finalizing the MVP for release.

# **TASK_BLUEPRINT.md (Revision 6)**

## **Task/Feature Addressed: Task 5.4 & 5.5 - Finalize Documentation and Prepare for Deployment**

This blueprint outlines the final steps required to prepare the AI CV Generator MVP for a production-like deployment. It covers the creation of comprehensive user and developer documentation, and the finalization of the containerization setup to ensure a smooth, reproducible deployment process.

---

### **Part 1: Documentation (Task 5.4)**

**Overall Strategy:** Create two distinct sets of documentation: one for end-users of the Streamlit application and one for developers who will maintain or extend the system. All documentation should be written in Markdown and stored within the `/docs` directory.

*   **Affected Component(s):**
    *   `README.md` (Update)
    *   `/docs/user_guide.md` (New File)
    *   `/docs/developer_guide.md` (New File)
    *   `/docs/architecture.md` (New File)

*   **Detailed Implementation Steps:**

    1.  **Update `README.md`:** This is the project's front door. It needs to be a concise, high-level overview.
        *   **Action:** Review and update the "Features," "Getting Started," and "Usage Guide" sections to reflect the final MVP functionality.
        *   **Action:** Ensure the installation instructions (both local venv and Docker) are accurate and have been tested on a clean machine.
        *   **Action:** Add a "Project Structure" section that briefly explains the purpose of the main directories (`src/agents`, `src/orchestration`, `data/prompts`, etc.).
        *   **Action:** Add links to the more detailed guides in the `/docs` directory.

    2.  **Create User Guide (`/docs/user_guide.md`):** This document is for non-technical end-users.
        *   **Content:**
            *   **Introduction:** A simple explanation of what the application does.
            *   **Step-by-Step Walkthrough:** Guide the user through a complete session with screenshots.
                *   Entering a Job Description and CV.
                *   Understanding the "Review & Edit" screen.
                *   Explaining the "Accept" and "Regenerate" buttons and what they do.
                *   Explaining the "Big 10 Skills" and "Raw LLM Output" sections.
                *   How to export the final PDF.
            *   **Troubleshooting:** Simple answers to common questions (e.g., "What if the content is not good?", "Why is my API key not working?").

    3.  **Create Developer Guide (`/docs/developer_guide.md`):** This document is for engineers.
        *   **Content:**
            *   **Development Setup:** Detailed instructions on setting up the development environment, including installing system dependencies for `WeasyPrint`.
            *   **Running Tests:** Commands for running unit, integration, and E2E tests (`pytest -m "not live_api"`). Explain how to run the live API tests (`pytest -m live_api`).
            *   **Code Style & Linting:** Instructions on using `black` and `flake8`.
            *   **Adding a New Agent:** A step-by-step guide on how to create a new agent class, define its `run_as_node` method, and integrate it into the `cv_workflow_graph.py`.
            *   **Adding a New Prompt:** Instructions on where to add new `.md` prompt files and how to load them in an agent.

    4.  **Create Architecture Overview (`/docs/architecture.md`):** A high-level technical document.
        *   **Content:**
            *   **High-Level Diagram:** A simple flowchart showing the main components (Streamlit UI, LangGraph Orchestrator, Agents, LLM Service, State Manager).
            *   **LangGraph Workflow:** A detailed explanation of the `cv_workflow_graph.py`, describing each node and the conditional logic of the `route_after_review` edge. This is crucial for understanding the application's flow.
            *   **State Management:** An explanation of how `AgentState` is used as the single source of truth and how it's passed between the UI and the graph.
            *   **Key Design Patterns:** Briefly explain the "Generate -> Clean -> Update State" pattern and the "Advisory QA" model.

---

### **Part 2: Deployment Preparation (Task 5.5)**

**Overall Strategy:** Ensure the `Dockerfile` is complete, secure, and optimized for production. This includes verifying system dependencies, creating a non-root user, and ensuring the container starts correctly.

*   **Affected Component(s):**
    *   `Dockerfile`
    *   `docker-compose.yml`

*   **Detailed Implementation Steps:**

    1.  **Finalize `Dockerfile`:**
        *   **Verify Dependencies:** Confirm that the `apt-get install` command includes all necessary libraries for `WeasyPrint` (`build-essential`, `libpango-1.0-0`, `libcairo2`, `libpangocairo-1.0-0`). This was addressed in the prerequisite task.
        *   **Security Best Practices:**
            *   Confirm that the application runs as a **non-root user**. The current `Dockerfile` correctly creates and switches to an `aicvgen` user. This is a critical security measure.
            *   Ensure there are no hardcoded secrets or API keys in any layer of the Docker image. The use of `.env` files and environment variables at runtime is the correct approach.
        *   **Optimization:** The existing multi-stage build (`builder` and `production` stages) is a best practice and should be kept. It ensures the final image is as small as possible.
        *   **Entrypoint:** Verify the `CMD ["python", "run_app.py"]` command is correct and successfully starts the Streamlit application.

    2.  **Review `docker-compose.yml`:**
        *   **Service Definition:** Review the `aicvgen` service definition. Ensure the port mapping (`8501:8501`), volume mounts (`./data:/app/data`, `./logs:/app/logs`), and `env_file` configuration are correct for a standard deployment.
        *   **Health Check:** The existing health check (`curl -f http://localhost:8501/_stcore/health`) is a good practice for container orchestration systems to know when the app is ready. Verify this endpoint is available in Streamlit.
        *   **Remove Unused Services for MVP:** The current `docker-compose.yml` contains optional services for `nginx`, `redis`, `prometheus`, and `grafana` under different profiles. For the MVP deployment documentation, the focus should be solely on the `aicvgen` service. The other services should be clearly documented as optional, post-MVP enhancements.

---

### **Testing Considerations**

*   **Documentation Testing:**
    *   Have a team member who is *not* the primary developer follow the `README.md` and `developer_guide.md` instructions on a completely clean machine to set up the project. This is the best way to find gaps in the documentation.
*   **Deployment Testing:**
    *   On a clean machine with only Docker installed, run `docker-compose up --build`.
    *   Once the container is running, access `http://localhost:8501` and perform a full E2E test run to ensure all functionality, including PDF export, works correctly within the container. This validates that all dependencies have been correctly installed.

---

### **Critical Gaps & Questions**

*   No critical gaps are identified. This blueprint finalizes the project for an MVP release by ensuring it is well-documented and can be reliably deployed. The successful completion of these tasks marks the end of the planned development phases.

---

# **TASK_BLUEPRINT.md (Final Handoff)**

## **Conclusion of Planning Phase & Handoff for Implementation**

The technical planning and blueprinting phase for all tasks outlined in the "Unified MVP Refactoring & Development Plan" is now complete. The preceding series of `TASK_BLUEPRINT.md` responses constitutes the full, detailed, and actionable technical specification for the implementation of the AI CV Generator MVP.

This final document serves as a comprehensive summary and official handoff to the `Senior Python/AI Engineer AI` for execution.

---

### **Summary of Key Architectural Directives**

The implementation must adhere to the following core architectural principles established throughout the blueprints:

1.  **LangGraph as the Core Orchestrator:** The entire CV generation and review process is a state machine managed by the compiled `cv_graph_app`. All business logic must be encapsulated within graph nodes.
2.  **`AgentState` as the Single Source of Truth:** All data flows through the `AgentState` object (`src/orchestration/state.py`). Agents and nodes read from this state and return dictionaries of the fields they have modified. Direct state mutation is strictly forbidden.
3.  **The "Generate -> Clean -> Update" Pattern:** All generative LLM calls must be followed by a dedicated cleaning step. The generative node places its raw output into a temporary state field (e.g., `raw_skills_output`), a `CleaningAgent` node processes it into a structured format and places it in `cleaned_output`, and a final utility node updates the main `StructuredCV` model.
4.  **"Advisory" Quality Assurance:** The `QualityAssuranceAgent`'s role in the MVP is to *annotate* content items with metadata (`qa_status`, `qa_issues`). It does **not** trigger automatic regeneration loops. The UI will use this metadata to display warnings, but the user remains in full control of accepting or regenerating content.
5.  **Stateless UI Interaction Model:** The Streamlit application is a pure "view" of the current `AgentState`. User actions (button clicks) update the `user_feedback` field in `st.session_state.agent_state` via `on_click` callbacks. The main script loop then triggers a new invocation of the LangGraph application with this updated state. The graph does not "pause".

---

### **Implementation Sequence**

The generated blueprints cover all pending tasks from Phase 2 through Phase 5. The engineer is instructed to follow this sequence:

1.  **Task 3.1 & 3.2 & 3.4 (Unified):** Implement the granular processing workflow, the "Big 10" skills generation, and the raw output display. This involves creating the `CleaningAgent` and refactoring the `ContentWriterAgent` and LangGraph workflow as detailed in the prior blueprints.
2.  **Task 3.3 & 5.5 (Unified):** Implement PDF output generation, including the `FormatterAgent` logic and the required updates to the `Dockerfile` and `README.md` for system dependencies (`WeasyPrint`).
3.  **Task 3.5:** Implement the Streamlit UI for hybrid control, enabling the user to interact with the item-by-item workflow.
4.  **Task 4.1 & 4.2 (Unified):** Implement the robust fallback logic in the `ParserAgent` and `ContentWriterAgent`, and the `tenacity`-based retry mechanism in the `LLMService`.
5.  **Task 4.3:** Fully integrate the `ResearchAgent` and the "advisory" `QualityAssuranceAgent` into the LangGraph workflow.
6.  **Task 4.4:** Perform the final agent interface refactoring to ensure all agents strictly adhere to the `run_as_node(state)` signature.
7.  **Task 5.1, 5.2, 5.3:** Execute the comprehensive testing and performance tuning strategy.
8.  **Task 5.4:** Complete all user and developer documentation.

The implementation is now ready to proceed, starting with the foundational prerequisite tasks (renaming prompts, creating the JSON cleaner) followed by the core implementation of Task 3.1.

---

### **Final Critical Gaps & Questions**

This final review identifies one area of potential ambiguity in the codebase that requires clarification before proceeding to full E2E testing and deployment.

*   **Ambiguity in Application Entry Point:**
    *   **Observation:** There appears to be a conflict in the application's intended entry point. The `run_app.py` script is configured to launch a FastAPI application via `streamlit run src/api/main.py`. However, the primary application file, `app.py`, is configured to launch a pure Streamlit application via `src/core/main.py`.
    *   **Impact:** This creates ambiguity about whether the primary user interface is intended to be served via Streamlit directly or through a FastAPI backend. This will affect how the UI interacts with the LangGraph orchestrator and how the final application is deployed.
    *   **Recommendation:** For the MVP, the focus should be on the pure Streamlit application as defined in `app.py` and detailed in the UI-focused blueprints. The `run_app.py` script should be updated to match this, removing the dependency on `src/api/main.py` for the initial release. The FastAPI components can be considered for future versions that may require a dedicated API.
    *   **Action for Engineer:** Modify `run_app.py` to execute `app.py` or `src/core/main.py` directly using `streamlit run`.

        ```python
        # Proposed change for run_app.py
        # ... (sys.path setup) ...
        sys.argv = [
            "streamlit",
            "run",
            str(project_root / "app.py"), # Point to the main Streamlit launcher
            "--server.port=8501"
        ]
        stcli.main()
        ```

This concludes the planning phase. The technical blueprints are complete and ready for execution.

---

The audit provides an excellent and accurate analysis of the architectural ambiguity. The recommendation to focus solely on the "Pure Streamlit Architecture" for the MVP is strategically sound and aligns perfectly with the goal of delivering a stable product efficiently. The clarification of how Streamlit's execution model differs from a traditional client-server (FastAPI) setup is crucial for the implementation team.

This audit validates the final action item identified in the previous blueprint. Therefore, the next logical step is to create a blueprint for executing this final prerequisite task. This will be the last task before the project is considered fully planned and ready for the implementation of the remaining phases.

# **TASK_BLUEPRINT.md (Final Prerequisite Task)**

## **Task/Feature Addressed: Task 5.5 Prerequisite - Resolve Application Entry Point Ambiguity**

This blueprint details the final refactoring task required to unify the application's architecture before proceeding with further development and testing. It addresses the critical ambiguity between the pure Streamlit and the Streamlit+FastAPI entry points, ensuring a single, clear, and consistent execution path for the MVP.

---

### **Overall Technical Strategy**

The strategy is to definitively commit to the "Pure Streamlit Architecture" for the MVP. This will be achieved by:
1.  Modifying the `run_app.py` script to serve as the official launcher for the main Streamlit application file (`app.py`).
2.  Deprecating the entire `src/api` directory by moving it to the `src/obsolete` folder, clearly signaling that the FastAPI components are not part of the MVP.
3.  Updating the `docker-compose.yml` to remove any references to the now-obsolete API entry point, ensuring containerized deployments use the correct architecture.
4.  Updating the `README.md` to reflect the single, correct way to run the application.

This will eliminate confusion, simplify the development process, and align the codebase with the strategic decision to defer a separate API backend.

---

### **1. `run_app.py` Refactoring**

*   **Affected Component(s):**
    *   `run_app.py`

*   **Detailed Implementation Steps:**
    *   Modify `run_app.py` to programmatically execute the `streamlit run app.py` command. This makes `run_app.py` the single, correct entry point for all developers.

    ```python
    # run_app.py (Updated Content)

    #!/usr/bin/env python3
    """
    Official entry point for the AI CV Generator Streamlit application.
    This script ensures the Python path is correctly configured and runs the app.
    """
    import sys
    from pathlib import Path
    from streamlit.web import cli as stcli

    # Add the project root to the Python path to allow absolute imports from `src`
    project_root = Path(__file__).parent
    sys.path.insert(0, str(project_root))

    if __name__ == "__main__":
        # Programmatically construct the command: streamlit run app.py --server.port=8501
        # This ensures we are launching the pure Streamlit application defined in app.py
        sys.argv = [
            "streamlit",
            "run",
            str(project_root / "app.py"),
            "--server.port=8501"
        ]

        # Execute the Streamlit CLI with the constructed arguments
        stcli.main()
    ```

*   **Rationale:** This change makes `python run_app.py` the canonical way to start the application, removing any doubt about which script to run. It correctly points to `app.py`, which is the intended pure Streamlit launcher.

---

### **2. Deprecate the API Directory**

*   **Affected Component(s):**
    *   `src/api/` (Directory)
    *   `src/obsolete/` (Directory)

*   **Detailed Implementation Steps:**
    *   Move the entire `src/api/` directory into the `src/obsolete/` directory.
    *   **New Structure:** `src/obsolete/api/`
    *   Add a `README.md` file inside `src/obsolete/api/` explaining its status.

        ```markdown
        # /src/obsolete/api/README.md

        This directory contains the legacy FastAPI application components. For the MVP, a pure Streamlit architecture was chosen to simplify development and deployment. These files are kept for future reference in case a dedicated API backend is required for post-MVP versions of the application. They are not used by the current application.
        ```

*   **Rationale:** This physically removes the unused and confusing code from the active source tree while preserving it for potential future use, providing a clear signal to developers about the current architecture.

---

### **3. `docker-compose.yml` Update**

*   **Affected Component(s):**
    *   `docker-compose.yml`

*   **Detailed Implementation Steps:**
    *   Review the `aicvgen` service definition. The existing `Dockerfile` and `docker-compose.yml` already seem to correctly use `run_app.py` as the entry point. This step is a final verification. If the `command` or `CMD` were pointing to `src/api/main.py`, they would need to be changed to `python run_app.py`. The current files appear correct, but this verification is mandatory.

    ```yaml
    # docker-compose.yml (Verification)
    services:
      aicvgen:
        build:
          context: .
          dockerfile: Dockerfile
        # ... other configs ...
        # The Dockerfile's CMD ["python", "run_app.py"] will be used.
        # This is correct as long as Dockerfile has the right CMD.
    ```
    ```dockerfile
    # Dockerfile (Verification)
    # ...
    # Default command to run the Streamlit app
    CMD ["python", "run_app.py"] # This is correct. No changes needed.
    ```

---

### **4. Documentation Update**

*   **Affected Component(s):**
    *   `README.md`

*   **Detailed Implementation Steps:**
    *   Review the "Quick Installation" and "Usage" sections of the `README.md`.
    *   Ensure that the **only** command listed for running the application is `python run_app.py`.
    *   Remove any references to `uvicorn` or running `src/api/main.py`.

    ```markdown
    # README.md (Example Snippet)

    4. **Configure environment:**
    ```bash
    # ...
    ```

    5. **Run the application:**
    ```bash
    # Use the official launcher script
    python run_app.py
    ```
    ```

---

### **Testing Considerations**

*   **Local Execution:** After making all changes, run `python run_app.py` from the project root. The Streamlit application should launch successfully at `http://localhost:8501`.
*   **Docker Execution:** Run `docker-compose up --build --force-recreate`. Access the application via the mapped port and confirm it runs correctly. This validates that the entire chain (`docker-compose` -> `Dockerfile` -> `run_app.py` -> `app.py` -> `src/core/main.py`) is working as intended.

---

### **Conclusion of Planning**

With the completion of this final prerequisite task, the project's architecture is now unified and unambiguous. All identified inconsistencies have been addressed, and a clear, single path for development and execution has been established. The project is now fully planned and ready for the implementation of the remaining functional features as detailed in the preceding blueprints for Phases 3 through 5.