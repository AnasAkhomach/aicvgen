# TASK_BLUEPRINT.md

## **AI CV Generator: MVP Refactoring and Feature Implementation Plan**

### **1. Technical Strategy Overview**

This document outlines the comprehensive technical blueprint for refactoring the `aicvgen` project into a stable, feature-complete Minimum Viable Product (MVP). The plan is based on the requirements specified in `SRS Update for MVP_.txt` (v1.3) and addresses critical issues identified in the codebase, including data model inconsistencies, security vulnerabilities, and agent processing failures.

The core strategy is divided into three prioritized phases:
1.  **Foundational Stabilization:** Remediate critical security flaws, fix core agent bugs, and establish strong data contracts using Pydantic. This creates a reliable base for development.
2.  **MVP Core Feature Implementation:** Implement the primary user-facing features of the MVP, with a focus on the architecturally significant shift to a granular, item-by-item processing workflow.
3.  **"Smart Agent" Logic & Resilience:** Enhance agents with robust fallback mechanisms and prepare the system for future scalability with LangGraph-compatible interfaces.

This plan prioritizes stability, maintainability, and adherence to the MVP requirements, ensuring the delivery of a reliable and robust product.

---

### **2. Phase 1: Foundational Stabilization & Critical Fixes**

**Goal:** Resolve critical security vulnerabilities and core operational blockers; establish stable data foundations with Pydantic; implement secure and informative logging.

#### **2.1. Task: Remediate API Key Logging & Implement Secure Logging**

-   **Task/Feature Addressed:** Critical security vulnerability (CI-001) where API keys are logged in plain text. Implements REQ-NONFUNC-SECURITY-1.
-   **Affected Component(s):** `src/config/logging_config.py`, `src/utils/security_utils.py`, and any module that logs configuration objects (e.g., `src/integration/enhanced_cv_system.py`).
-   **Pydantic Model Changes:** None.
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:** None.
-   **Detailed Implementation Steps:**
    1.  **Enhance Security Utility:** In `src/utils/security_utils.py`, ensure the `redact_sensitive_data` function can recursively traverse nested dictionaries and lists, redacting values for keys matching a predefined sensitive list (e.g., `api_key`, `secret`, `token`).
    2.  **Create a Redaction Filter:** In `src/config/logging_config.py`, create a new `logging.Filter` class named `SensitiveDataFilter`.
    3.  In the `filter` method of `SensitiveDataFilter`, check if the log record has an `extra` attribute. If it does, recursively apply the `redact_sensitive_data` utility to the `record.extra` dictionary. Also, apply redaction to the `record.msg` and `record.args`.
    4.  **Apply Filter:** In the `setup_logging` function within `src/config/logging_config.py`, instantiate `SensitiveDataFilter` and add it to all configured log handlers (e.g., `console_handler`, `file_handler`).
    5.  **Load Keys Securely:** Verify that the `GEMINI_API_KEY` is loaded from a `.env` file using `python-dotenv` within `src/config/environment.py` or `src/config/settings.py` and is never hardcoded.
    6.  **Transition to Structured Logging:** Modify the `logging.Formatter` to produce JSON-formatted logs to improve parsing and analysis. Use `python-json-logger` or a custom JSON formatter. Logs should include contextual identifiers like `session_id` where available.
-   **Testing Considerations:**
    -   Unit test `SensitiveDataFilter` to confirm it redacts keys in nested dictionaries.
    -   Manually inspect all log outputs (`app.log`, console) during application startup and a test workflow run to confirm that no API keys or other sensitive data appear in plain text.
    -   Verify that logs are now in a structured JSON format.
-   **Potential Challenges & Critical Considerations:** Ensure the redaction logic does not significantly impact logging performance. The list of sensitive keys must be comprehensive.

#### **2.2. Task: Pydantic Model Standardization (Foundation)**

-   **Task/Feature Addressed:** Data model inconsistencies and flow irregularities (CI-004), which cause cascading failures (CI-003). This is the foundational fix for the entire data flow and supports REQ-FUNC-PARSE-1.
-   **Affected Component(s):** `src/models/data_models.py`, `src/models/validation_schemas.py`.
-   **Pydantic Model Changes:** Define and/or refactor the core data models using Pydantic to enforce strict data contracts.

    **File:** `src/models/data_models.py`
    ```python
    from pydantic import BaseModel, Field, HttpUrl
    from typing import List, Optional, Dict, Any
    from uuid import UUID, uuid4
    from enum import Enum

    class ItemStatus(str, Enum):
        INITIAL = "initial"
        GENERATED = "generated"
        USER_MODIFIED = "user_modified"
        USER_ACCEPTED = "user_accepted"
        TO_REGENERATE = "to_regenerate"
        GENERATION_FAILED = "generation_failed"
        GENERATED_FALLBACK = "generated_fallback"
        STATIC = "static"

    class ItemType(str, Enum):
        BULLET_POINT = "bullet_point"
        KEY_QUALIFICATION = "key_qualification"
        EXECUTIVE_SUMMARY_PARA = "executive_summary_para"
        EXPERIENCE_ROLE_TITLE = "experience_role_title"
        PROJECT_DESCRIPTION_BULLET = "project_description_bullet"
        EDUCATION_ENTRY = "education_entry"
        CERTIFICATION_ENTRY = "certification_entry"
        LANGUAGE_ENTRY = "language_entry"

    class Item(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        content: str
        status: ItemStatus = ItemStatus.INITIAL
        item_type: ItemType = ItemType.BULLET_POINT
        raw_llm_output: Optional[str] = None  # REQ-FUNC-UI-6
        confidence_score: Optional[float] = None
        metadata: Dict[str, Any] = Field(default_factory=dict)
        user_feedback: Optional[str] = None

    class Subsection(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        name: str  # e.g., "Senior Software Engineer @ TechCorp Inc."
        items: List[Item] = Field(default_factory=list)
        metadata: Dict[str, Any] = Field(default_factory=dict) # e.g., dates, company, location

    class Section(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        name: str # e.g., "Professional Experience"
        content_type: str = "DYNAMIC"  # DYNAMIC or STATIC
        subsections: List[Subsection] = Field(default_factory=list)
        items: List[Item] = Field(default_factory=list) # For sections without subsections
        order: int = 0
        status: ItemStatus = ItemStatus.INITIAL

    class StructuredCV(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        sections: List[Section] = Field(default_factory=list)
        metadata: Dict[str, Any] = Field(default_factory=dict)

    class JobDescriptionData(BaseModel):
        raw_text: str
        skills: List[str] = Field(default_factory=list)
        experience_level: Optional[str] = None
        responsibilities: List[str] = Field(default_factory=list)
        industry_terms: List[str] = Field(default_factory=list)
        company_values: List[str] = Field(default_factory=list)
        error: Optional[str] = None
    ```
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:** None in this task, but subsequent tasks will rely heavily on these models.
-   **Detailed Implementation Steps:**
    1.  Replace the existing dataclasses or dictionaries in `src/models/data_models.py` with the Pydantic models defined above.
    2.  Create corresponding validation schemas in `src/models/validation_schemas.py` that can be used for API-level validation if a FastAPI endpoint is used.
    3.  Ensure all enums (`ItemStatus`, `ItemType`) are defined and used consistently.
-   **Testing Considerations:**
    -   Write unit tests for each Pydantic model to ensure they validate correct data and raise `ValidationError` for incorrect data (e.g., wrong types, missing required fields).
-   **Potential Challenges & Critical Considerations:** This is a foundational change. All parts of the system that create or consume CV or job description data will need to be updated to use these models.

#### **2.3. Task: Core Agent Bug Fixes**

-   **Task/Feature Addressed:** `TypeError` in `ParserAgent` (CI-002) and preparing `EnhancedContentWriterAgent` to avoid `AttributeError` (CI-003).
-   **Affected Component(s):** `src/agents/parser_agent.py`, `src/agents/enhanced_content_writer.py`, `src/services/llm.py`.
-   **Pydantic Model Changes:** Agents will now be expected to return and accept the Pydantic models defined in Task 2.2.
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:**
    -   **`ParserAgent` (`src/agents/parser_agent.py`):**
        1.  Refactor `parse_job_description` to be an `async` method.
        2.  Inside `parse_job_description`, change the LLM call to `response = await self.llm.generate_content(prompt)`.
        3.  The response from the LLM service should be an `LLMResponse` object. Access the content via `response.content`.
        4.  After parsing the JSON from `response.content`, validate it by creating an instance of the `JobDescriptionData` Pydantic model.
        5.  Wrap the entire method in a `try...except` block. On failure, return an `AgentResult` object with `success=False` and a descriptive `error_message`.
    -   **`EnhancedContentWriterAgent` (`src/agents/enhanced_content_writer.py`):**
        1.  As a defensive measure, at the beginning of the `run_async` method, use a `try...except` block with `JobDescriptionData.model_validate()` on the `input_data['job_description_data']` part of the input. This ensures the data structure is correct before proceeding. If validation fails, return a failed `AgentResult` immediately.
-   **Detailed Implementation Steps:**
    1.  Modify `src/agents/parser_agent.py`: Change `def parse_job_description(...)` to `async def parse_job_description(...)`.
    2.  In the same method, change `response = self.llm.generate_content(...)` to `response = await self.llm.generate_content(...)`.
    3.  Update the logic to use `response.content` for JSON parsing.
    4.  Instantiate the `JobDescriptionData` Pydantic model from the parsed dictionary.
    5.  Refactor the `run` method in `ParserAgent` to correctly call the new `async` method and return a validated `JobDescriptionData` object within its result payload.
-   **Testing Considerations:**
    -   Create a unit test for `ParserAgent.parse_job_description` that mocks the `llm.generate_content` call to return a mock `LLMResponse`. Verify the method correctly `await`s the call and parses the `.content`.
    -   Create a unit test for `EnhancedContentWriterAgent.run_async` that passes malformed `job_description_data` (e.g., a string) and asserts that it returns a failed `AgentResult` without hitting an `AttributeError`.

---

### **3. Phase 2: MVP Core Feature Implementation**

**Goal:** Deliver the core MVP functional requirements, focusing on granular processing, "Big 10" skills generation, and UI support for the new workflow.

#### **3.1. Task: Implement Granular, Item-by-Item Processing Workflow**

-   **Task/Feature Addressed:** Implements REQ-FUNC-GEN-3, REQ-FUNC-GEN-4, REQ-FUNC-UI-2, REQ-NONFUNC-PERF-1, and REQ-NONFUNC-RELIABILITY-1.
-   **Affected Component(s):** `src/core/enhanced_orchestrator.py`, `src/agents/enhanced_content_writer.py`, `src/core/state_manager.py`, `src/core/main.py` (Streamlit UI).
-   **Pydantic Model Changes:** `ItemStatus` enum will be heavily used to track the state of each role/project.
-   **LLM Prompt Changes:** `resume_role_prompt.md` and `side_project_prompt.md` will now be populated with the context of a *single* role or project, not an entire section.
-   **Agent Logic Modifications:**
    -   **`EnhancedContentWriterAgent`:**
        1.  Refactor `run_async` to accept the `StructuredCV` and a specific `item_id` (or `subsection_id`) to process.
        2.  The prompt building logic must be updated to only pull context for that single role or project from the `StructuredCV`. This results in smaller, more focused prompts.
        3.  The agent's output should be the updated `content` for just that single `Item` or `Subsection`.
    -   **`EnhancedOrchestrator`:**
        1.  The main `execute_workflow` method needs a new internal loop. After generating Key Qualifications, it should iterate through the `subsections` of the "Professional Experience" `Section` in the `StructuredCV`.
        2.  Inside the loop, for each `Subsection` (role), it calls the refactored `ContentWriterAgent` with the `subsection.id`.
        3.  After the agent returns, the orchestrator updates the corresponding `Subsection` in the `StructuredCV` via the `StateManager` and sets its status to `GENERATED`.
        4.  The orchestrator must then yield control back to the UI to allow for user review of the newly generated item. This requires tight integration with Streamlit's state management.
    -   **UI (`src/core/main.py`):**
        1.  The UI rendering logic must iterate through the `StructuredCV` and display each section.
        2.  For "Professional Experience" and "Side Projects", it must render each `Subsection` (role/project) individually.
        3.  Each rendered role/project must have its own "Accept" and "Regenerate" buttons.
        4.  Clicking "Regenerate" on `subsection-A` should update `subsection-A.status` to `TO_REGENERATE` in the `StateManager` and trigger a rerun of the orchestration logic, which will now pick up this specific item for processing.
-   **Detailed Implementation Steps:**
    1.  Refactor `EnhancedContentWriterAgent.run_async` to accept `item_id_to_process` as an argument.
    2.  Modify the `EnhancedOrchestrator` to loop through experience/project items and call the writer agent for each one.
    3.  Update the Streamlit UI to display items individually and include "Accept"/"Regenerate" buttons for each.
    4.  Implement the callback logic for the "Regenerate" button to update the item's status and re-trigger the orchestrator.
-   **Testing Considerations:**
    -   Integration test: Verify that clicking "Regenerate" for Role B does not affect the content of Role A.
    -   E2E test (using `tests/e2e/test_individual_item_processing.py`): A full user flow of generating content for 3 roles, regenerating the 2nd one, and accepting all.
-   **Potential Challenges & Critical Considerations:** Managing the state between the backend orchestrator and the Streamlit frontend will be complex. Streamlit's execution model requires careful use of `st.session_state` to maintain the `StructuredCV` object and track the current state of the workflow between user interactions.

#### **3.2. Task: Implement "Big 10" Skills & Raw LLM Output Display**

-   **Task/Feature Addressed:** REQ-FUNC-GEN-2 ("Big 10" skills) and REQ-FUNC-UI-6 (Display Raw LLM Output).
-   **Affected Component(s):** `src/agents/enhanced_content_writer.py`, `src/models/data_models.py`, `src/core/main.py`.
-   **Pydantic Model Changes:** The `Item` model in `src/models/data_models.py` already includes `raw_llm_output: Optional[str]`. This will now be populated.
-   **LLM Prompt Changes:** Review `data/prompts/key_qualifications_prompt.md` to ensure it explicitly asks for 10 skills.
-   **Agent Logic Modifications:**
    -   In `EnhancedContentWriterAgent`, whenever a call to `llm.generate_content()` is made, the agent must store the `response.content` string in a temporary variable *before* any cleaning or parsing.
    -   This raw string must then be saved to the `raw_llm_output` field of the corresponding `Item` object being generated.
-   **Detailed Implementation Steps:**
    1.  Rename any functions named `extract_big_6_skills` to `generate_key_qualifications` or similar for clarity.
    2.  In `EnhancedContentWriterAgent`, after receiving a response from the LLM, assign the raw text to `item.raw_llm_output`.
    3.  In `src/core/main.py`, within the UI rendering loop for each generated item, add an `st.expander`.
        ```python
        if item.raw_llm_output:
            with st.expander("Show Raw AI Output"):
                st.code(item.raw_llm_output, language='text')
        ```
-   **Testing Considerations:**
    -   Unit test `EnhancedContentWriterAgent` to confirm the `raw_llm_output` field is populated.
    -   UI test: Manually verify that the expander appears and displays the raw text for a generated item.

---

### **4. Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

### **5. Phase 3: "Smart Agent" Logic, Fallbacks, and Full SRS Alignment**

**Goal:** Enhance agent capabilities with robust primary LLM (Gemini) utilization coupled with effective fallback mechanisms; ensure all remaining MVP-scoped SRS v1.3 requirements are met; and prepare agent interfaces for future scalability.

#### **5.1. Task: Implement "Smart Agent" Logic with Robust Fallbacks**

-   **Task/Feature Addressed:** Implements REQ-NONFUNC-RELIABILITY-1 by making agents resilient to LLM failures. Enhances the quality of all `REQ-FUNC-GEN-*` requirements by using a primary LLM (Gemini) effectively.
-   **Affected Component(s):** `src/agents/parser_agent.py`, `src/agents/enhanced_content_writer.py`, `src/services/llm.py`.
-   **Pydantic Model Changes:** The `ItemStatus` enum in `src/models/data_models.py` will be used to flag items generated via fallback (`GENERATED_FALLBACK`) or where generation failed completely (`GENERATION_FAILED`).
-   **LLM Prompt Changes:** None for this task, but the logic determines *if* the prompts are used.
-   **Agent Logic Modifications:**
    -   **`ParserAgent` (`src/agents/parser_agent.py`):**
        1.  In `parse_job_description`, wrap the LLM call and JSON parsing in a `try...except` block.
        2.  **Primary Logic:** Attempt to get a structured response from Gemini.
        3.  **Fallback Logic:** If the `try` block fails (due to API error, timeout, or unparsable JSON), fall back to a simpler, regex-based extraction for essential fields (e.g., find lines containing "skills:", "responsibilities:", etc.). Populate the `JobDescriptionData` object with this partial data and log a warning.
    -   **`EnhancedContentWriterAgent` (`src/agents/enhanced_content_writer.py`):**
        1.  In the `run_async` method (or its helper that processes a single item), wrap the LLM call in a `try...except` block.
        2.  **Primary Logic:** Attempt to generate content for the specific item using Gemini.
        3.  **Fallback Logic:** If the call fails:
            -   Do not halt the entire workflow.
            -   Update the `Item`'s status in the `StructuredCV` to `GENERATION_FAILED`.
            -   Set the `Item`'s `content` to a user-friendly placeholder message (e.g., "AI content generation failed. You can try again or edit this field manually.").
            -   Log the error with the `item_id` for debugging.
-   **Detailed Implementation Steps:**
    1.  In `parser_agent.py`, implement the `try...except` block in `parse_job_description`. Create private helper methods for regex-based parsing as a fallback.
    2.  In `enhanced_content_writer.py`, implement the `try...except` block for the single-item generation logic.
    3.  Define the user-facing error message as a constant.
    4.  Ensure the agent returns a successful `AgentResult` even on fallback, but the content and status of the specific item reflect the failure. This prevents the orchestrator from stopping.
-   **Testing Considerations:**
    -   Unit test the `ParserAgent`'s fallback logic by mocking the LLM service to raise an exception and asserting that the regex-based parsing is invoked.
    -   Unit test the `EnhancedContentWriterAgent`'s fallback by mocking the LLM service to fail for a specific `item_id`. Assert that the corresponding `Item` in the `StructuredCV` has its status set to `GENERATION_FAILED` and its content updated with the placeholder message.

#### **5.2. Task: Integrate Remaining MVP Agents (QA, Research)**

-   **Task/Feature Addressed:** Integrates REQ-FUNC-QA-1 and REQ-FUNC-RESEARCH-X into the workflow.
-   **Affected Component(s):** `src/core/enhanced_orchestrator.py`, `src/agents/quality_assurance_agent.py`, `src/agents/research_agent.py`.
-   **Pydantic Model Changes:** The `Item` model's `metadata` field can be used to store QA scores or research findings.
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:**
    -   **`ResearchAgent` (`src/agents/research_agent.py`):**
        1.  This agent should be called by the orchestrator after the `ParserAgent` has successfully parsed the user's CV and the job description.
        2.  It should take the `StructuredCV` and `JobDescriptionData` as input.
        3.  Logic:
            -   Generate embeddings for each `Item` in the user's base CV and store them in ChromaDB.
            -   Generate an embedding for the key requirements of the `JobDescriptionData`.
            -   This is a preparatory step. The agent's primary use will be to find relevant content *during* the content generation phase.
    -   **`QualityAssuranceAgent` (`src/agents/quality_assurance_agent.py`):**
        1.  This agent should be called by the orchestrator immediately after the `ContentWriterAgent` generates content for an item.
        2.  It takes the newly generated `Item` as input.
        3.  Logic: Perform automated checks as per REQ-FUNC-QA-1 (e.g., check for length constraints, presence of keywords from `JobDescriptionData`).
        4.  Output: Update the `Item`'s `metadata` with a `qa_score` (e.g., 0.0 to 1.0) and a list of any identified issues. This data can be optionally displayed in the UI.
    -   **`EnhancedOrchestrator` (`src/core/enhanced_orchestrator.py`):**
        1.  Modify the main workflow to call the `ResearchAgent` once after the initial parsing step to populate the vector store.
        2.  Modify the item-by-item generation loop. After a successful call to `ContentWriterAgent`, immediately call `QualityAssuranceAgent` with the newly generated item before yielding to the UI.
-   **Testing Considerations:**
    -   Integration test for the `ResearchAgent` to verify that CV content is correctly added to ChromaDB.
    -   Unit test for the `QualityAssuranceAgent` with sample `Item` objects (e.g., one too long, one missing keywords) to verify it correctly identifies issues.

#### **5.3. Task: Finalize LangGraph-Compatible Agent Interfaces**

-   **Task/Feature Addressed:** Strategic refactoring to prepare for future scalability with LangGraph.
-   **Affected Component(s):** All agent classes in `src/agents/`.
-   **Pydantic Model Changes:** A new Pydantic model, `AgentState`, will be created in a new file, e.g., `src/orchestration/state.py`.

    **File:** `src/orchestration/state.py`
    ```python
    from pydantic import BaseModel, Field
    from typing import List, Dict, Optional, Any
    from src.models.data_models import StructuredCV, JobDescriptionData

    class AgentState(BaseModel):
        structured_cv: StructuredCV
        job_description: JobDescriptionData
        current_item_id: Optional[str] = None
        current_section_key: Optional[str] = None
        user_feedback: Optional[Dict[str, Any]] = None
        research_findings: Optional[Dict[str, Any]] = Field(default_factory=dict)
        error_messages: List[str] = Field(default_factory=list)
        final_cv_output_path: Optional[str] = None

        class Config:
            arbitrary_types_allowed = True
    ```
-   **Agent Logic Modifications:**
    -   All agent classes (`ParserAgent`, `EnhancedContentWriterAgent`, etc.) must be refactored.
    -   The primary execution method (e.g., `run_async`) will be deprecated or wrapped.
    -   A new primary execution method will be created with the signature: `def run_as_node(self, state: AgentState) -> dict:`.
    -   This method will take the entire `AgentState` object as input.
    -   It will perform its processing based on the `state`.
    -   It **must not** modify the input `state` object directly.
    -   It must return a dictionary containing **only the fields of `AgentState` that it has changed**. For example, the `ParserAgent` would return `{"structured_cv": ..., "job_description": ...}`.
-   **Detailed Implementation Steps:**
    1.  Create the `src/orchestration/state.py` file with the `AgentState` Pydantic model.
    2.  Go through each agent class in `src/agents/`.
    3.  Create the `run_as_node(self, state: AgentState) -> dict` method in each agent.
    4.  Move the core logic from the old `run_async` method into `run_as_node`.
    5.  Adapt the logic to read inputs from the `state` object and return a dictionary of updates.
-   **Testing Considerations:** Code review is critical for this task to ensure all agents conform to the new stateless, node-like interface.

---

### **6. Phase 4: LangGraph Integration, E2E Testing, and Deployment**

**Goal:** Implement LangGraph for robust orchestration, conduct thorough end-to-end testing, and prepare the application for deployment.

#### **6.1. Task: Integrate LangGraph for Workflow Orchestration**

-   **Task/Feature Addressed:** Replaces the custom procedural loop in `EnhancedOrchestrator` with a more robust, extensible, and stateful graph.
-   **Affected Component(s):** `src/core/enhanced_orchestrator.py` (will be significantly refactored or replaced by a new LangGraph-based orchestrator in `src/orchestration/`).
-   **Detailed Implementation Steps:**
    1.  Create a new module, e.g., `src/orchestration/cv_workflow_graph.py`.
    2.  Import `StateGraph` from `langchain_core.runnables` and the `AgentState` model.
    3.  Instantiate the graph: `workflow = StateGraph(AgentState)`.
    4.  Add nodes to the graph, where each node is a method from an agent (e.g., `workflow.add_node("parse_inputs", parser_agent.run_as_node)`).
    5.  Define the entry point: `workflow.set_entry_point("parse_inputs")`.
    6.  Define conditional edges to manage the iterative review loop for "Professional Experience" and "Side Projects". An edge function will check the state (e.g., `state.user_feedback`) to decide whether to regenerate the current item, move to the next item, or proceed to the next section.
    7.  Compile the graph: `app = workflow.compile()`.
    8.  Refactor `EnhancedOrchestrator` to use this compiled `app` to run the workflow.
-   **Testing Considerations:**
    -   E2E tests from `tests/e2e/` will be the primary validation method. Tracing the graph's execution for a full CV generation run will confirm correct state transitions.

#### **6.2. Task: End-to-End (E2E) Testing and NFR Validation**

-   **Task/Feature Addressed:** Validates all functional and non-functional requirements from SRS v1.3.
-   **Affected Component(s):** `tests/e2e/test_complete_cv_generation.py`, `tests/e2e/test_individual_item_processing.py`, `tests/e2e/test_error_recovery.py`.
-   **Detailed Implementation Steps:**
    1.  Review and expand the existing E2E test suites to cover all MVP features.
    2.  Create test cases that simulate user interactions: starting from scratch, uploading a CV, generating all sections, regenerating a specific role, editing content, and exporting the final PDF.
    3.  Validate performance against REQ-NONFUNC-PERF-1 by timing the display of the first section and subsequent individual items.
    4.  Validate reliability against REQ-NONFUNC-RELIABILITY-1 by mocking LLM API failures and ensuring the UI displays appropriate messages and allows for retries without crashing.
-   **Testing Considerations:** These tests should be automated and run as part of the CI/CD pipeline to prevent regressions.

#### **6.3. Task: Finalize Documentation and Prepare for Deployment**

-   **Task/Feature Addressed:** Prepares the application for operational readiness.
-   **Affected Component(s):** `README.md`, `Dockerfile`, `docs/`.
-   **Detailed Implementation Steps:**
    1.  Update `README.md` with final setup and usage instructions for the MVP.
    2.  Create comprehensive user documentation in `docs/user/` explaining the workflow and features.
    3.  Update developer documentation in `docs/dev/` to reflect the new architecture, including the Pydantic models and LangGraph workflow.
    4.  Review and test the `Dockerfile` to ensure it builds a runnable image of the final application.
    5.  Prepare deployment scripts or configurations (e.g., for Streamlit Cloud or another hosting service).
-   **Testing Considerations:** The final Docker image should be tested to ensure the application runs correctly in a containerized environment.

---

### **5. Phase 4: LangGraph Integration, E2E Testing, and Deployment (Continued)**

#### **6.4. Task: Performance Tuning and Optimization**

-   **Task/Feature Addressed:** Ensures the application meets performance requirements (REQ-NONFUNC-PERF-1) and provides a responsive user experience.
-   **Affected Component(s):** All agents performing LLM calls (`ParserAgent`, `EnhancedContentWriterAgent`), `LLMService`, and the `EnhancedOrchestrator` (LangGraph implementation).
-   **Detailed Implementation Steps:**
    1.  **Profile LLM Calls:** Use logging or a performance monitoring tool to measure the latency of each distinct LLM call (e.g., parsing, key skill generation, single role generation).
    2.  **Identify Bottlenecks:** Analyze the collected data to identify the slowest steps in the workflow.
    3.  **Optimize Prompts:** For any slow-performing LLM calls, review the prompts (`data/prompts/`) for verbosity. Experiment with more concise prompts that yield the same quality output to reduce token processing time.
    4.  **Implement Caching:** In `src/services/llm.py`, implement a caching layer (e.g., using `functools.lru_cache` for in-memory caching or a more robust solution like Redis if scaling is a concern). The cache key should be a hash of the prompt content and model parameters. This is highly effective for regeneration requests where the input hasn't changed.
    5.  **Review Asynchronous Operations:** Ensure that all I/O-bound operations, especially LLM API calls and file I/O, are fully asynchronous to prevent blocking the Streamlit application thread.
-   **Testing Considerations:**
    -   Run the E2E tests before and after optimization to quantify improvements in overall workflow time.
    -   Specifically test the caching by regenerating the same item twice and asserting that the second call is significantly faster and does not trigger an LLM API call.
-   **Potential Challenges & Critical Considerations:** Overly aggressive prompt optimization can degrade output quality. Caching requires careful key management to avoid serving stale content when the underlying context has changed.

---

### **7. Comprehensive Testing Strategy**

This section consolidates the testing approach for the entire refactoring plan.

#### **7.1. Unit Testing**

-   **Pydantic Models (`src/models/`):**
    -   Test successful validation with correct data.
    -   Test `ValidationError` is raised for missing required fields and incorrect data types.
-   **Agents (`src/agents/`):**
    -   Test the `run_as_node(self, state: AgentState)` method for each agent.
    -   Mock the input `AgentState` object.
    -   Assert that the returned dictionary contains the correct updated keys and that other parts of the state are untouched.
    -   **Specifically test fallback logic:** Mock the `LLMService` to raise an API error and assert that the agent's fallback path is executed and returns a correctly structured failure/placeholder output.
-   **LLMService (`src/services/llm.py`):**
    -   Mock the `google-generativeai` client.
    -   Test the retry logic by having the mocked client raise a transient error (e.g., `InternalServerError`) multiple times before succeeding. Verify that the service retries the correct number of times with exponential backoff.
-   **StateManager (`src/core/state_manager.py`):**
    -   Test `save_state` and `load_state` by saving a `StructuredCV` object, loading it back, and asserting that the loaded object is identical to the original.
    -   Test state update methods (e.g., `update_item_status`) and verify the changes are correctly reflected and persisted on save.

#### **7.2. Integration Testing**

-   **ParserAgent âž” ContentWriterAgent:**
    -   Create a test that runs the `ParserAgent` on a raw CV string to produce a `StructuredCV` object.
    -   Pass this `StructuredCV` object into the `EnhancedContentWriterAgent` along with a specific `subsection_id` to process.
    -   Assert that the `ContentWriterAgent` correctly uses the parsed data to generate content for the specified role.
-   **Orchestrator âž” Agents (LangGraph Workflow):**
    -   Run the compiled LangGraph application (`app.invoke(...)`).
    -   Verify that the sequence of agent node calls is correct for the MVP workflow (Parse âž” KeyQuals âž” Loop[ExperienceItem] âž” ...).
    -   Inspect the final `AgentState` to ensure all sections have been processed and the data is aggregated correctly.
-   **UI âž” StateManager:**
    -   Simulate a Streamlit button click (e.g., "Regenerate" for a specific item).
    -   Verify that the corresponding `Item` in the `StateManager`'s `StructuredCV` has its `status` updated to `TO_REGENERATE`.
    -   This test verifies the connection between UI actions and backend state changes.

#### **7.3. End-to-End (E2E) Testing**

-   The tests in `tests/e2e/` provide the framework for this.
-   **Happy Path:** A full workflow run. Input a sample CV and JD, proceed through all generation and review steps, and download the final PDF. Verify the PDF content for correctness and formatting.
-   **Granular Control Path:** For a CV with multiple experience roles, generate content for all. Then, specifically edit one bullet point in the second role, and regenerate the third role entirely. Accept all and export. Verify the final PDF reflects these specific, granular changes correctly.
-   **Error Path:** Use mocks to force the LLM API to fail during the generation of one specific experience role. Verify that the UI shows the "Generation Failed" placeholder for that item but allows the user to continue working on other items.

---

### **8. Security Hardening Plan**

-   **Task/Feature Addressed:** Further harden the application against common vulnerabilities, building on the initial logging fix. Implements REQ-NONFUNC-SECURITY-1.
-   **Affected Component(s):** Primarily affects input handling in `src/core/main.py` (Streamlit) and dependency management.
-   **Detailed Implementation Steps:**
    1.  **Input Sanitization:** Before user-provided text (from CVs or JDs) is rendered in any HTML context (e.g., via `st.markdown(unsafe_allow_html=True)` or before being passed to `WeasyPrint`), it must be sanitized. Use a library like `bleach` to remove potentially malicious HTML/JS tags.
        ```python
        import bleach
        # Before rendering user_provided_text to HTML:
        sanitized_text = bleach.clean(user_provided_text, tags=[], attributes={}, strip=True)
        st.markdown(sanitized_text)
        ```
    2.  **Dependency Scanning:** Integrate a security scanner into the CI/CD pipeline. Use `pip-audit` or `safety` to check `requirements.txt` for known vulnerabilities in third-party packages on every commit or pull request.
        ```bash
        # Example CI step
        pip install pip-audit
        pip-audit
        ```
    3.  **Container Security:**
        -   In the `Dockerfile`, add steps to create a non-root user and group.
        -   Use the `USER` instruction to switch to this non-root user before the `CMD` instruction.
            ```dockerfile
            # In Dockerfile
            RUN addgroup --system app && adduser --system --group app
            USER app
            CMD ["streamlit", "run", "src/core/main.py"]
            ```

---

### **9. Deployment & Operational Readiness Plan**

-   **Task/Feature Addressed:** Prepares the application for a repeatable, secure deployment.
-   **Affected Component(s):** `Dockerfile`, `requirements.txt`.
-   **Detailed Implementation Steps:**
    1.  **Finalize Production `Dockerfile`:**
        ```dockerfile
        # Use a slim, secure base image
        FROM python:3.11-slim

        # Install system dependencies for WeasyPrint
        RUN apt-get update && apt-get install -y --no-install-recommends \
            build-essential \
            libffi-dev \
            libcairo2-dev \
            libpango1.0-dev \
            libpangocairo-1.0-0 \
            libgdk-pixbuf2.0-dev \
            libjpeg-dev \
            zlib1g-dev \
            && rm -rf /var/lib/apt/lists/*

        WORKDIR /app

        # Copy only necessary files
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt

        COPY src/ src/
        COPY data/ data/
        COPY app.py .

        # Create non-root user
        RUN addgroup --system app && adduser --system --group app
        USER app

        # Expose the Streamlit port
        EXPOSE 8501

        # Set health check for the container
        HEALTHCHECK CMD streamlit hello

        # Command to run the application
        CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
        ```
    2.  **Configuration Management:**
        -   Create a `.env.example` file in the root directory listing all required environment variables (e.g., `GEMINI_API_KEY`, `LOG_LEVEL`).
        -   Ensure this `.env.example` file is committed to the repository, but the actual `.env` file is listed in `.gitignore`.
        -   The application (e.g., in `src/config/environment.py`) must be configured to read these variables from the environment.
    3.  **Static Asset Handling (Critical for WeasyPrint):**
        -   The `Dockerfile` *must* include the installation of system dependencies required by WeasyPrint for PDF generation (e.g., Pango, Cairo). This is a common point of failure in deployments. The example `Dockerfile` above includes this step.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **10. Detailed Implementation Guides for Key Components**

This section provides more granular, code-level guidance for implementing the most critical and architecturally significant components of the refactoring plan.

#### **10.1. Granular Processing in `EnhancedOrchestrator` & Streamlit UI**

**Objective:** To implement the iterative, item-by-item processing and review loop for "Professional Experience" and "Side Projects" as required by REQ-FUNC-GEN-3 and REQ-FUNC-UI-2.

**Conceptual Logic Flow:** The interaction between the Streamlit UI (`src/core/main.py`) and the `EnhancedOrchestrator` is crucial. Streamlit's script re-runs on every user interaction, so state must be meticulously managed in `st.session_state`.

**`src/core/main.py` (Streamlit UI) - Main Loop Snippet:**

```python
# In main function of src/core/main.py

# Initialize state manager and orchestrator
# This should be cached or placed in session_state to avoid re-initialization
if 'state_manager' not in st.session_state:
    st.session_state.state_manager = StateManager(session_id=st.session_state.session_id)
    # Load state if it exists
    st.session_state.state_manager.load_state()

if 'orchestrator' not in st.session_state:
    st.session_state.orchestrator = EnhancedOrchestrator(
        state_manager=st.session_state.state_manager
        # ... other dependencies
    )

state_manager = st.session_state.state_manager
orchestrator = st.session_state.orchestrator
structured_cv = state_manager.get_structured_cv()

# -- UI Rendering --
if structured_cv:
    # Iterate through sections to render them
    for section in structured_cv.sections:
        if section.name in ["Professional Experience", "Side Projects"]:
            st.subheader(section.name)
            # Render each subsection (role/project) individually
            for subsection in section.subsections:
                # Find the corresponding item to check its status
                # Assuming the first item represents the subsection's state for simplicity
                first_item = subsection.items[0] if subsection.items else None

                if first_item:
                    # UI component to display the subsection content
                    display_subsection_content(subsection)

                    # Buttons for user interaction
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        if st.button("âœ… Accept", key=f"accept_{subsection.id}"):
                            orchestrator.update_item_status(first_item.id, ItemStatus.USER_ACCEPTED)
                            st.rerun()
                    with col2:
                        if st.button("ðŸ”„ Regenerate", key=f"regen_{subsection.id}"):
                            # This action triggers the orchestrator
                            st.session_state['item_to_process'] = subsection.id
                            st.rerun() # Re-run the script to trigger processing
# ... other sections

# -- Orchestration Logic --
# This part of the script runs after the UI is drawn.
# We check if an action has triggered a processing need.
if st.session_state.get('item_to_process'):
    subsection_id_to_process = st.session_state.pop('item_to_process')
    with st.spinner(f"AI is tailoring item..."):
        # The orchestrator finds the item, updates its status, and calls the agent
        updated_cv = orchestrator.process_single_item(subsection_id_to_process)
        # Update the state and rerun to show the new content
        state_manager.set_structured_cv(updated_cv)
        st.rerun()

```

**`src/core/enhanced_orchestrator.py` - Core Processing Logic Snippet:**

```python
# In EnhancedOrchestrator class

class EnhancedOrchestrator:
    def __init__(self, state_manager: StateManager, ...):
        self.state_manager = state_manager
        self.content_writer_agent = EnhancedContentWriterAgent(...)
        # ... other agents

    def process_single_item(self, subsection_id: UUID) -> StructuredCV:
        """
        Processes a single subsection (role/project) marked for regeneration.
        """
        structured_cv = self.state_manager.get_structured_cv()

        # Find the subsection to process
        target_subsection = None
        for section in structured_cv.sections:
            for sub in section.subsections:
                if sub.id == subsection_id:
                    target_subsection = sub
                    break
            if target_subsection:
                break

        if not target_subsection:
            # Handle error: subsection not found
            return structured_cv

        # Update status to indicate processing
        for item in target_subsection.items:
            item.status = ItemStatus.GENERATING # A new status to show spinner
        self.state_manager.save_state()

        # Invoke the content writer agent for this specific subsection
        # The agent's interface is now compatible with this granular call
        agent_result = self.content_writer_agent.run_as_node({
            "structured_cv": structured_cv,
            "job_description": structured_cv.metadata.get('job_description'),
            "current_item_id": str(target_subsection.id) # Pass ID for context
        })

        # Process the result from the agent
        if agent_result:
             # The result should contain the updated 'structured_cv'
            updated_cv = agent_result.get("structured_cv", structured_cv)
            self.state_manager.set_structured_cv(updated_cv)
            self.state_manager.save_state()
            return updated_cv

        # Handle agent failure
        # ... set status to GENERATION_FAILED ...
        self.state_manager.save_state()
        return structured_cv
```

#### **10.2. Gemini-Powered `ParserAgent` Segmentation**

**Objective:** To robustly parse the "Professional Experience" and "Side Projects" sections from raw CV text into structured `Subsection` and `Item` objects using an LLM call. This is the definitive fix for CI-004.

**LLM Prompt for Experience Segmentation:** This prompt should be saved in `data/prompts/experience_segmentation_prompt.md`.

```text
You are a highly accurate resume parsing expert. Your task is to analyze the "Professional Experience" section of a CV and extract each distinct job role into a structured JSON format.

Analyze the following text:
---
{{experience_text}}
---

Identify each unique job role and format the output as a JSON array. Each object in the array must represent one role and have the following keys:
- "name": The job title and company (e.g., "Senior Software Engineer at Tech Corp").
- "metadata": A dictionary containing "duration" (e.g., "2021-2023") and "location" (e.g., "San Francisco, CA") if available.
- "items": A list of strings, where each string is a bullet point describing an accomplishment or responsibility for that role.

IMPORTANT:
- Extract the content exactly as it appears.
- If a duration or location is not present for a role, omit the key or set its value to null.
- Ensure the output is a single, valid JSON array. Do not include any text or explanations outside of the JSON structure.
```

**`src/agents/parser_agent.py` - Implementation Snippet:**

```python
# In ParserAgent class

    async def _parse_experience_section_with_llm(self, experience_text: str) -> List[Subsection]:
        """
        Uses an LLM to parse the raw text of an experience section into structured Subsection objects.
        """
        if not experience_text.strip():
            return []

        try:
            # Load the specialized prompt
            prompt_template = self.prompt_loader.load_prompt("experience_segmentation_prompt")
            prompt = prompt_template.format(experience_text=experience_text)

            # Call the LLM
            response = await self.llm.generate_content(prompt)
            llm_output = response.content

            # Clean and parse the JSON response
            # Find the start and end of the JSON array
            start_index = llm_output.find('[')
            end_index = llm_output.rfind(']') + 1
            if start_index == -1 or end_index == 0:
                self.logger.warning("LLM did not return a valid JSON array for experience parsing.")
                return [] # Fallback to regex or return empty

            json_str = llm_output[start_index:end_index]
            parsed_roles = json.loads(json_str)

            # Convert parsed dictionaries into Subsection objects
            subsections = []
            for role_data in parsed_roles:
                if not isinstance(role_data, dict) or "name" not in role_data:
                    continue

                items = [
                    Item(content=bullet, item_type=ItemType.BULLET_POINT, status=ItemStatus.INITIAL)
                    for bullet in role_data.get("items", [])
                ]

                subsection = Subsection(
                    name=role_data.get("name"),
                    metadata=role_data.get("metadata", {}),
                    items=items
                )
                subsections.append(subsection)

            return subsections

        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to decode JSON from LLM for experience parsing: {e}")
            return [] # Fallback
        except Exception as e:
            self.logger.error(f"An unexpected error occurred during LLM experience parsing: {e}")
            return [] # Fallback
```

This detailed guide provides the engineer with a clear, executable path for implementing the most complex architectural changes required for the MVP, ensuring the final product is both robust and aligns with all specified requirements.

---

---

### **11. Non-Functional Requirements (NFR) Adherence Strategy**

This section summarizes how the proposed refactoring plan systematically addresses the key Non-Functional Requirements (NFRs) outlined in `SRS Update for MVP_.txt` (v1.3).

#### **11.1. Performance (REQ-NONFUNC-PERF-1)**

-   **Requirement:** The first dynamic section should display within 30 seconds, and subsequent individual items (roles/projects) within 15-30 seconds.
-   **Adherence Strategy:**
    -   The core architectural shift to **Granular, Item-by-Item Processing (Task 3.1)** is the primary strategy to meet this NFR.
    -   Instead of a long, single wait time for all content, users will experience a much shorter "time-to-first-content" (e.g., for Key Qualifications).
    -   The processing time for each subsequent role or project is isolated, fitting within the 15-30 second target window, as each LLM call is smaller and more focused. This transforms the user experience from a batch-processing model to an interactive, responsive one.
    -   **Performance Tuning and Caching (Task 6.4)** in Phase 4 will further optimize these interaction times.

#### **11.2. Reliability (REQ-NONFUNC-RELIABILITY-1)**

-   **Requirement:** The system must gracefully handle transient errors from external services (LLM API rate limits, network issues, content generation failures for individual items) and provide retry mechanisms.
-   **Adherence Strategy:**
    -   **Fault Isolation:** The **Granular Processing (Task 3.1)** strategy inherently improves reliability. A failure in generating content for one role will not prevent the user from working with other roles, dramatically reducing the "blast radius" of any single error.
    -   **Smart Agent Fallbacks (Task 5.1):** By implementing robust fallback logic within the `ParserAgent` and `EnhancedContentWriterAgent`, the system can degrade gracefully. If a primary LLM call fails, the agents can revert to simpler logic or provide placeholder content, ensuring the application remains functional and does not crash.
    -   **LLM Service Error Handling:** The plan includes enhancing `LLMService` to handle transient network errors and API status codes (e.g., 5xx errors) with automated retries and exponential backoff, making the lowest level of external communication more resilient.
    -   **Rate Limit Mitigation:** The granular processing approach is a proactive measure against hitting `429 Too Many Requests` errors from the Gemini API. By breaking large requests into smaller, time-distributed calls, the system is less likely to exceed RPM/TPM limits.

#### **11.3. Security (REQ-NONFUNC-SECURITY-1)**

-   **Requirement:** LLM API keys and other sensitive credentials must be stored and accessed securely and must not be logged.
-   **Adherence Strategy:**
    -   **Immediate Remediation (Task 2.1):** The highest priority task in Phase 1 is to fix the API key logging vulnerability by implementing a robust redaction filter in the logging configuration.
    -   **Secure Loading:** The plan specifies using `.env` files for managing secrets, ensuring they are not hardcoded in the source code.
    -   **Comprehensive Security Hardening (Section 8):** The plan includes further security measures like input sanitization to prevent XSS vulnerabilities and integrating dependency scanning tools into the CI/CD pipeline to protect against vulnerabilities in third-party packages.

#### **11.4. Maintainability (REQ-NONFUNC-MAINTAIN-2)**

-   **Requirement:** Prompt templates must be stored externally and loaded dynamically. The codebase should be modular and follow standards.
-   **Adherence Strategy:**
    -   **Externalized Prompts:** The plan includes creating a `PromptManager` utility to load all LLM prompts from the `data/prompts/` directory, satisfying this requirement directly.
    -   **Modularity & Data Contracts:** The rigorous enforcement of **Pydantic Models (Task 2.2)** establishes clear, validated data contracts between all components. This, combined with the modular agent-based architecture, makes the system significantly easier to understand, debug, and extend.
    -   **LangGraph-Compatible Interfaces (Task 5.3):** Refactoring agents to have a standardized, stateless interface (`run_as_node(state) -> state_update`) makes them highly modular and reusable, simplifying both current orchestration and future migration to more complex workflow engines.

---

### **12. Final Recommendations & Implementation Order**

This refactoring plan is comprehensive and designed to be executed in a prioritized manner to maximize stability and deliver value quickly.

#### **12.1. Summary of Expected Outcomes**

-   **A Stable and Reliable MVP:** The system will be free from the critical data flow, security, and asynchronous processing errors that currently plague it.
-   **Full Adherence to SRS v1.3:** All functional and non-functional requirements for the MVP will be met, including the critical granular processing and hybrid UI control model.
-   **Enhanced User Experience:** Users will benefit from a more interactive, responsive, and transparent application with greater control over the generated content.
-   **A Scalable and Maintainable Codebase:** The refactored architecture, built on Pydantic data contracts and LangGraph-compatible agents, will provide a robust foundation for future features and complexity.

#### **12.2. Prioritized Implementation Order**

The successful execution of this plan depends on adhering to the following phased order, as later phases build upon the stability and features of earlier ones.

1.  **Phase 1 - Foundational Stabilization (Highest Priority):**
    *   **1A: Fix API Key Logging (Task 2.1):** This is the immediate, non-negotiable first step.
    *   **1B: Implement Pydantic Models (Task 2.2):** Establish the data contracts that the rest of the system will use.
    *   **1C: Fix Core Agent Bugs (Task 2.3):** Resolve the `TypeError` in `ParserAgent` and prepare `EnhancedContentWriterAgent`.
    *   **1D: Secure & Structured Logging (Task 2.1):** Complete the transition to secure, structured logging.

2.  **Phase 2 - Core MVP Feature Implementation:**
    *   **2A: Implement Granular Processing (Task 3.1):** Refactor the Orchestrator, Content Writer, and UI to support the item-by-item workflow. This is the most significant architectural change for the MVP.
    *   **2B: "Big 10" Skills & Raw LLM Output (Task 3.2):** Implement these key user-facing features.

3.  **Phase 3 - Resilience and Future-Proofing:**
    *   **3A: Implement "Smart Agent" Fallbacks (Task 5.1):** Build resilience against LLM and other external service failures.
    *   **3B: Integrate QA & Research Agents (Task 5.2):** Integrate the remaining MVP agents into the workflow.
    *   **3C: Finalize LangGraph-Compatible Interfaces (Task 5.3):** Refactor all agent interfaces to prepare for the final orchestration step.

4.  **Phase 4 - Finalization and Deployment:**
    *   **4A: Integrate LangGraph (Task 6.1):** Replace the custom orchestrator logic with the more robust LangGraph implementation.
    *   **4B: E2E Testing & NFR Validation (Task 6.2):** Conduct thorough testing of the complete, integrated system.
    *   **4C: Performance Tuning & Deployment Prep (Tasks 6.4, 6.3):** Optimize and containerize the application for deployment.

Adherence to this prioritized plan will ensure a systematic and low-risk transformation of the `aicvgen` project into a successful MVP.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **10. Detailed Implementation Guides for Key Components (Continued)**

#### **10.3. `LLMService` Enhancements for Resilience**

**Objective:** To make the `LLMService` in `src/services/llm.py` more robust by implementing sophisticated error handling, retry logic, and API key fallbacks.

**`src/services/llm.py` - Implementation Snippet:**

```python
# In EnhancedLLMService class

    def __init__(self, ...):
        # ... (existing init code)
        # Add a flag to track which API key is in use
        self.api_key_in_use = "primary" # or "user", "fallback"
        self._configure_llm_client()

    def _configure_llm_client(self):
        """Configures the Gemini client with the current API key."""
        api_key_to_use = self.user_api_key or self.primary_api_key
        self.api_key_in_use = "user" if self.user_api_key else "primary"

        if not api_key_to_use:
            api_key_to_use = self.fallback_api_key
            self.api_key_in_use = "fallback"

        if not api_key_to_use:
            self.logger.critical("No valid API key available.")
            raise ValueError("LLM Service cannot be initialized without an API key.")

        genai.configure(api_key=api_key_to_use)
        self.llm = genai.GenerativeModel(self.model_name)
        self.logger.info(f"LLM Service configured with '{self.api_key_in_use}' key.")

    def _switch_to_fallback_key(self) -> bool:
        """Switches to the fallback API key if available."""
        if self.api_key_in_use != "fallback" and self.fallback_api_key:
            self.logger.warning("Attempting to switch to fallback API key due to persistent errors.")
            self.api_key_in_use = "fallback"
            self._configure_llm_client()
            return True
        self.logger.error("Could not switch to fallback key. Either already using it or none is configured.")
        return False

    async def generate_content(self, prompt: str, ..., max_retries: int = 3) -> LLMResponse:
        # ... (existing setup code)
        attempt = 0
        while attempt <= max_retries:
            try:
                # ... (existing logic to generate content)
                # On successful response:
                return LLMResponse(...)

            except (google.api_core.exceptions.ResourceExhausted,
                    google.api_core.exceptions.InternalServerError,
                    google.api_core.exceptions.ServiceUnavailable) as e:

                attempt += 1
                if attempt > max_retries:
                    self.logger.error(f"LLM call failed after {max_retries} retries.", error=e)
                    # Try switching to fallback key as a final recovery attempt
                    if self._switch_to_fallback_key():
                        attempt = max_retries # Allow one final attempt on the fallback key
                        continue
                    return LLMResponse(success=False, error_message=str(e), ...)

                # Exponential backoff
                delay = (2 ** attempt) + (random.uniform(0, 1)) # Add jitter
                self.logger.warning(f"LLM API error encountered. Retrying in {delay:.2f} seconds...", error=e, attempt=attempt)
                await asyncio.sleep(delay)

            except Exception as e:
                self.logger.error(f"Unhandled LLM error: {e}")
                return LLMResponse(success=False, error_message=str(e), ...)

        return LLMResponse(success=False, error_message="Exhausted all retry attempts.", ...)
```

---

### **11. Prompt Management and Versioning Strategy**

**Objective:** Establish a robust system for managing, versioning, and dynamically loading LLM prompts, decoupling them from the application code as per REQ-NONFUNC-MAINTAIN-2.

-   **Task/Feature Addressed:** Implements a centralized and maintainable prompt management system.
-   **Affected Component(s):** A new utility class, `PromptManager`, will be created in `src/utils/prompt_manager.py`. All agents will use this manager instead of loading prompts directly.
-   **Pydantic Model Changes:** None.
-   **`PromptManager` Implementation (`src/utils/prompt_manager.py`):**

    ```python
    import os
    from pathlib import Path
    from typing import Dict, Optional
    from functools import lru_cache
    from jinja2 import Environment, FileSystemLoader

    class PromptManager:
        def __init__(self, prompts_dir: Path = Path("data/prompts")):
            self.prompts_dir = prompts_dir
            if not self.prompts_dir.is_dir():
                raise FileNotFoundError(f"Prompts directory not found at: {self.prompts_dir}")

            # Use Jinja2 for powerful templating
            self.jinja_env = Environment(loader=FileSystemLoader(self.prompts_dir))

        @lru_cache(maxsize=128)
        def _load_template(self, prompt_name: str) -> str:
            """Loads a prompt template from a file using Jinja2."""
            try:
                template = self.jinja_env.get_template(f"{prompt_name}.md")
                return template
            except Exception as e:
                # Log error
                raise FileNotFoundError(f"Prompt '{prompt_name}.md' not found or could not be loaded.") from e

        def get_prompt(self, prompt_name: str, **kwargs) -> str:
            """
            Loads a prompt template and renders it with the provided context variables.

            Args:
                prompt_name: The base name of the prompt file (without .md).
                **kwargs: The context variables to render the template with.

            Returns:
                The rendered prompt as a string.
            """
            try:
                template = self._load_template(prompt_name)
                return template.render(**kwargs)
            except Exception as e:
                # Log error
                # Fallback to a very basic prompt to avoid crashing
                return f"Generate content for the following context: {kwargs}"

    # Singleton instance
    prompt_manager = PromptManager()
    ```

-   **Refactoring Agents:** All agents (e.g., `ParserAgent`, `EnhancedContentWriterAgent`) will be refactored to use the `prompt_manager` singleton.

    **Example in `EnhancedContentWriterAgent`:**
    ```python
    from src.utils.prompt_manager import prompt_manager

    class EnhancedContentWriterAgent:
        def _build_experience_prompt(self, ...):
            # ... (gather context variables like role_info, target_skills)

            # Instead of manually loading and formatting a string:
            # prompt = template.format(...)

            # Use the PromptManager with Jinja2-style variables:
            return prompt_manager.get_prompt(
                prompt_name="resume_role_prompt",
                Target_Skills=target_skills_text,
                batched_structured_output=role_info
            )
    ```

-   **Testing Considerations:**
    -   Unit test the `PromptManager` to ensure it correctly loads templates and renders variables.
    -   Test the case where a prompt file is missing and verify it handles the error gracefully.
    -   Update agent unit tests to mock the `prompt_manager.get_prompt` method instead of file I/O.

---

### **12. Legacy Session Data Migration Strategy**

**Objective:** Ensure the application can gracefully handle potentially incompatible `state.json` files from previous versions after the transition to strict Pydantic models.

-   **Task/Feature Addressed:** Prevents application crashes due to data schema evolution and improves backward compatibility.
-   **Affected Component(s):** `src/core/state_manager.py`. A new module `src/core/migration.py` will be created.
-   **Strategy:** Implement a graceful degradation and optional migration path. The system will attempt to load the state using the new Pydantic models. If it fails due to a validation error, it will try to run a migration function. If migration also fails, it will inform the user and start a new session.

**`src/core/migration.py` Implementation:**

```python
from src.models.data_models import StructuredCV, Section, Subsection, Item
from typing import Dict, Any

def migrate_legacy_state(legacy_data: Dict[str, Any]) -> StructuredCV:
    """
    Attempts to migrate a legacy state dictionary to the new StructuredCV Pydantic model.
    This function will need to be adapted based on the specific structure of the old state.
    """
    # This is a best-effort migration. Add logic here to map old fields to new fields.
    # For example, if old sections were just a list of strings:

    new_sections = []
    if "sections" in legacy_data and isinstance(legacy_data["sections"], list):
        for i, old_section_data in enumerate(legacy_data["sections"]):
            if isinstance(old_section_data, dict):
                 # Assume it's already somewhat structured, try to parse it
                 new_sections.append(Section.model_validate(old_section_data))
            # Add more specific migration rules here as needed.

    migrated_cv = StructuredCV(
        id=legacy_data.get("id", uuid.uuid4()),
        sections=new_sections,
        metadata=legacy_data.get("metadata", {})
    )
    return migrated_cv
```

**`src/core/state_manager.py` - `load_state` Modification:**

```python
# In StateManager class
from pydantic import ValidationError
from src.core.migration import migrate_legacy_state

class StateManager:
    def load_state(self):
        # ... (existing code to find state_file path)
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Primary path: Try to validate against the strict Pydantic model
            self._structured_cv = StructuredCV.model_validate(data)
            self.logger.info("Successfully loaded state using Pydantic model.")
            return self._structured_cv

        except ValidationError as e:
            self.logger.warning(f"State file {self.session_id} is incompatible with current models. Attempting migration.", error=str(e))
            try:
                # Fallback path: Attempt to migrate the legacy data
                migrated_cv = migrate_legacy_state(data)
                self._structured_cv = migrated_cv
                self.save_state() # Save in the new format immediately
                self.logger.info("Successfully migrated and loaded legacy state.")
                return self._structured_cv
            except Exception as migration_error:
                self.logger.error(f"Failed to migrate legacy state file {self.session_id}.", error=str(migration_error))
                # Archive the incompatible file
                os.rename(state_file, f"{state_file}.legacy")
                return None # Indicate failure to load

        except (FileNotFoundError, json.JSONDecodeError) as e:
            # ... (existing error handling)
            return None
```

-   **Testing Considerations:**
    -   Create a sample `state.json` file in the old format.
    -   Write a unit test for `StateManager.load_state()` that points to this legacy file.
    -   Assert that the migration logic is triggered and that a valid `StructuredCV` object is returned.
    -   Write another test with a corrupted/unmigratable legacy file and assert that `load_state` returns `None` and archives the old file.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **13. Project Timeline & Milestone Summary**

This table provides a high-level overview of the phased implementation plan, outlining the primary goal, key deliverables, and estimated duration for each phase. The duration is estimated in 2-week sprints.

| Phase | Primary Goal                                       | Key Deliverables / Milestones                                                                                                                                                             | Estimated Duration (Sprints) |
| :---- | :------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------- |
| **1** | **Foundational Stabilization & Critical Fixes**    | - API key logging vulnerability eliminated.<br>- Core `ParserAgent` and `EnhancedContentWriterAgent` bugs fixed.<br>- Foundational Pydantic models (`StructuredCV`, `JobDescriptionData`) implemented.<br>- Secure, structured logging framework established. | 2 - 3                        |
| **2** | **MVP Core Feature Implementation**                | - Granular, item-by-item processing for Experience & Projects is functional.<br>- Streamlit UI supports hybrid (section/item) review and regeneration.<br>- "Big 10" Key Qualifications generated.<br>- PDF output from scratch is implemented. | 3 - 4                        |
| **3** | **"Smart Agent" Logic & Full SRS Alignment**       | - `ParserAgent` and `ContentWriterAgent` have robust fallback logic for LLM failures.<br>- All MVP-scoped agents (`QA`, `Research`) are integrated into the workflow.<br>- `LLMService` has resilient error handling (e.g., retries).<br>- All agent interfaces are LangGraph-compatible. | 3 - 4                        |
| **4** | **LangGraph Integration & Deployment Preparation** | - Custom orchestrator logic is replaced by a more robust LangGraph workflow.<br>- Comprehensive E2E test suite passes, validating all NFRs.<br>- Application is containerized via `Dockerfile`.<br>- Final user and developer documentation is complete. | 3 - 4                        |

**Total Estimated MVP Timeline:** 11 - 15 Sprints

---

### **14. Conclusion**

This technical blueprint provides a clear, actionable, and prioritized path for transforming the `aicvgen` project from its current inconsistent state into a robust and reliable Minimum Viable Product. By systematically addressing foundational issues such as security vulnerabilities, data model integrity, and core agent logic before layering on complex features, this plan mitigates risk and ensures that development proceeds from a stable base.

The strategic pivot to a granular, item-by-item processing workflow is the architectural cornerstone of this plan. It directly addresses the most critical non-functional requirements: **performance** (by providing a faster, more interactive user experience) and **reliability** (by proactively mitigating LLM rate limits and isolating potential failures).

Upon successful completion of the outlined phases, the expected outcome is not just a functional application, but a well-architected system that is maintainable, scalable, and fully aligned with the business goals detailed in SRS v1.3. The final MVP will empower users with a powerful and intuitive tool for CV tailoring, while providing a solid foundation for future enhancements, including the planned integration of more advanced agentic behaviors with LangGraph.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **15. Appendix A: Detailed `EnhancedOrchestrator` to LangGraph Migration Path**

**Objective:** Provide a step-by-step technical guide for the engineer tasked with migrating the custom `EnhancedOrchestrator` logic to a `langchain_core.runnables.StateGraph` implementation during Phase 4.

**Prerequisites:**
-   All agents have been refactored to have a stateless `run_as_node(self, state: AgentState) -> dict` interface (Task 5.3).
-   The `AgentState` Pydantic model is fully defined in `src/orchestration/state.py` (Task 5.3).

**Migration Steps:**

1.  **Graph Definition (`src/orchestration/cv_workflow_graph.py`):**
    -   Create a new file for the graph definition.
    -   Import `StateGraph`, `END`, and the `AgentState` model.
    -   Instantiate the state graph: `workflow = StateGraph(AgentState)`.

2.  **Node Definition:**
    -   For each agent (`ParserAgent`, `EnhancedContentWriterAgent`, etc.), create a corresponding node function that wraps the agent's `run_as_node` method.
    -   Instantiate each agent once.
    -   Define the nodes within `cv_workflow_graph.py`:

        ```python
        # In src/orchestration/cv_workflow_graph.py
        from .state import AgentState
        from src.agents.parser_agent import ParserAgent
        from src.agents.enhanced_content_writer import EnhancedContentWriterAgent
        # ... import other agents

        # Instantiate agents (they are stateless and can be reused)
        parser_agent = ParserAgent(...)
        content_writer_agent = EnhancedContentWriterAgent(...)
        # ...

        def parse_inputs_node(state: AgentState) -> dict:
            return parser_agent.run_as_node(state)

        def content_writer_node(state: AgentState) -> dict:
            return content_writer_agent.run_as_node(state)

        # ... define nodes for all other agents
        ```

3.  **Add Nodes to Graph:**
    -   Add each defined node function to the `workflow` graph.

        ```python
        workflow.add_node("parse_inputs", parse_inputs_node)
        workflow.add_node("generate_key_quals", content_writer_node) # Can reuse the same node function if the agent's logic handles different content types based on state
        workflow.add_node("generate_experience_item", content_writer_node)
        workflow.add_node("generate_project_item", content_writer_node)
        workflow.add_node("generate_summary", content_writer_node)
        workflow.add_node("format_output", formatter_agent.run_as_node)
        ```

4.  **Define Entry Point and Edges:**
    -   Set the starting node of the graph.

        ```python
        workflow.set_entry_point("parse_inputs")
        ```
    -   Define the static edges for the sequential parts of the workflow.

        ```python
        workflow.add_edge("parse_inputs", "generate_key_quals")
        workflow.add_edge("generate_summary", "format_output")
        workflow.add_edge("format_output", END)
        ```

5.  **Implement Conditional Logic for Iterative Processing:**
    -   This is the most critical step for handling the granular review loops. Define conditional routing functions that inspect the state to decide the next step.

        ```python
        def should_continue_to_experience(state: AgentState) -> str:
            # Logic after Key Quals generation
            # Check if there are experience roles to process
            if state.structured_cv.get_section_by_name("Professional Experience").subsections:
                 # Set up the queue for processing experience items
                state.items_to_process_queue = [sub.id for sub in state.structured_cv.get_section_by_name("Professional Experience").subsections]
                return "generate_experience_item"
            else:
                return "generate_summary" # Skip to summary if no experience

        def route_after_item_generation(state: AgentState) -> str:
            # This node runs after 'generate_experience_item' or 'generate_project_item'
            if state.user_feedback and state.user_feedback.get('action') == 'regenerate':
                # Loop back to the same node for regeneration
                return "generate_experience_item" # or "generate_project_item" based on current section

            if state.items_to_process_queue:
                # If there are more items in the current section's queue, process the next one
                return "generate_experience_item" # or "generate_project_item"
            else:
                # If the queue is empty, decide which section is next
                if state.current_section_key == "Professional Experience":
                    return "continue_to_projects" # Conditional edge to projects
                else:
                    return "generate_summary" # All dynamic items are done
        ```

    -   Add these conditional edges to the graph.

        ```python
        workflow.add_conditional_edges(
            "generate_key_quals",
            should_continue_to_experience,
            {
                "generate_experience_item": "generate_experience_item",
                "generate_summary": "generate_summary"
            }
        )

        # A Human-in-the-loop step would be implicitly handled by the UI updating the state
        # The graph would pause, UI gets user feedback, updates state, and invokes the graph again.
        # The routing function below simulates this decision point.
        workflow.add_conditional_edges(
            "generate_experience_item",
            route_after_item_generation,
            {
                "generate_experience_item": "generate_experience_item", # Loop back for next item
                "continue_to_projects": "generate_project_item", # Move to next section
                "generate_summary": "generate_summary" # Move to summary
            }
        )
        # Add similar routing for projects
        ```

6.  **Compile and Integrate:**
    -   Compile the final graph: `app = workflow.compile()`.
    -   The `EnhancedOrchestrator` is now refactored to be a thin wrapper around this compiled LangGraph `app`. Its `execute_workflow` method will now primarily involve populating the initial `AgentState` from user inputs and then invoking `app.invoke(initial_state)`.

### **16. Appendix B: Post-MVP Enhancement Roadmap**

**Objective:** To outline potential future features and architectural evolution beyond the MVP, providing a long-term vision for the `aicvgen` project.

| Feature / Enhancement                  | Description                                                                                                                                                                                                      | Architectural Impact & Considerations                                                                                                                                                                      |
| :------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Full FastAPI Backend & React Frontend** | Decouple the UI from the backend logic. The FastAPI backend will handle all agent orchestration and state management, exposing RESTful endpoints. A modern React frontend will consume these APIs.                  | - **Major Refactoring:** Requires migrating all Streamlit UI logic to React. `StateManager` logic moves entirely to the backend. <br>- **API Contracts:** Formalize API request/response schemas in `src/models/validation_schemas.py`. <br>- **Authentication:** Implement user authentication (e.g., JWT) to manage user sessions and data securely. |
| **Advanced ResearchAgent Capabilities**    | Enable the `ResearchAgent` to perform web searches (e.g., on LinkedIn, company websites) to gather additional context about the company culture, recent news, and specific role requirements.               | - **New Service Integration:** Add a web scraping service (`BeautifulSoup`, `Scrapy`) or a search API service (`SerpAPI`). <br>- **LangGraph Enhancement:** Add a `web_research` node to the graph. <br>- **Prompt Engineering:** Prompts must be updated to incorporate this new, rich context. |
| **Multi-CV Version Management**            | Allow users to save, name, and manage multiple tailored versions of their CV for different job applications within a single account.                                                                           | - **Database Schema Changes:** Requires `SQLAlchemy` models for `User`, `CVVersion`, and their relationships. The `StructuredCV` Pydantic model will need a `version_name` field. <br>- **UI/API Changes:** New UI components and API endpoints for listing, loading, and deleting CV versions. |
| **Fine-Tuning a Specialized Model**      | For highly repetitive tasks like cleaning and formatting LLM outputs, fine-tune a smaller, faster model (like a Gemini variant) on a dataset of raw/clean pairs to improve accuracy and reduce costs.       | - **MLOps Pipeline:** Requires setting up a pipeline for data collection, training, and deploying the fine-tuned model. <br>- **Model Registry:** A system to manage versions of the fine-tuned model. <br>- **`LLMService` Update:** The service needs to be able to route requests to this custom fine-tuned model endpoint. |
| **Interactive CV Feedback & Scoring**    | The `CVAnalyzerAgent` provides real-time, actionable feedback on user-edited content, scoring it against the job description for keyword density, tone, and impact, guiding the user to improve their edits. | - **Real-Time API:** May require WebSockets for instantaneous feedback as the user types. <br>- **Agent Logic:** The `CVAnalyzerAgent` needs to be enhanced to analyze small text fragments and return structured feedback quickly. <br>- **Complex UI:** The frontend needs to highlight text and display contextual suggestions. |

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

# TASK_BLUEPRINT.md

## **AI CV Generator: MVP Refactoring and Feature Implementation Plan**

### **1. Technical Strategy Overview**

This document outlines the comprehensive technical blueprint for refactoring the `aicvgen` project into a stable, feature-complete Minimum Viable Product (MVP). The plan is based on the requirements specified in `SRS Update for MVP_.txt` (v1.3) and addresses critical issues identified in the codebase, including data model inconsistencies, security vulnerabilities, and agent processing failures.

The core strategy is divided into three prioritized phases:
1.  **Foundational Stabilization:** Remediate critical security flaws, fix core agent bugs, and establish strong data contracts using Pydantic. This creates a reliable base for development.
2.  **MVP Core Feature Implementation:** Implement the primary user-facing features of the MVP, with a focus on the architecturally significant shift to a granular, item-by-item processing workflow.
3.  **"Smart Agent" Logic & Resilience:** Enhance agents with robust fallback mechanisms and prepare the system for future scalability with LangGraph-compatible interfaces.

This plan prioritizes stability, maintainability, and adherence to the MVP requirements, ensuring the delivery of a reliable and robust product.

---

### **2. Phase 1: Foundational Stabilization & Critical Fixes**

**Goal:** Resolve critical security vulnerabilities and core operational blockers; establish stable data foundations with Pydantic; implement secure and informative logging.

#### **2.1. Task: Remediate API Key Logging & Implement Secure Logging**

-   **Task/Feature Addressed:** Critical security vulnerability (CI-001) where API keys are logged in plain text. Implements REQ-NONFUNC-SECURITY-1.
-   **Affected Component(s):** `src/config/logging_config.py`, `src/utils/security_utils.py`, and any module that logs configuration objects (e.g., `src/integration/enhanced_cv_system.py`).
-   **Pydantic Model Changes:** None.
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:** None.
-   **Detailed Implementation Steps:**
    1.  **Enhance Security Utility:** In `src/utils/security_utils.py`, ensure the `redact_sensitive_data` function can recursively traverse nested dictionaries and lists, redacting values for keys matching a predefined sensitive list (e.g., `api_key`, `secret`, `token`).
    2.  **Create a Redaction Filter:** In `src/config/logging_config.py`, create a new `logging.Filter` class named `SensitiveDataFilter`.
    3.  In the `filter` method of `SensitiveDataFilter`, check if the log record has an `extra` attribute. If it does, recursively apply the `redact_sensitive_data` utility to the `record.extra` dictionary. Also, apply redaction to the `record.msg` and `record.args`.
    4.  **Apply Filter:** In the `setup_logging` function within `src/config/logging_config.py`, instantiate `SensitiveDataFilter` and add it to all configured log handlers (e.g., `console_handler`, `file_handler`).
    5.  **Load Keys Securely:** Verify that the `GEMINI_API_KEY` is loaded from a `.env` file using `python-dotenv` within `src/config/environment.py` or `src/config/settings.py` and is never hardcoded.
    6.  **Transition to Structured Logging:** Modify the `logging.Formatter` to produce JSON-formatted logs to improve parsing and analysis. Use `python-json-logger` or a custom JSON formatter. Logs should include contextual identifiers like `session_id` where available.
-   **Testing Considerations:**
    -   Unit test `SensitiveDataFilter` to confirm it redacts keys in nested dictionaries.
    -   Manually inspect all log outputs (`app.log`, console) during application startup and a test workflow run to confirm that no API keys or other sensitive data appear in plain text.
    -   Verify that logs are now in a structured JSON format.
-   **Potential Challenges & Critical Considerations:** Ensure the redaction logic does not significantly impact logging performance. The list of sensitive keys must be comprehensive.

#### **2.2. Task: Pydantic Model Standardization (Foundation)**

-   **Task/Feature Addressed:** Data model inconsistencies and flow irregularities (CI-004), which cause cascading failures (CI-003). This is the foundational fix for the entire data flow and supports REQ-FUNC-PARSE-1.
-   **Affected Component(s):** `src/models/data_models.py`, `src/models/validation_schemas.py`.
-   **Pydantic Model Changes:** Define and/or refactor the core data models using Pydantic to enforce strict data contracts.

    **File:** `src/models/data_models.py`
    ```python
    from pydantic import BaseModel, Field, HttpUrl
    from typing import List, Optional, Dict, Any
    from uuid import UUID, uuid4
    from enum import Enum

    class ItemStatus(str, Enum):
        INITIAL = "initial"
        GENERATED = "generated"
        USER_MODIFIED = "user_modified"
        USER_ACCEPTED = "user_accepted"
        TO_REGENERATE = "to_regenerate"
        GENERATION_FAILED = "generation_failed"
        GENERATED_FALLBACK = "generated_fallback"
        STATIC = "static"

    class ItemType(str, Enum):
        BULLET_POINT = "bullet_point"
        KEY_QUALIFICATION = "key_qualification"
        EXECUTIVE_SUMMARY_PARA = "executive_summary_para"
        EXPERIENCE_ROLE_TITLE = "experience_role_title"
        PROJECT_DESCRIPTION_BULLET = "project_description_bullet"
        EDUCATION_ENTRY = "education_entry"
        CERTIFICATION_ENTRY = "certification_entry"
        LANGUAGE_ENTRY = "language_entry"

    class Item(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        content: str
        status: ItemStatus = ItemStatus.INITIAL
        item_type: ItemType = ItemType.BULLET_POINT
        raw_llm_output: Optional[str] = None  # REQ-FUNC-UI-6
        confidence_score: Optional[float] = None
        metadata: Dict[str, Any] = Field(default_factory=dict)
        user_feedback: Optional[str] = None

    class Subsection(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        name: str  # e.g., "Senior Software Engineer @ TechCorp Inc."
        items: List[Item] = Field(default_factory=list)
        metadata: Dict[str, Any] = Field(default_factory=dict) # e.g., dates, company, location

    class Section(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        name: str # e.g., "Professional Experience"
        content_type: str = "DYNAMIC"  # DYNAMIC or STATIC
        subsections: List[Subsection] = Field(default_factory=list)
        items: List[Item] = Field(default_factory=list) # For sections without subsections
        order: int = 0
        status: ItemStatus = ItemStatus.INITIAL

    class StructuredCV(BaseModel):
        id: UUID = Field(default_factory=uuid4)
        sections: List[Section] = Field(default_factory=list)
        metadata: Dict[str, Any] = Field(default_factory=dict)

    class JobDescriptionData(BaseModel):
        raw_text: str
        skills: List[str] = Field(default_factory=list)
        experience_level: Optional[str] = None
        responsibilities: List[str] = Field(default_factory=list)
        industry_terms: List[str] = Field(default_factory=list)
        company_values: List[str] = Field(default_factory=list)
        error: Optional[str] = None
    ```
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:** None in this task, but subsequent tasks will rely heavily on these models.
-   **Detailed Implementation Steps:**
    1.  Replace the existing dataclasses or dictionaries in `src/models/data_models.py` with the Pydantic models defined above.
    2.  Create corresponding validation schemas in `src/models/validation_schemas.py` that can be used for API-level validation if a FastAPI endpoint is used.
    3.  Ensure all enums (`ItemStatus`, `ItemType`) are defined and used consistently.
-   **Testing Considerations:**
    -   Write unit tests for each Pydantic model to ensure they validate correct data and raise `ValidationError` for incorrect data (e.g., wrong types, missing required fields).
-   **Potential Challenges & Critical Considerations:** This is a foundational change. All parts of the system that create or consume CV or job description data will need to be updated to use these models.

#### **2.3. Task: Core Agent Bug Fixes**

-   **Task/Feature Addressed:** `TypeError` in `ParserAgent` (CI-002) and preparing `EnhancedContentWriterAgent` to avoid `AttributeError` (CI-003).
-   **Affected Component(s):** `src/agents/parser_agent.py`, `src/agents/enhanced_content_writer.py`, `src/services/llm.py`.
-   **Pydantic Model Changes:** Agents will now be expected to return and accept the Pydantic models defined in Task 2.2.
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:**
    -   **`ParserAgent` (`src/agents/parser_agent.py`):**
        1.  Refactor `parse_job_description` to be an `async` method.
        2.  Inside `parse_job_description`, change the LLM call to `response = await self.llm.generate_content(prompt)`.
        3.  The response from the LLM service should be an `LLMResponse` object. Access the content via `response.content`.
        4.  After parsing the JSON from `response.content`, validate it by creating an instance of the `JobDescriptionData` Pydantic model.
        5.  Wrap the entire method in a `try...except` block. On failure, return an `AgentResult` object with `success=False` and a descriptive `error_message`.
    -   **`EnhancedContentWriterAgent` (`src/agents/enhanced_content_writer.py`):**
        1.  As a defensive measure, at the beginning of the `run_async` method, use a `try...except` block with `JobDescriptionData.model_validate()` on the `input_data['job_description_data']` part of the input. This ensures the data structure is correct before proceeding. If validation fails, return a failed `AgentResult` immediately.
-   **Detailed Implementation Steps:**
    1.  Modify `src/agents/parser_agent.py`: Change `def parse_job_description(...)` to `async def parse_job_description(...)`.
    2.  In the same method, change `response = self.llm.generate_content(...)` to `response = await self.llm.generate_content(...)`.
    3.  Update the logic to use `response.content` for JSON parsing.
    4.  Instantiate the `JobDescriptionData` Pydantic model from the parsed dictionary.
    5.  Refactor the `run` method in `ParserAgent` to correctly call the new `async` method and return a validated `JobDescriptionData` object within its result payload.
-   **Testing Considerations:**
    -   Create a unit test for `ParserAgent.parse_job_description` that mocks the `llm.generate_content` call to return a mock `LLMResponse`. Verify the method correctly `await`s the call and parses the `.content`.
    -   Create a unit test for `EnhancedContentWriterAgent.run_async` that passes malformed `job_description_data` (e.g., a string) and asserts that it returns a failed `AgentResult` without hitting an `AttributeError`.

---

### **3. Phase 2: MVP Core Feature Implementation**

**Goal:** Deliver the core MVP functional requirements, focusing on granular processing, "Big 10" skills generation, and UI support for the new workflow.

#### **3.1. Task: Implement Granular, Item-by-Item Processing Workflow**

-   **Task/Feature Addressed:** Implements REQ-FUNC-GEN-3, REQ-FUNC-GEN-4, REQ-FUNC-UI-2, REQ-NONFUNC-PERF-1, and REQ-NONFUNC-RELIABILITY-1.
-   **Affected Component(s):** `src/core/enhanced_orchestrator.py`, `src/agents/enhanced_content_writer.py`, `src/core/state_manager.py`, `src/core/main.py` (Streamlit UI).
-   **Pydantic Model Changes:** `ItemStatus` enum will be heavily used to track the state of each role/project.
-   **LLM Prompt Changes:** `resume_role_prompt.md` and `side_project_prompt.md` will now be populated with the context of a *single* role or project, not an entire section.
-   **Agent Logic Modifications:**
    -   **`EnhancedContentWriterAgent`:**
        1.  Refactor `run_async` to accept the `StructuredCV` and a specific `item_id` (or `subsection_id`) to process.
        2.  The prompt building logic must be updated to only pull context for that single role or project from the `StructuredCV`. This results in smaller, more focused prompts.
        3.  The agent's output should be the updated `content` for just that single `Item` or `Subsection`.
    -   **`EnhancedOrchestrator`:**
        1.  The main `execute_workflow` method needs a new internal loop. After generating Key Qualifications, it should iterate through the `subsections` of the "Professional Experience" `Section` in the `StructuredCV`.
        2.  Inside the loop, for each `Subsection` (role), it calls the refactored `ContentWriterAgent` with the `subsection.id`.
        3.  After the agent returns, the orchestrator updates the corresponding `Subsection` in the `StructuredCV` via the `StateManager` and sets its status to `GENERATED`.
        4.  The orchestrator must then yield control back to the UI to allow for user review of the newly generated item. This requires tight integration with Streamlit's state management.
    -   **UI (`src/core/main.py`):**
        1.  The UI rendering logic must iterate through the `StructuredCV` and display each section.
        2.  For "Professional Experience" and "Side Projects", it must render each `Subsection` (role/project) individually.
        3.  Each rendered role/project must have its own "Accept" and "Regenerate" buttons.
        4.  Clicking "Regenerate" on `subsection-A` should update `subsection-A.status` to `TO_REGENERATE` in the `StateManager` and trigger a rerun of the orchestration logic, which will now pick up this specific item for processing.
-   **Detailed Implementation Steps:**
    1.  Refactor `EnhancedContentWriterAgent.run_async` to accept `item_id_to_process` as an argument.
    2.  Modify the `EnhancedOrchestrator` to loop through experience/project items and call the writer agent for each one.
    3.  Update the Streamlit UI to display items individually and include "Accept"/"Regenerate" buttons for each.
    4.  Implement the callback logic for the "Regenerate" button to update the item's status and re-trigger the orchestrator.
-   **Testing Considerations:**
    -   Integration test: Verify that clicking "Regenerate" for Role B does not affect the content of Role A.
    -   E2E test (using `tests/e2e/test_individual_item_processing.py`): A full user flow of generating content for 3 roles, regenerating the 2nd one, and accepting all.
-   **Potential Challenges & Critical Considerations:** Managing the state between the backend orchestrator and the Streamlit frontend will be complex. Streamlit's execution model requires careful use of `st.session_state` to maintain the `StructuredCV` object and track the current state of the workflow between user interactions.

#### **3.2. Task: Implement "Big 10" Skills & Raw LLM Output Display**

-   **Task/Feature Addressed:** REQ-FUNC-GEN-2 ("Big 10" skills) and REQ-FUNC-UI-6 (Display Raw LLM Output).
-   **Affected Component(s):** `src/agents/enhanced_content_writer.py`, `src/models/data_models.py`, `src/core/main.py`.
-   **Pydantic Model Changes:** The `Item` model in `src/models/data_models.py` already includes `raw_llm_output: Optional[str]`. This will now be populated.
-   **LLM Prompt Changes:** Review `data/prompts/key_qualifications_prompt.md` to ensure it explicitly asks for 10 skills.
-   **Agent Logic Modifications:**
    -   In `EnhancedContentWriterAgent`, whenever a call to `llm.generate_content()` is made, the agent must store the `response.content` string in a temporary variable *before* any cleaning or parsing.
    -   This raw string must then be saved to the `raw_llm_output` field of the corresponding `Item` object being generated.
-   **Detailed Implementation Steps:**
    1.  Rename any functions named `extract_big_6_skills` to `generate_key_qualifications` or similar for clarity.
    2.  In `EnhancedContentWriterAgent`, after receiving a response from the LLM, assign the raw text to `item.raw_llm_output`.
    3.  In `src/core/main.py`, within the UI rendering loop for each generated item, add an `st.expander`.
        ```python
        if item.raw_llm_output:
            with st.expander("Show Raw AI Output"):
                st.code(item.raw_llm_output, language='text')
        ```
-   **Testing Considerations:**
    -   Unit test `EnhancedContentWriterAgent` to confirm the `raw_llm_output` field is populated.
    -   UI test: Manually verify that the expander appears and displays the raw text for a generated item.

---

### **4. Phase 3: "Smart Agent" Logic, Fallbacks, and Full SRS Alignment**

**Goal:** Enhance agent capabilities with robust primary LLM (Gemini) utilization coupled with effective fallback mechanisms; ensure all remaining MVP-scoped SRS v1.3 requirements are met; and prepare agent interfaces for future scalability.

#### **4.1. Task: Implement "Smart Agent" Logic with Robust Fallbacks**

-   **Task/Feature Addressed:** Implements REQ-NONFUNC-RELIABILITY-1 by making agents resilient to LLM failures. Enhances the quality of all `REQ-FUNC-GEN-*` requirements by using a primary LLM (Gemini) effectively.
-   **Affected Component(s):** `src/agents/parser_agent.py`, `src/agents/enhanced_content_writer.py`, `src/services/llm.py`.
-   **Pydantic Model Changes:** The `ItemStatus` enum in `src/models/data_models.py` will be used to flag items generated via fallback (`GENERATED_FALLBACK`) or where generation failed completely (`GENERATION_FAILED`).
-   **LLM Prompt Changes:** None for this task, but the logic determines *if* the prompts are used.
-   **Agent Logic Modifications:**
    -   **`ParserAgent` (`src/agents/parser_agent.py`):**
        1.  In `parse_job_description`, wrap the LLM call and JSON parsing in a `try...except` block.
        2.  **Primary Logic:** Attempt to get a structured response from Gemini.
        3.  **Fallback Logic:** If the `try` block fails (due to API error, timeout, or unparsable JSON), fall back to a simpler, regex-based extraction for essential fields (e.g., find lines containing "skills:", "responsibilities:", etc.). Populate the `JobDescriptionData` object with this partial data and log a warning.
    -   **`EnhancedContentWriterAgent` (`src/agents/enhanced_content_writer.py`):**
        1.  In the `run_async` method (or its helper that processes a single item), wrap the LLM call in a `try...except` block.
        2.  **Primary Logic:** Attempt to generate content for the specific item using Gemini.
        3.  **Fallback Logic:** If the call fails:
            -   Do not halt the entire workflow.
            -   Update the `Item`'s status in the `StructuredCV` to `GENERATION_FAILED`.
            -   Set the `Item`'s `content` to a user-friendly placeholder message (e.g., "AI content generation failed. You can try again or edit this field manually.").
            -   Log the error with the `item_id` for debugging.
-   **Detailed Implementation Steps:**
    1.  In `parser_agent.py`, implement the `try...except` block in `parse_job_description`. Create private helper methods for regex-based parsing as a fallback.
    2.  In `enhanced_content_writer.py`, implement the `try...except` block for the single-item generation logic.
    3.  Define the user-facing error message as a constant.
    4.  Ensure the agent returns a successful `AgentResult` even on fallback, but the content and status of the specific item reflect the failure. This prevents the orchestrator from stopping.
-   **Testing Considerations:**
    -   Unit test the `ParserAgent`'s fallback logic by mocking the LLM service to raise an exception and asserting that the regex-based parsing is invoked.
    -   Unit test the `EnhancedContentWriterAgent`'s fallback by mocking the LLM service to fail for a specific `item_id`. Assert that the corresponding `Item` in the `StructuredCV` has its status set to `GENERATION_FAILED` and its content updated with the placeholder message.

#### **4.2. Task: Integrate Remaining MVP Agents (QA, Research)**

-   **Task/Feature Addressed:** Integrates REQ-FUNC-QA-1 and REQ-FUNC-RESEARCH-X into the workflow.
-   **Affected Component(s):** `src/core/enhanced_orchestrator.py`, `src/agents/quality_assurance_agent.py`, `src/agents/research_agent.py`.
-   **Pydantic Model Changes:** The `Item` model's `metadata` field can be used to store QA scores or research findings.
-   **LLM Prompt Changes:** None.
-   **Agent Logic Modifications:**
    -   **`ResearchAgent` (`src/agents/research_agent.py`):**
        1.  This agent should be called by the orchestrator after the `ParserAgent` has successfully parsed the user's CV and the job description.
        2.  It should take the `StructuredCV` and `JobDescriptionData` as input.
        3.  Logic:
            -   Generate embeddings for each `Item` in the user's base CV and store them in ChromaDB.
            -   Generate an embedding for the key requirements of the `JobDescriptionData`.
            -   This is a preparatory step. The agent's primary use will be to find relevant content *during* the content generation phase.
    -   **`QualityAssuranceAgent` (`src/agents/quality_assurance_agent.py`):**
        1.  This agent should be called by the orchestrator immediately after the `ContentWriterAgent` generates content for an item.
        2.  It takes the newly generated `Item` as input.
        3.  Logic: Perform automated checks as per REQ-FUNC-QA-1 (e.g., check for length constraints, presence of keywords from `JobDescriptionData`).
        4.  Output: Update the `Item`'s `metadata` with a `qa_score` (e.g., 0.0 to 1.0) and a list of any identified issues. This data can be optionally displayed in the UI.
    -   **`EnhancedOrchestrator` (`src/core/enhanced_orchestrator.py`):**
        1.  Modify the main workflow to call the `ResearchAgent` once after the initial parsing step to populate the vector store.
        2.  Modify the item-by-item generation loop. After a successful call to `ContentWriterAgent`, immediately call `QualityAssuranceAgent` with the newly generated item before yielding to the UI.
-   **Testing Considerations:**
    -   Integration test for the `ResearchAgent` to verify that CV content is correctly added to ChromaDB.
    -   Unit test for the `QualityAssuranceAgent` with sample `Item` objects (e.g., one too long, one missing keywords) to verify it correctly identifies issues.

#### **4.3. Task: Finalize LangGraph-Compatible Agent Interfaces**

-   **Task/Feature Addressed:** Strategic refactoring to prepare for future scalability with LangGraph.
-   **Affected Component(s):** All agent classes in `src/agents/`.
-   **Pydantic Model Changes:** A new Pydantic model, `AgentState`, will be created in a new file, e.g., `src/orchestration/state.py`.

    **File:** `src/orchestration/state.py`
    ```python
    from pydantic import BaseModel, Field
    from typing import List, Dict, Optional, Any
    from src.models.data_models import StructuredCV, JobDescriptionData

    class AgentState(BaseModel):
        structured_cv: StructuredCV
        job_description: JobDescriptionData
        current_item_id: Optional[str] = None
        current_section_key: Optional[str] = None
        user_feedback: Optional[Dict[str, Any]] = None
        research_findings: Optional[Dict[str, Any]] = Field(default_factory=dict)
        error_messages: List[str] = Field(default_factory=list)
        final_cv_output_path: Optional[str] = None

        class Config:
            arbitrary_types_allowed = True
    ```
-   **Agent Logic Modifications:**
    -   All agent classes (`ParserAgent`, `EnhancedContentWriterAgent`, etc.) must be refactored.
    -   The primary execution method (e.g., `run_async`) will be deprecated or wrapped.
    -   A new primary execution method will be created with the signature: `def run_as_node(self, state: AgentState) -> dict:`.
    -   This method will take the entire `AgentState` object as input.
    -   It will perform its processing based on the `state`.
    -   It **must not** modify the input `state` object directly.
    -   It must return a dictionary containing **only the fields of `AgentState` that it has changed**. For example, the `ParserAgent` would return `{"structured_cv": ..., "job_description": ...}`.
-   **Detailed Implementation Steps:**
    1.  Create the `src/orchestration/state.py` file with the `AgentState` Pydantic model.
    2.  Go through each agent class in `src/agents/`.
    3.  Create the `run_as_node(self, state: AgentState) -> dict` method in each agent.
    4.  Move the core logic from the old `run_async` method into `run_as_node`.
    5.  Adapt the logic to read inputs from the `state` object and return a dictionary of updates.
-   **Testing Considerations:** Code review is critical for this task to ensure all agents conform to the new stateless, node-like interface.

---

### **5. Phase 4: LangGraph Integration, E2E Testing, and Deployment**

**Goal:** Implement LangGraph for robust orchestration, conduct thorough end-to-end testing, and prepare the application for deployment.

#### **5.1. Task: Integrate LangGraph for Workflow Orchestration**

-   **Task/Feature Addressed:** Replaces the custom procedural loop in `EnhancedOrchestrator` with a more robust, extensible, and stateful graph.
-   **Affected Component(s):** `src/core/enhanced_orchestrator.py` (will be significantly refactored or replaced by a new LangGraph-based orchestrator in `src/orchestration/`).
-   **Detailed Implementation Steps:**
    1.  Create a new module, e.g., `src/orchestration/cv_workflow_graph.py`.
    2.  Import `StateGraph` from `langchain_core.runnables` and the `AgentState` model.
    3.  Instantiate the graph: `workflow = StateGraph(AgentState)`.
    4.  Add nodes to the graph, where each node is a method from an agent (e.g., `workflow.add_node("parse_inputs", parser_agent.run_as_node)`).
    5.  Define the entry point: `workflow.set_entry_point("parse_inputs")`.
    6.  Define conditional edges to manage the iterative review loop for "Professional Experience" and "Side Projects". An edge function will check the state (e.g., `state.user_feedback`) to decide whether to regenerate the current item, move to the next item, or proceed to the next section.
    7.  Compile the graph: `app = workflow.compile()`.
    8.  Refactor `EnhancedOrchestrator` to be a thin wrapper around this compiled `app`. Its `execute_workflow` method will now primarily involve populating the initial `AgentState` from user inputs and then invoking `app.invoke(initial_state)`.
-   **Testing Considerations:**
    -   E2E tests from `tests/e2e/` will be the primary validation method. Tracing the graph's execution for a full CV generation run will confirm correct state transitions.

#### **5.2. Task: End-to-End (E2E) Testing and NFR Validation**

-   **Task/Feature Addressed:** Validates all functional and non-functional requirements from SRS v1.3.
-   **Affected Component(s):** `tests/e2e/test_complete_cv_generation.py`, `tests/e2e/test_individual_item_processing.py`, `tests/e2e/test_error_recovery.py`.
-   **Detailed Implementation Steps:**
    1.  Review and expand the existing E2E test suites to cover all MVP features.
    2.  Create test cases that simulate user interactions: starting from scratch, uploading a CV, generating all sections, regenerating a specific role, editing content, and exporting the final PDF.
    3.  Validate performance against REQ-NONFUNC-PERF-1 by timing the display of the first section and subsequent individual items.
    4.  Validate reliability against REQ-NONFUNC-RELIABILITY-1 by mocking LLM API failures and ensuring the UI displays appropriate messages and allows for retries without crashing.
-   **Testing Considerations:** These tests should be automated and run as part of the CI/CD pipeline to prevent regressions.

#### **5.3. Task: Finalize Documentation and Prepare for Deployment**

-   **Task/Feature Addressed:** Prepares the application for operational readiness.
-   **Affected Component(s):** `README.md`, `Dockerfile`, `docs/`.
-   **Detailed Implementation Steps:**
    1.  Update `README.md` with final setup and usage instructions for the MVP.
    2.  Create comprehensive user documentation in `docs/user/` explaining the workflow and features.
    3.  Update developer documentation in `docs/dev/` to reflect the new architecture, including the Pydantic models and LangGraph workflow.
    4.  Review and test the `Dockerfile` to ensure it builds a runnable image of the final application.
    5.  Prepare deployment scripts or configurations (e.g., for Streamlit Cloud or another hosting service).
-   **Testing Considerations:** The final Docker image should be tested to ensure the application runs correctly in a containerized environment.

#### **5.4. Task: Performance Tuning and Optimization**

-   **Task/Feature Addressed:** Ensures the application meets performance requirements (REQ-NONFUNC-PERF-1) and provides a responsive user experience.
-   **Affected Component(s):** All agents performing LLM calls (`ParserAgent`, `EnhancedContentWriterAgent`), `LLMService`, and the `EnhancedOrchestrator` (LangGraph implementation).
-   **Detailed Implementation Steps:**
    1.  **Profile LLM Calls:** Use logging or a performance monitoring tool to measure the latency of each distinct LLM call (e.g., parsing, key skill generation, single role generation).
    2.  **Identify Bottlenecks:** Analyze the collected data to identify the slowest steps in the workflow.
    3.  **Optimize Prompts:** For any slow-performing LLM calls, review the prompts (`data/prompts/`) for verbosity. Experiment with more concise prompts that yield the same quality output to reduce token processing time.
    4.  **Implement Caching:** In `src/services/llm.py`, implement a caching layer (e.g., using `functools.lru_cache` for in-memory caching or a more robust solution like Redis if scaling is a concern). The cache key should be a hash of the prompt content and model parameters. This is highly effective for regeneration requests where the input hasn't changed.
    5.  **Review Asynchronous Operations:** Ensure that all I/O-bound operations, especially LLM API calls and file I/O, are fully asynchronous to prevent blocking the Streamlit application thread.
-   **Testing Considerations:**
    -   Run the E2E tests before and after optimization to quantify improvements in overall workflow time.
    -   Specifically test the caching by regenerating the same item twice and asserting that the second call is significantly faster and does not trigger an LLM API call.
-   **Potential Challenges & Critical Considerations:** Overly aggressive prompt optimization can degrade output quality. Caching requires careful key management to avoid serving stale content when the underlying context has changed.

---

### **6. Comprehensive Testing Strategy**

This section consolidates the testing approach for the entire refactoring plan.

#### **6.1. Unit Testing**

-   **Pydantic Models (`src/models/`):**
    -   Test successful validation with correct data.
    -   Test `ValidationError` is raised for missing required fields and incorrect data types.
-   **Agents (`src/agents/`):**
    -   Test the `run_as_node(self, state: AgentState)` method for each agent.
    -   Mock the input `AgentState` object.
    -   Assert that the returned dictionary contains the correct updated keys and that other parts of the state are untouched.
    -   **Specifically test fallback logic:** Mock the `LLMService` to raise an API error and assert that the agent's fallback path is executed and returns a correctly structured failure/placeholder output.
-   **LLMService (`src/services/llm.py`):**
    -   Mock the `google-generativeai` client.
    -   Test the retry logic by having the mocked client raise a transient error (e.g., `InternalServerError`) multiple times before succeeding. Verify that the service retries the correct number of times with exponential backoff.
-   **StateManager (`src/core/state_manager.py`):**
    -   Test `save_state` and `load_state` by saving a `StructuredCV` object, loading it back, and asserting that the loaded object is identical to the original.
    -   Test state update methods (e.g., `update_item_status`) and verify the changes are correctly reflected and persisted on save.

#### **6.2. Integration Testing**

-   **ParserAgent âž” ContentWriterAgent:**
    -   Create a test that runs the `ParserAgent` on a raw CV string to produce a `StructuredCV` object.
    -   Pass this `StructuredCV` object into the `EnhancedContentWriterAgent` along with a specific `subsection_id` to process.
    -   Assert that the `ContentWriterAgent` correctly uses the parsed data to generate content for the specified role.
-   **Orchestrator âž” Agents (LangGraph Workflow):**
    -   Run the compiled LangGraph application (`app.invoke(...)`).
    -   Verify that the sequence of agent node calls is correct for the MVP workflow (Parse âž” KeyQuals âž” Loop[ExperienceItem] âž” ...).
    -   Inspect the final `AgentState` to ensure all sections have been processed and the data is aggregated correctly.
-   **UI âž” StateManager:**
    -   Simulate a Streamlit button click (e.g., "Regenerate" for a specific item).
    -   Verify that the corresponding `Item` in the `StateManager`'s `StructuredCV` has its `status` updated to `TO_REGENERATE`.
    -   This test verifies the connection between UI actions and backend state changes.

#### **6.3. End-to-End (E2E) Testing**

-   The tests in `tests/e2e/` provide the framework for this.
-   **Happy Path:** A full workflow run. Input a sample CV and JD, proceed through all generation and review steps, and download the final PDF. Verify the PDF content for correctness and formatting.
-   **Granular Control Path:** For a CV with multiple experience roles, generate content for all. Then, specifically edit one bullet point in the second role, and regenerate the third role entirely. Accept all and export. Verify the final PDF reflects these specific, granular changes correctly.
-   **Error Path:** Use mocks to force the LLM API to fail during the generation of one specific experience role. Verify that the UI shows the "Generation Failed" placeholder for that item but allows the user to continue working on other items.

---

### **7. Security Hardening Plan**

-   **Task/Feature Addressed:** Further harden the application against common vulnerabilities, building on the initial logging fix. Implements REQ-NONFUNC-SECURITY-1.
-   **Affected Component(s):** Primarily affects input handling in `src/core/main.py` (Streamlit) and dependency management.
-   **Detailed Implementation Steps:**
    1.  **Input Sanitization:** Before user-provided text (from CVs or JDs) is rendered in any HTML context (e.g., via `st.markdown(unsafe_allow_html=True)` or before being passed to `WeasyPrint`), it must be sanitized. Use a library like `bleach` to remove potentially malicious HTML/JS tags.
        ```python
        import bleach
        # Before rendering user_provided_text to HTML:
        sanitized_text = bleach.clean(user_provided_text, tags=[], attributes={}, strip=True)
        st.markdown(sanitized_text)
        ```
    2.  **Dependency Scanning:** Integrate a security scanner into the CI/CD pipeline. Use `pip-audit` or `safety` to check `requirements.txt` for known vulnerabilities in third-party packages on every commit or pull request.
        ```bash
        # Example CI step
        pip install pip-audit
        pip-audit
        ```    3.  **Container Security:**
        -   In the `Dockerfile`, add steps to create a non-root user and group.
        -   Use the `USER` instruction to switch to this non-root user before the `CMD` instruction.
            ```dockerfile
            # In Dockerfile
            RUN addgroup --system app && adduser --system --group app
            USER app
            CMD ["streamlit", "run", "src/core/main.py"]
            ```

---

### **8. Deployment & Operational Readiness Plan**

-   **Task/Feature Addressed:** Prepares the application for a repeatable, secure deployment.
-   **Affected Component(s):** `Dockerfile`, `requirements.txt`.
-   **Detailed Implementation Steps:**
    1.  **Finalize Production `Dockerfile`:**
        ```dockerfile
        # Use a slim, secure base image
        FROM python:3.11-slim

        # Install system dependencies for WeasyPrint
        RUN apt-get update && apt-get install -y --no-install-recommends \
            build-essential \
            libffi-dev \
            libcairo2-dev \
            libpango1.0-dev \
            libpangocairo-1.0-0 \
            libgdk-pixbuf2.0-dev \
            libjpeg-dev \
            zlib1g-dev \
            && rm -rf /var/lib/apt/lists/*

        WORKDIR /app

        # Copy only necessary files
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt

        COPY src/ src/
        COPY data/ data/
        COPY app.py .

        # Create non-root user
        RUN addgroup --system app && adduser --system --group app
        USER app

        # Expose the Streamlit port
        EXPOSE 8501

        # Set health check for the container
        HEALTHCHECK CMD streamlit hello

        # Command to run the application
        CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
        ```
    2.  **Configuration Management:**
        -   Create a `.env.example` file in the root directory listing all required environment variables (e.g., `GEMINI_API_KEY`, `LOG_LEVEL`).
        -   Ensure this `.env.example` file is committed to the repository, but the actual `.env` file is listed in `.gitignore`.
        -   The application (e.g., in `src/config/environment.py`) must be configured to read these variables from the environment.
    3.  **Static Asset Handling (Critical for WeasyPrint):**
        -   The `Dockerfile` *must* include the installation of system dependencies required by WeasyPrint for PDF generation (e.g., Pango, Cairo). This is a common point of failure in deployments. The example `Dockerfile` above includes this step.

---

### **9. Appendix A: Detailed `EnhancedOrchestrator` to LangGraph Migration Path**

**Objective:** Provide a step-by-step technical guide for the engineer tasked with migrating the custom `EnhancedOrchestrator` logic to a `langchain_core.runnables.StateGraph` implementation during Phase 4.

**Prerequisites:**
-   All agents have been refactored to have a stateless `run_as_node(self, state: AgentState) -> dict` interface (Task 4.3).
-   The `AgentState` Pydantic model is fully defined in `src/orchestration/state.py` (Task 4.3).

**Migration Steps:**

1.  **Graph Definition (`src/orchestration/cv_workflow_graph.py`):**
    -   Create a new file for the graph definition.
    -   Import `StateGraph`, `END`, and the `AgentState` model.
    -   Instantiate the state graph: `workflow = StateGraph(AgentState)`.

2.  **Node Definition:**
    -   For each agent (`ParserAgent`, `EnhancedContentWriterAgent`, etc.), create a corresponding node function that wraps the agent's `run_as_node` method.
    -   Instantiate each agent once.
    -   Define the nodes within `cv_workflow_graph.py`:

        ```python
        # In src/orchestration/cv_workflow_graph.py
        from .state import AgentState
        from src.agents.parser_agent import ParserAgent
        from src.agents.enhanced_content_writer import EnhancedContentWriterAgent
        # ... import other agents

        # Instantiate agents (they are stateless and can be reused)
        parser_agent = ParserAgent(...)
        content_writer_agent = EnhancedContentWriterAgent(...)
        # ...

        def parse_inputs_node(state: AgentState) -> dict:
            return parser_agent.run_as_node(state)

        def content_writer_node(state: AgentState) -> dict:
            return content_writer_agent.run_as_node(state)

        # ... define nodes for all other agents
        ```

3.  **Add Nodes to Graph:**
    -   Add each defined node function to the `workflow` graph.

        ```python
        workflow.add_node("parse_inputs", parse_inputs_node)
        workflow.add_node("generate_key_quals", content_writer_node) # Can reuse the same node function if the agent's logic handles different content types based on state
        workflow.add_node("generate_experience_item", content_writer_node)
        workflow.add_node("generate_project_item", content_writer_node)
        workflow.add_node("generate_summary", content_writer_node)
        workflow.add_node("format_output", formatter_agent.run_as_node)
        ```

4.  **Define Entry Point and Edges:**
    -   Set the starting node of the graph.

        ```python
        workflow.set_entry_point("parse_inputs")
        ```
    -   Define the static edges for the sequential parts of the workflow.

        ```python
        workflow.add_edge("parse_inputs", "generate_key_quals")
        workflow.add_edge("generate_summary", "format_output")
        workflow.add_edge("format_output", END)
        ```

5.  **Implement Conditional Logic for Iterative Processing:**
    -   This is the most critical step for handling the granular review loops. Define conditional routing functions that inspect the state to decide the next step.

        ```python
        def should_continue_to_experience(state: AgentState) -> str:
            # Logic after Key Quals generation
            # Check if there are experience roles to process
            if state.structured_cv.get_section_by_name("Professional Experience").subsections:
                 # Set up the queue for processing experience items
                state.items_to_process_queue = [sub.id for sub in state.structured_cv.get_section_by_name("Professional Experience").subsections]
                return "generate_experience_item"
            else:
                return "generate_summary" # Skip to summary if no experience

        def route_after_item_generation(state: AgentState) -> str:
            # This node runs after 'generate_experience_item' or 'generate_project_item'
            if state.user_feedback and state.user_feedback.get('action') == 'regenerate':
                # Loop back to the same node for regeneration
                return "generate_experience_item" # or "generate_project_item" based on current section

            if state.items_to_process_queue:
                # If there are more items in the current section's queue, process the next one
                return "generate_experience_item" # or "generate_project_item"
            else:
                # If the queue is empty, decide which section is next
                if state.current_section_key == "Professional Experience":
                    return "continue_to_projects" # Conditional edge to projects
                else:
                    return "generate_summary" # All dynamic items are done
        ```

    -   Add these conditional edges to the graph.

        ```python
        workflow.add_conditional_edges(
            "generate_key_quals",
            should_continue_to_experience,
            {
                "generate_experience_item": "generate_experience_item",
                "generate_summary": "generate_summary"
            }
        )

        # A Human-in-the-loop step would be implicitly handled by the UI updating the state
        # The graph would pause, UI gets user feedback, updates state, and invokes the graph again.
        # The routing function below simulates this decision point.
        workflow.add_conditional_edges(
            "generate_experience_item",
            route_after_item_generation,
            {
                "generate_experience_item": "generate_experience_item", # Loop back for next item
                "continue_to_projects": "generate_project_item", # Move to next section
                "generate_summary": "generate_summary" # Move to summary
            }
        )
        # Add similar routing for projects
        ```

6.  **Compile and Integrate:**
    -   Compile the final graph: `app = workflow.compile()`.
    -   The `EnhancedOrchestrator` is now refactored to be a thin wrapper around this compiled LangGraph `app`. Its `execute_workflow` method will now primarily involve populating the initial `AgentState` from user inputs and then invoking `app.invoke(initial_state)`.

### **10. Appendix B: Post-MVP Enhancement Roadmap**

**Objective:** To outline potential future features and architectural evolution beyond the MVP, providing a long-term vision for the `aicvgen` project.

| Feature / Enhancement                  | Description                                                                                                                                                                                                      | Architectural Impact & Considerations                                                                                                                                                                      |
| :------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Full FastAPI Backend & React Frontend** | Decouple the UI from the backend logic. The FastAPI backend will handle all agent orchestration and state management, exposing RESTful endpoints. A modern React frontend will consume these APIs.                  | - **Major Refactoring:** Requires migrating all Streamlit UI logic to React. `StateManager` logic moves entirely to the backend. <br>- **API Contracts:** Formalize API request/response schemas in `src/models/validation_schemas.py`. <br>- **Authentication:** Implement user authentication (e.g., JWT) to manage user sessions and data securely. |
| **Advanced ResearchAgent Capabilities**    | Enable the `ResearchAgent` to perform web searches (e.g., on LinkedIn, company websites) to gather additional context about the company culture, recent news, and specific role requirements.               | - **New Service Integration:** Add a web scraping service (`BeautifulSoup`, `Scrapy`) or a search API service (`SerpAPI`). <br>- **LangGraph Enhancement:** Add a `web_research` node to the graph. <br>- **Prompt Engineering:** Prompts must be updated to incorporate this new, rich context. |
| **Multi-CV Version Management**            | Allow users to save, name, and manage multiple tailored versions of their CV for different job applications within a single account.                                                                           | - **Database Schema Changes:** Requires `SQLAlchemy` models for `User`, `CVVersion`, and their relationships. The `StructuredCV` Pydantic model will need a `version_name` field. <br>- **UI/API Changes:** New UI components and API endpoints for listing, loading, and deleting CV versions. |
| **Fine-Tuning a Specialized Model**      | For highly repetitive tasks like cleaning and formatting LLM outputs, fine-tune a smaller, faster model (like a Gemini variant) on a dataset of raw/clean pairs to improve accuracy and reduce costs.       | - **MLOps Pipeline:** Requires setting up a pipeline for data collection, training, and deploying the fine-tuned model. <br>- **Model Registry:** A system to manage versions of the fine-tuned model. <br>- **`LLMService` Update:** The service needs to be able to route requests to this custom fine-tuned model endpoint. |
| **Interactive CV Feedback & Scoring**    | The `CVAnalyzerAgent` provides real-time, actionable feedback on user-edited content, scoring it against the job description for keyword density, tone, and impact, guiding the user to improve their edits. | - **Real-Time API:** May require WebSockets for instantaneous feedback as the user types. <br>- **Agent Logic:** The `CVAnalyzerAgent` needs to be enhanced to analyze small text fragments and return structured feedback quickly. <br>- **Complex UI:** The frontend needs to highlight text and display contextual suggestions. |

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

 ---

---

### **13. Project Timeline & Milestone Summary**

This table provides a high-level overview of the phased implementation plan, outlining the primary goal, key deliverables, and estimated duration for each phase. The duration is estimated in 2-week sprints.

| Phase | Primary Goal                                       | Key Deliverables / Milestones                                                                                                                                                             | Estimated Duration (Sprints) |
| :---- | :------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------- |
| **1** | **Foundational Stabilization & Critical Fixes**    | - API key logging vulnerability eliminated.<br>- Core `ParserAgent` and `EnhancedContentWriterAgent` bugs fixed.<br>- Foundational Pydantic models (`StructuredCV`, `JobDescriptionData`) implemented.<br>- Secure, structured logging framework established. | 2 - 3                        |
| **2** | **MVP Core Feature Implementation**                | - Granular, item-by-item processing for Experience & Projects is functional.<br>- Streamlit UI supports hybrid (section/item) review and regeneration.<br>- "Big 10" Key Qualifications generated.<br>- PDF output from scratch is implemented. | 3 - 4                        |
| **3** | **"Smart Agent" Logic & Full SRS Alignment**       | - `ParserAgent` and `ContentWriterAgent` have robust fallback logic for LLM failures.<br>- All MVP-scoped agents (`QA`, `Research`) are integrated into the workflow.<br>- `LLMService` has resilient error handling (e.g., retries).<br>- All agent interfaces are LangGraph-compatible. | 3 - 4                        |
| **4** | **LangGraph Integration & Deployment Preparation** | - Custom orchestrator logic is replaced by a more robust LangGraph workflow.<br>- Comprehensive E2E test suite passes, validating all NFRs.<br>- Application is containerized via `Dockerfile`.<br>- Final user and developer documentation is complete. | 3 - 4                        |

**Total Estimated MVP Timeline:** 11 - 15 Sprints

---

### **14. Conclusion**

This technical blueprint provides a clear, actionable, and prioritized path for transforming the `aicvgen` project from its current inconsistent state into a robust and reliable Minimum Viable Product. By systematically addressing foundational issues such as security vulnerabilities, data model integrity, and core agent logic before layering on complex features, this plan mitigates risk and ensures that development proceeds from a stable base.

The strategic pivot to a granular, item-by-item processing workflow is the architectural cornerstone of this plan. It directly addresses the most critical non-functional requirements: **performance** (by providing a faster, more interactive user experience) and **reliability** (by proactively mitigating LLM rate limits and isolating potential failures).

Upon successful completion of the outlined phases, the expected outcome is not just a functional application, but a well-architected system that is maintainable, scalable, and fully aligned with the business goals detailed in SRS v1.3. The final MVP will empower users with a powerful and intuitive tool for CV tailoring, while providing a solid foundation for future enhancements, including the planned integration of more advanced agentic behaviors with LangGraph.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **15. Appendix A: Detailed `EnhancedOrchestrator` to LangGraph Migration Path**

**Objective:** Provide a step-by-step technical guide for the engineer tasked with migrating the custom `EnhancedOrchestrator` logic to a `langchain_core.runnables.StateGraph` implementation during Phase 4.

**Prerequisites:**
-   All agents have been refactored to have a stateless `run_as_node(self, state: AgentState) -> dict` interface (Task 5.3).
-   The `AgentState` Pydantic model is fully defined in `src/orchestration/state.py` (Task 5.3).

**Migration Steps:**

1.  **Graph Definition (`src/orchestration/cv_workflow_graph.py`):**
    -   Create a new file for the graph definition.
    -   Import `StateGraph`, `END`, and the `AgentState` model.
    -   Instantiate the state graph: `workflow = StateGraph(AgentState)`.

2.  **Node Definition:**
    -   For each agent (`ParserAgent`, `EnhancedContentWriterAgent`, etc.), create a corresponding node function that wraps the agent's `run_as_node` method.
    -   Instantiate each agent once.
    -   Define the nodes within `cv_workflow_graph.py`:

        ```python
        # In src/orchestration/cv_workflow_graph.py
        from .state import AgentState
        from src.agents.parser_agent import ParserAgent
        from src.agents.enhanced_content_writer import EnhancedContentWriterAgent
        # ... import other agents

        # Instantiate agents (they are stateless and can be reused)
        parser_agent = ParserAgent(...)
        content_writer_agent = EnhancedContentWriterAgent(...)
        # ...

        def parse_inputs_node(state: AgentState) -> dict:
            # This node calls the parser agent and updates the state
            # with structured_cv and job_description.
            return parser_agent.run_as_node(state)

        def content_writer_node(state: AgentState) -> dict:
            # This node can be used for various content generation tasks.
            # The agent's internal logic will use state.current_section_key
            # and state.current_item_id to decide what to generate.
            return content_writer_agent.run_as_node(state)

        # ... define nodes for all other agents (formatter, qa, etc.)
        ```

3.  **Add Nodes to Graph:**
    -   Add each defined node function to the `workflow` graph.

        ```python
        workflow.add_node("parse_inputs", parse_inputs_node)
        workflow.add_node("generate_key_quals", content_writer_node)
        workflow.add_node("generate_experience_item", content_writer_node)
        workflow.add_node("generate_project_item", content_writer_node)
        workflow.add_node("generate_summary", content_writer_node)
        workflow.add_node("format_output", formatter_agent.run_as_node)
        ```

4.  **Define Entry Point and Edges:**
    -   Set the starting node of the graph.

        ```python
        workflow.set_entry_point("parse_inputs")
        ```
    -   Define the static edges for the sequential parts of the workflow.

        ```python
        workflow.add_edge("parse_inputs", "generate_key_quals")
        workflow.add_edge("generate_summary", "format_output")
        workflow.add_edge("format_output", END)
        ```

5.  **Implement Conditional Logic for Iterative Processing:**
    -   Define conditional routing functions that inspect the state to decide the next step. This is the core of managing the iterative loops.

        ```python
        def should_continue_to_experience(state: AgentState) -> str:
            """Logic after Key Quals generation. Determines if there are experience roles to process."""
            experience_section = state.structured_cv.get_section_by_name("Professional Experience")
            if experience_section and experience_section.subsections:
                # Prepare the queue for processing experience items
                # Note: State modification should ideally happen in a node, not an edge.
                # A better pattern is to have a "prepare_experience" node.
                return "prepare_experience"
            else:
                # No experience, skip to projects or summary
                return "should_continue_to_projects"

        def route_after_item_generation(state: AgentState) -> str:
            """Routes workflow after an item has been generated, allowing for user review loop."""
            # In a real app, the graph would pause here. The UI would update the state
            # with user_feedback. For this blueprint, we simulate the logic.
            if state.user_feedback and state.user_feedback.get('action') == 'regenerate':
                # Loop back to the same node for regeneration
                return "generate_experience_item" # or "generate_project_item" based on state.current_section_key

            # Check if there are more items in the current section's queue
            if state.items_to_process_queue:
                return "generate_experience_item" # or "generate_project_item"
            else:
                # Queue for the current section is empty, move to the next section
                if state.current_section_key == "Professional Experience":
                    return "should_continue_to_projects"
                else: # Assumes projects were last
                    return "generate_summary"
        ```

    -   Add these conditional edges to the graph.

        ```python
        workflow.add_conditional_edges(
            "generate_key_quals",
            should_continue_to_experience,
            {
                "prepare_experience": "prepare_experience_node", # A new node to set up the queue
                "should_continue_to_projects": "should_continue_to_projects" # Another conditional edge
            }
        )

        # The loop for experience items
        workflow.add_edge("prepare_experience_node", "generate_experience_item")
        workflow.add_conditional_edges("generate_experience_item", route_after_item_generation, ...)
        ```

6.  **Compile and Integrate:**
    -   Compile the final graph: `app = workflow.compile()`.
    -   Refactor the `EnhancedOrchestrator` to become a thin wrapper that initializes and invokes this compiled `app`. The orchestrator's primary role will be to manage the interaction between the UI (getting user input and feedback) and the LangGraph application (invoking the graph with updated state).

### **16. Appendix B: Proposed Database Schema (SQLAlchemy)**

**Objective:** To define a preliminary database schema for persisting user data, CV sessions, and documents, which will be essential for post-MVP features like user accounts and multi-CV management.

-   **Rationale:** While the MVP uses file-based session management (`StateManager`), a structured database is necessary for scalability. Defining the schema now ensures that data models and future logic align with long-term persistence goals. `SQLAlchemy` is chosen for its flexibility and ORM capabilities.
-   **File:** `src/models/db_models.py` (New File)

    ```python
    from sqlalchemy import (create_engine, Column, Integer, String, Text, DateTime,
                            ForeignKey, JSON, Enum as SQLEnum)
    from sqlalchemy.orm import relationship, sessionmaker
    from sqlalchemy.ext.declarative import declarative_base
    from datetime import datetime
    from enum import Enum as PyEnum

    Base = declarative_base()

    class User(Base):
        __tablename__ = 'users'
        id = Column(Integer, primary_key=True)
        email = Column(String(255), unique=True, nullable=False, index=True)
        hashed_password = Column(String(255), nullable=False)
        created_at = Column(DateTime, default=datetime.utcnow)
        cv_sessions = relationship("CVSession", back_populates="user")

    class CVSessionStatus(PyEnum):
        ACTIVE = "active"
        COMPLETED = "completed"
        ARCHIVED = "archived"

    class CVSession(Base):
        __tablename__ = 'cv_sessions'
        id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
        user_id = Column(Integer, ForeignKey('users.id'), nullable=True, index=True)
        status = Column(SQLEnum(CVSessionStatus), nullable=False, default=CVSessionStatus.ACTIVE)
        created_at = Column(DateTime, default=datetime.utcnow)
        updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
        # Store the entire AgentState or StructuredCV as JSON
        # For performance, larger objects might be stored in a file/blob storage
        # with a reference stored here.
        state_json = Column(JSON, nullable=False)

        user = relationship("User", back_populates="cv_sessions")
        documents = relationship("CVDocument", back_populates="session")

    class CVDocumentFormat(PyEnum):
        PDF = "pdf"
        MARKDOWN = "md"
        HTML = "html"

    class CVDocument(Base):
        __tablename__ = 'cv_documents'
        id = Column(Integer, primary_key=True)
        session_id = Column(String(36), ForeignKey('cv_sessions.id'), nullable=False, index=True)
        version = Column(Integer, default=1, nullable=False)
        format = Column(SQLEnum(CVDocumentFormat), nullable=False)
        # Store path to the generated file in blob storage (e.g., S3 path) or local file system
        file_path = Column(String(1024), nullable=False)
        created_at = Column(DateTime, default=datetime.utcnow)

        session = relationship("CVSession", back_populates="documents")
    ```

### **17. Appendix C: Post-MVP Enhancement Roadmap**

**Objective:** To outline potential future features and architectural evolution beyond the MVP, providing a long-term vision for the `aicvgen` project.

| Feature / Enhancement                  | Description                                                                                                                                                                                                      | Architectural Impact & Considerations                                                                                                                                                                      |
| :------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Full FastAPI Backend & React Frontend** | Decouple the UI from the backend logic. The FastAPI backend will handle all agent orchestration and state management, exposing RESTful endpoints. A modern React frontend will consume these APIs.                  | - **Major Refactoring:** Requires migrating all Streamlit UI logic to React. `StateManager` logic moves entirely to the backend. <br>- **API Contracts:** Formalize API request/response schemas in `src/models/validation_schemas.py`. <br>- **Authentication:** Implement user authentication (e.g., JWT) to manage user sessions and data securely. |
| **Advanced ResearchAgent Capabilities**    | Enable the `ResearchAgent` to perform web searches (e.g., on LinkedIn, company websites) to gather additional context about the company culture, recent news, and specific role requirements.               | - **New Service Integration:** Add a web scraping service (`BeautifulSoup`, `Scrapy`) or a search API service (`SerpAPI`). <br>- **LangGraph Enhancement:** Add a `web_research` node to the graph. <br>- **Prompt Engineering:** Prompts must be updated to incorporate this new, rich context. |
| **Multi-CV Version Management**            | Allow users to save, name, and manage multiple tailored versions of their CV for different job applications within a single account.                                                                           | - **Database Schema Changes:** Requires `SQLAlchemy` models for `User`, `CVSession`, and their relationships. The `StructuredCV` Pydantic model will need a `version_name` field. <br>- **UI/API Changes:** New UI components and API endpoints for listing, loading, and deleting CV versions. |
| **Fine-Tuning a Specialized Model**      | For highly repetitive tasks like cleaning and formatting LLM outputs, fine-tune a smaller, faster model (like a Gemini variant) on a dataset of raw/clean pairs to improve accuracy and reduce costs.       | - **MLOps Pipeline:** Requires setting up a pipeline for data collection, training, and deploying the fine-tuned model. <br>- **Model Registry:** A system to manage versions of the fine-tuned model. <br>- **`LLMService` Update:** The service needs to be able to route requests to this custom fine-tuned model endpoint. |
| **Interactive CV Feedback & Scoring**    | The `CVAnalyzerAgent` provides real-time, actionable feedback on user-edited content, scoring it against the job description for keyword density, tone, and impact, guiding the user to improve their edits. | - **Real-Time API:** May require WebSockets for instantaneous feedback as the user types. <br>- **Agent Logic:** The `CVAnalyzerAgent` needs to be enhanced to analyze small text fragments and return structured feedback quickly. <br>- **Complex UI:** The frontend needs to highlight text and display contextual suggestions. |

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **10. Detailed Implementation Guides for Key Components (Continued)**

#### **10.3. `LLMService` Enhancements for Resilience**

**Objective:** To make the `LLMService` in `src/services/llm.py` more robust by implementing sophisticated error handling, retry logic, and API key fallbacks.

**`src/services/llm.py` - Implementation Snippet:**

```python
# In EnhancedLLMService class
import random
import google.api_core.exceptions

class EnhancedLLMService:
    def __init__(self, ...):
        # ... (existing init code)
        # Add a flag to track which API key is in use
        self.api_key_in_use = "primary" # or "user", "fallback"
        self._configure_llm_client()

    def _configure_llm_client(self):
        """Configures the Gemini client with the current API key."""
        api_key_to_use = self.user_api_key or self.primary_api_key
        self.api_key_in_use = "user" if self.user_api_key else "primary"

        if not api_key_to_use:
            api_key_to_use = self.fallback_api_key
            self.api_key_in_use = "fallback"

        if not api_key_to_use:
            self.logger.critical("No valid API key available.")
            raise ValueError("LLM Service cannot be initialized without an API key.")

        genai.configure(api_key=api_key_to_use)
        self.llm = genai.GenerativeModel(self.model_name)
        self.logger.info(f"LLM Service configured with '{self.api_key_in_use}' key.")

    def _switch_to_fallback_key(self) -> bool:
        """Switches to the fallback API key if available."""
        if self.api_key_in_use != "fallback" and self.fallback_api_key:
            self.logger.warning("Attempting to switch to fallback API key due to persistent errors.")
            self.api_key_in_use = "fallback"
            self._configure_llm_client()
            return True
        self.logger.error("Could not switch to fallback key. Either already using it or none is configured.")
        return False

    async def generate_content(self, prompt: str, ..., max_retries: int = 3) -> LLMResponse:
        # ... (existing setup code)
        attempt = 0
        while attempt <= max_retries:
            try:
                # ... (existing logic to generate content)
                # On successful response:
                return LLMResponse(...)

            except (google.api_core.exceptions.ResourceExhausted,
                    google.api_core.exceptions.InternalServerError,
                    google.api_core.exceptions.ServiceUnavailable) as e:

                attempt += 1
                if attempt > max_retries:
                    self.logger.error(f"LLM call failed after {max_retries} retries.", error=str(e))
                    # Try switching to fallback key as a final recovery attempt
                    if self._switch_to_fallback_key():
                        attempt = max_retries # Allow one final attempt on the fallback key
                        continue
                    return LLMResponse(success=False, error_message=str(e), ...)

                # Exponential backoff
                delay = (2 ** attempt) + (random.uniform(0, 1)) # Add jitter
                self.logger.warning(f"LLM API error encountered. Retrying in {delay:.2f} seconds...", error=str(e), attempt=attempt)
                await asyncio.sleep(delay)

            except Exception as e:
                self.logger.error(f"Unhandled LLM error: {e}")
                return LLMResponse(success=False, error_message=str(e), ...)

        return LLMResponse(success=False, error_message="Exhausted all retry attempts.", ...)
```

---

### **11. Prompt Management and Versioning Strategy**

**Objective:** Establish a robust system for managing, versioning, and dynamically loading LLM prompts, decoupling them from the application code as per REQ-NONFUNC-MAINTAIN-2.

-   **Task/Feature Addressed:** Implements a centralized and maintainable prompt management system.
-   **Affected Component(s):** A new utility class, `PromptManager`, will be created in `src/utils/prompt_manager.py`. All agents will use this manager instead of loading prompts directly.
-   **Pydantic Model Changes:** None.
-   **`PromptManager` Implementation (`src/utils/prompt_manager.py`):**

    ```python
    import os
    from pathlib import Path
    from typing import Dict, Optional
    from functools import lru_cache
    from jinja2 import Environment, FileSystemLoader

    class PromptManager:
        def __init__(self, prompts_dir: Path = Path("data/prompts")):
            self.prompts_dir = prompts_dir
            if not self.prompts_dir.is_dir():
                raise FileNotFoundError(f"Prompts directory not found at: {self.prompts_dir}")

            # Use Jinja2 for powerful templating
            self.jinja_env = Environment(loader=FileSystemLoader(self.prompts_dir))

        @lru_cache(maxsize=128)
        def _load_template(self, prompt_name: str) -> str:
            """Loads a prompt template from a file using Jinja2."""
            try:
                template = self.jinja_env.get_template(f"{prompt_name}.md")
                return template
            except Exception as e:
                # Log error
                raise FileNotFoundError(f"Prompt '{prompt_name}.md' not found or could not be loaded.") from e

        def get_prompt(self, prompt_name: str, **kwargs) -> str:
            """
            Loads a prompt template and renders it with the provided context variables.

            Args:
                prompt_name: The base name of the prompt file (without .md).
                **kwargs: The context variables to render the template with.

            Returns:
                The rendered prompt as a string.
            """
            try:
                template = self._load_template(prompt_name)
                return template.render(**kwargs)
            except Exception as e:
                # Log error
                # Fallback to a very basic prompt to avoid crashing
                return f"Generate content for the following context: {kwargs}"

    # Singleton instance
    prompt_manager = PromptManager()
    ```

-   **Refactoring Agents:** All agents (e.g., `ParserAgent`, `EnhancedContentWriterAgent`) will be refactored to use the `prompt_manager` singleton.

    **Example in `EnhancedContentWriterAgent`:**
    ```python
    from src.utils.prompt_manager import prompt_manager

    class EnhancedContentWriterAgent:
        def _build_experience_prompt(self, ...):
            # ... (gather context variables like role_info, target_skills)

            # Instead of manually loading and formatting a string:
            # prompt = template.format(...)

            # Use the PromptManager with Jinja2-style variables:
            return prompt_manager.get_prompt(
                prompt_name="resume_role_prompt",
                Target_Skills=target_skills_text,
                batched_structured_output=role_info
            )
    ```

-   **Testing Considerations:**
    -   Unit test the `PromptManager` to ensure it correctly loads templates and renders variables.
    -   Test the case where a prompt file is missing and verify it handles the error gracefully.
    -   Update agent unit tests to mock the `prompt_manager.get_prompt` method instead of file I/O.

---

### **12. Legacy Session Data Migration Strategy**

**Objective:** Ensure the application can gracefully handle potentially incompatible `state.json` files from previous versions after the transition to strict Pydantic models.

-   **Task/Feature Addressed:** Prevents application crashes due to data schema evolution and improves backward compatibility.
-   **Affected Component(s):** `src/core/state_manager.py`. A new module `src/core/migration.py` will be created.
-   **Strategy:** Implement a graceful degradation and optional migration path. The system will attempt to load the state using the new Pydantic models. If it fails due to a validation error, it will try to run a migration function. If migration also fails, it will inform the user and start a new session.

**`src/core/migration.py` Implementation:**

```python
from src.models.data_models import StructuredCV, Section, Subsection, Item
from typing import Dict, Any

def migrate_legacy_state(legacy_data: Dict[str, Any]) -> StructuredCV:
    """
    Attempts to migrate a legacy state dictionary to the new StructuredCV Pydantic model.
    This function will need to be adapted based on the specific structure of the old state.
    """
    # This is a best-effort migration. Add logic here to map old fields to new fields.
    # For example, if old sections were just a list of strings:

    new_sections = []
    if "sections" in legacy_data and isinstance(legacy_data["sections"], list):
        for i, old_section_data in enumerate(legacy_data["sections"]):
            if isinstance(old_section_data, dict):
                 # Assume it's already somewhat structured, try to parse it
                 new_sections.append(Section.model_validate(old_section_data))
            # Add more specific migration rules here as needed.

    migrated_cv = StructuredCV(
        id=legacy_data.get("id", uuid.uuid4()),
        sections=new_sections,
        metadata=legacy_data.get("metadata", {})
    )
    return migrated_cv
```

**`src/core/state_manager.py` - `load_state` Modification:**

```python
# In StateManager class
from pydantic import ValidationError
from src.core.migration import migrate_legacy_state

class StateManager:
    def load_state(self):
        # ... (existing code to find state_file path)
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Primary path: Try to validate against the strict Pydantic model
            self._structured_cv = StructuredCV.model_validate(data)
            self.logger.info("Successfully loaded state using Pydantic model.")
            return self._structured_cv

        except ValidationError as e:
            self.logger.warning(f"State file {self.session_id} is incompatible with current models. Attempting migration.", error=str(e))
            try:
                # Fallback path: Attempt to migrate the legacy data
                migrated_cv = migrate_legacy_state(data)
                self._structured_cv = migrated_cv
                self.save_state() # Save in the new format immediately
                self.logger.info("Successfully migrated and loaded legacy state.")
                return self._structured_cv
            except Exception as migration_error:
                self.logger.error(f"Failed to migrate legacy state file {self.session_id}.", error=str(migration_error))
                # Archive the incompatible file
                os.rename(state_file, f"{state_file}.legacy")
                return None # Indicate failure to load

        except (FileNotFoundError, json.JSONDecodeError) as e:
            # ... (existing error handling)
            return None
```

-   **Testing Considerations:**
    -   Create a sample `state.json` file in the old format.
    -   Write a unit test for `StateManager.load_state()` that points to this legacy file.
    -   Assert that the migration logic is triggered and that a valid `StructuredCV` object is returned.
    -   Write another test with a corrupted/unmigratable legacy file and assert that `load_state` returns `None` and archives the old file.

---

### **13. Appendix D: Developer Onboarding & Workflow Guide**

**Objective:** To provide a clear, actionable guide for engineers to set up their local development environment and adhere to project standards.

#### **13.1. Local Development Environment Setup**

1.  **Clone the Repository:**
    ```bash
    git clone <repository_url>
    cd anasakhomach-aicvgen
    ```
2.  **Python Version:** The project is standardized on **Python 3.11**. Ensure you have this version installed.
3.  **Create a Virtual Environment:** It is strongly recommended to use a virtual environment.
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    ```
4.  **Install Dependencies:** Install all required packages from `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```
5.  **Configure Environment Variables:**
    -   Copy the example environment file: `cp .env.example .env`
    -   Open the newly created `.env` file in a text editor.
    -   Add your Gemini API key: `GEMINI_API_KEY="AIzaSy...your...key..."`
6.  **Run the Application:** Launch the Streamlit UI.
    ```bash
    streamlit run app.py
    ```    The application should now be accessible in your web browser, typically at `http://localhost:8501`.

#### **13.2. Code Quality, Formatting, and Linting**

To maintain code consistency and quality, the following tools are enforced. Run these checks before committing your code.

1.  **Code Formatter (`black`):** Automatically formats all Python code to a consistent style.
    ```bash
    pip install black
    black .
    ```
2.  **Import Sorter (`isort`):** Automatically sorts and formats import statements.
    ```bash
    pip install isort
    isort .
    ```
3.  **Linter (`pylint`):** Analyzes code for bugs, errors, and stylistic problems based on the rules in `.pylintrc`.
    ```bash
    pip install pylint
    pylint src/
    ```

#### **13.3. Branching and Commit Strategy**

This project follows a simplified GitFlow model.

1.  **Branches:**
    -   `main`: Contains production-ready, stable code. Direct commits are forbidden.
    -   `develop`: The primary integration branch for new features. All feature branches are merged into `develop`.
    -   `feature/<task-id>-<short-description>`: All new work must be done on a feature branch created from `develop`.
        -   *Example:* `git checkout -b feature/T3.1-granular-processing`

2.  **Workflow:**
    -   Create your feature branch from the latest `develop`.
    -   Make small, atomic commits with clear, descriptive messages (e.g., `feat(parser): Add fallback logic for JD parsing`).
    -   Once the feature is complete and passes all local tests, push your branch to the remote repository.
    -   Open a Pull Request (PR) from your feature branch into the `develop` branch.
    -   The PR will be reviewed, and once approved, it will be merged into `develop`.

#### **13.4. API Documentation (FastAPI)**

While the MVP focuses on the Streamlit UI, the `src/api` directory contains a FastAPI application. For any future development on these API endpoints:

-   **Leverage Auto-documentation:** All new endpoints **must** use Pydantic models for request bodies and responses. This allows FastAPI to automatically generate interactive API documentation (Swagger UI and ReDoc).
-   **Accessing Docs:** Once the FastAPI application is running (e.g., via `uvicorn src.api.main:app`), the documentation will be available at `/docs` (Swagger) and `/redoc` (ReDoc).
-   **Contract:** The OpenAPI schema generated by FastAPI serves as the official API contract.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **18. Appendix E: Detailed Data Flow and State Management**

**Objective:** To provide a clear, step-by-step illustration of the data flow through the refactored MVP system and to specify the interaction pattern between the `StateManager` and Streamlit's `st.session_state`.

#### **18.1. MVP Data Flow Diagram (Textual Representation)**

This sequence describes the journey of data from user input to final output within the proposed granular workflow.

1.  **User Input (Streamlit UI):**
    -   `st.text_area` captures `raw_cv_text` (string) and `raw_jd_text` (string).
    -   `st.button("Generate CV")` triggers the orchestration.
    -   **Data Handed Off:** `raw_cv_text`, `raw_jd_text`.

2.  **`ParserAgent` (Node 1):**
    -   **Input:** `{ "raw_cv_text": str, "raw_jd_text": str }`.
    -   **Processing:**
        -   Uses Gemini to parse `raw_jd_text`.
        -   Uses Gemini with the `experience_segmentation_prompt.md` to parse `raw_cv_text`.
    -   **Output:** `{ "structured_cv": StructuredCV, "job_description": JobDescriptionData }`. This dictionary updates the `AgentState`.

3.  **`EnhancedContentWriterAgent` (for Key Qualifications - Node 2):**
    -   **Input (from `AgentState`):** Reads `state.structured_cv`, `state.job_description`.
    -   **Processing:**
        -   The agent's internal logic identifies that "Key Qualifications" is the current target.
        -   Generates a prompt using `key_qualifications_prompt.md`.
        -   Calls LLM.
        -   Populates the `items` list of the "Key Qualifications" `Section` in a *new* `StructuredCV` object. Each item gets `content`, `raw_llm_output`, and status `GENERATED`.
    -   **Output:** `{ "structured_cv": StructuredCV }` (the updated object).

4.  **Orchestrator/UI Loop (Human-in-the-Loop for Experience):**
    -   The UI renders the generated Key Qualifications. The workflow now prepares for the "Professional Experience" loop.
    -   The orchestrator (or a preparatory LangGraph node) identifies the first `Subsection` (role) to process and sets `state.current_item_id`.

5.  **`EnhancedContentWriterAgent` (for a Single Experience Role - Node 3):**
    -   **Input (from `AgentState`):** Reads `state.structured_cv`, `state.job_description`, and `state.current_item_id`.
    -   **Processing:**
        -   Finds the specific `Subsection` corresponding to `current_item_id`.
        -   Builds a focused prompt using `resume_role_prompt.md` with only the context for that single role.
        -   Calls LLM.
        -   Updates the `content` and `raw_llm_output` for the `Items` within that `Subsection`. Sets their status to `GENERATED`.
    -   **Output:** `{ "structured_cv": StructuredCV }`.

6.  **UI Review / User Feedback:**
    -   The UI re-renders, now showing the newly generated content for the specific role.
    -   The user clicks "Accept" or "Regenerate".
    -   **Data Handed Off:** The UI action updates `st.session_state`, which in turn modifies the `AgentState` with user feedback (e.g., `{"user_feedback": {"action": "regenerate", "item_id": "..."}}`). The graph is invoked again.

7.  **Loop/Continuation (Conditional Edge):**
    -   The conditional routing logic in the LangGraph graph checks `state.user_feedback`.
    -   It either routes back to `generate_experience_item` (for regeneration or the next item) or proceeds to the next section (e.g., "Side Projects").

8.  **... (Loop continues for all roles and projects) ...**

9.  **`EnhancedContentWriterAgent` (for Executive Summary - Node 4):**
    -   **Input (from `AgentState`):** Reads the now fully populated `state.structured_cv`.
    -   **Processing:** Gathers all accepted/edited content to form a rich context for the `executive_summary_prompt.md`. Calls LLM.
    -   **Output:** `{ "structured_cv": StructuredCV }` (with the summary section updated).

10. **`FormatterAgent` (Final Node):**
    -   **Input (from `AgentState`):** Reads the final `state.structured_cv`.
    -   **Processing:** Renders the `StructuredCV` into an HTML string using a Jinja2 template, then uses `WeasyPrint` to generate a PDF.
    -   **Output:** `{ "final_cv_output_path": str }`. The path to the generated PDF.

#### **10.2. `StateManager` and `st.session_state` Interaction**

**Objective:** To provide a clear code pattern for managing the application's state within Streamlit's re-run-on-interaction model.

**File:** `src/core/main.py`

```python
import streamlit as st
from src.core.state_manager import StateManager
from src.models.data_models import StructuredCV
from uuid import UUID

def get_state_manager() -> StateManager:
    """
    Retrieves the StateManager instance from the Streamlit session state,
    initializing it if it doesn't exist.
    """
    if 'state_manager' not in st.session_state:
        # Initialize the state manager for the new session
        st.session_state.state_manager = StateManager(session_id=str(uuid.uuid4()))
        st.session_state.state_manager.create_new_cv()
        st.write("DEBUG: New StateManager created.") # For debugging
    return st.session_state.state_manager

def main():
    st.title("AI CV Generator")

    # Get the state manager for the current session
    state_manager = get_state_manager()
    structured_cv = state_manager.get_structured_cv()

    # --- UI Components ---
    # Example: A text area to edit the executive summary
    summary_section = structured_cv.get_section_by_name("Executive Summary")
    if summary_section and summary_section.items:
        summary_item = summary_section.items[0]

        new_summary_content = st.text_area(
            "Executive Summary",
            value=summary_item.content,
            key=f"summary_edit_{summary_item.id}"
        )

        if new_summary_content != summary_item.content:
            # When the user types, the script re-runs. We update the state directly.
            state_manager.update_item_content(summary_item.id, new_summary_content)
            state_manager.update_item_status(summary_item.id, "user_modified")

            # Persist the change immediately
            state_manager.save_state()
            st.success("Summary updated and saved automatically!")

    # --- Example of a button triggering a backend process ---
    if st.button("Generate All Key Qualifications"):
        # This would trigger the orchestrator/LangGraph
        with st.spinner("Generating..."):
            # In a real app, this would call the orchestrator, which returns the updated CV
            # For this example, we simulate the update.
            quals_section = structured_cv.get_section_by_name("Key Qualifications")
            if quals_section:
                for item in quals_section.items:
                    item.content = f"Generated Skill: {item.id}"
                    item.status = "generated"
                state_manager.save_state()
        st.rerun() # Rerun the script to display the new content

# ... rest of the app logic ...
if __name__ == "__main__":
    main()
```

---

### **19. Appendix F: Component-Specific Implementation Guides**

#### **19.1. Vector Store (`ChromaDB`) Data Structure**

**Objective:** Define the structure for documents to be stored in ChromaDB to support the `ResearchAgent`.

-   **Logic:** When the `ResearchAgent` processes the user's CV (`StructuredCV`), it should iterate through every meaningful `Item` (e.g., bullet points in "Experience" and "Projects"). Each item will become a separate "document" in ChromaDB.
-   **Implementation in `ResearchAgent`:**

    ```python
    # In ResearchAgent, after receiving the parsed StructuredCV
    from chromadb.types import Collection

    def populate_vector_store(self, structured_cv: StructuredCV, db_collection: Collection):
        documents_to_add = []
        metadatas_to_add = []
        ids_to_add = []

        for section in structured_cv.sections:
            if section.name in ["Professional Experience", "Project Experience"]:
                for subsection in section.subsections:
                    for item in subsection.items:
                        # The content to be embedded
                        documents_to_add.append(item.content)

                        # The unique ID for the document
                        ids_to_add.append(str(item.id))

                        # The metadata for filtering and context retrieval
                        metadatas_to_add.append({
                            "section_name": section.name,
                            "subsection_name": subsection.name,
                            "item_type": item.item_type.value,
                            "original_cv_id": str(structured_cv.id)
                        })

        if documents_to_add:
            db_collection.add(
                documents=documents_to_add,
                metadatas=metadatas_to_add,
                ids=ids_to_add
            )
            self.logger.info(f"Added {len(documents_to_add)} CV items to ChromaDB.")
    ```

#### **19.2. PDF Generation with `FormatterAgent` and `WeasyPrint`**

**Objective:** Detail the process for converting the final `StructuredCV` into a styled PDF document.

-   **Logic:** The `FormatterAgent` will use the `Jinja2` templating engine to render the `StructuredCV` data into an HTML document. This HTML, combined with a separate CSS file for styling, will then be converted to PDF by `WeasyPrint`.
-   **Steps:**
    1.  The `FormatterAgent` receives the final, accepted `StructuredCV` object.
    2.  It loads a Jinja2 HTML template (e.g., `data/templates/cv_template.html`).
    3.  It loads a CSS stylesheet (e.g., `data/templates/cv_style.css`).
    4.  It passes the `StructuredCV` object as context to the Jinja2 `render` method.
    5.  The rendered HTML string is passed to `WeasyPrint`.
    6.  `WeasyPrint` generates the final PDF file and saves it to the `output/` directory.

-   **Simplified Jinja2 Template (`data/templates/cv_template.html`):**

    ```html
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>{{ cv.metadata.get('name', 'CV') }}</title>
        <style>
            {{ css_content }}
        </style>
    </head>
    <body>
        <header>
            <h1>{{ cv.metadata.get('name') }}</h1>
            <p>{{ cv.metadata.get('email') }} | {{ cv.metadata.get('phone') }}</p>
        </header>

        {% for section in cv.sections %}
        <section class="cv-section">
            <h2>{{ section.name }}</h2>

            {# Render items directly under the section #}
            {% if section.items %}
            <ul>
                {% for item in section.items %}
                <li>{{ item.content }}</li>
                {% endfor %}
            </ul>
            {% endif %}

            {# Render subsections #}
            {% for subsection in section.subsections %}
            <div class="subsection">
                <h3>{{ subsection.name }}</h3>
                {% if subsection.metadata.get('duration') %}
                <p class="duration"><em>{{ subsection.metadata.get('duration') }}</em></p>
                {% endif %}
                <ul>
                    {% for item in subsection.items %}
                    <li>{{ item.content }}</li>
                    {% endfor %}
                </ul>
            </div>
            {% endfor %}
        </section>
        {% endfor %}
    </body>
    </html>
    ```
-   **`FormatterAgent` Implementation Snippet:**

    ```python
    # In FormatterAgent class
    from jinja2 import Environment, FileSystemLoader
    from weasyprint import HTML, CSS

    class FormatterAgent:
        def __init__(self, template_dir="data/templates"):
            self.jinja_env = Environment(loader=FileSystemLoader(template_dir))

        def render_cv_to_pdf(self, structured_cv: StructuredCV, output_path: str):
            try:
                # Load HTML template and CSS
                template = self.jinja_env.get_template("cv_template.html")
                with open(Path(self.template_dir) / "cv_style.css", "r") as f:
                    css_content = f.read()

                # Render HTML
                rendered_html = template.render(cv=structured_cv, css_content=css_content)

                # Generate PDF
                html = HTML(string=rendered_html)
                html.write_pdf(output_path)

                self.logger.info(f"Successfully generated PDF at {output_path}")
                return True
            except Exception as e:
                self.logger.error(f"Failed to generate PDF: {e}")
                return False
    ```

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

---

### **16. Appendix F: Centralized Configuration Management**

**Objective:** To establish a single, typed, and validated source of truth for all application settings, making the system more maintainable and less prone to configuration errors. This replaces scattered configuration logic with a robust, Pydantic-based approach.

-   **Task/Feature Addressed:** Addresses general maintainability and resolves potential issues from hardcoded or scattered configuration values.
-   **Affected Component(s):** `src/config/settings.py`, `src/config/environment.py`, and all modules that require configuration values.
-   **Pydantic Model Changes:** New Pydantic models will be defined in `src/config/settings.py` to represent the application's configuration structure.

    **File:** `src/config/settings.py`
    ```python
    from pydantic_settings import BaseSettings, SettingsConfigDict
    from typing import List, Optional

    class LLMConfig(BaseSettings):
        gemini_api_key: str
        generation_model: str = "gemini-1.5-flash-latest"
        # Max retries for transient API errors
        max_retries: int = 3
        # Request timeout in seconds
        request_timeout: int = 120

    class DatabaseConfig(BaseSettings):
        # Path for ChromaDB vector store persistence
        vector_db_path: str = "data/chroma_db"
        # Path for file-based session state
        session_storage_path: str = "data/sessions"

    class AppConfig(BaseSettings):
        """Main application settings, loaded from .env file."""
        # Pydantic will automatically read environment variables
        # e.g., 'LLM_GEMINI_API_KEY' will map to llm.gemini_api_key
        model_config = SettingsConfigDict(env_file='.env', env_nested_delimiter='__')

        llm: LLMConfig = LLMConfig()
        database: DatabaseConfig = DatabaseConfig()
        log_level: str = "INFO"

    # Singleton instance of the configuration
    settings = AppConfig()
    ```

-   **Implementation Steps:**
    1.  **Create `.env` file:** Ensure a `.env` file exists in the project root with the following structure:
        ```env
        # .env file
        LLM__GEMINI_API_KEY="AIzaSy...your...key"
        LOG_LEVEL="INFO"
        ```
    2.  **Implement `settings.py`:** Create the `src/config/settings.py` file with the Pydantic models shown above. The `pydantic-settings` library will automatically load values from the `.env` file.
    3.  **Refactor Application:** Replace all direct calls to `os.getenv()` or hardcoded configuration values throughout the application with imports from the settings module.
        ```python
        # Example in src/services/llm.py
        from src.config.settings import settings

        class LLMService:
            def __init__(self):
                # Access config values in a typed, validated way
                self.api_key = settings.llm.gemini_api_key
                self.model_name = settings.llm.generation_model
                genai.configure(api_key=self.api_key)
                # ...
        ```
-   **Testing Considerations:**
    -   Unit test that the `AppConfig` model correctly loads variables from a test `.env` file.
    -   Verify that missing a required variable (like `LLM__GEMINI_API_KEY`) in the `.env` file raises a `ValidationError` on startup, preventing the application from running with an invalid configuration.

### **17. Appendix G: Standardized Error Handling Framework**

**Objective:** To implement a consistent, structured way to handle and report errors throughout the application, improving debuggability and providing a better user experience.

-   **Task/Feature Addressed:** Addresses flawed error propagation (CI-005) and supports reliability requirements (REQ-NONFUNC-RELIABILITY-1).
-   **Affected Component(s):** A new utility module `src/utils/error_handling.py` will be created. All agent `try...except` blocks will be updated to use this framework.
-   **Implementation Details (`src/utils/error_handling.py`):**
    -   Define custom, specific exception classes to represent different failure modes.

        ```python
        # In src/utils/error_handling.py
        class AgentError(Exception):
            """Base exception for all agent-related errors."""
            def __init__(self, message, component, error_code=None):
                self.message = message
                self.component = component
                self.error_code = error_code
                super().__init__(f"[{component}] {message}")

        class LLMError(AgentError):
            """Exception for LLM API or content generation failures."""
            pass

        class ParsingError(AgentError):
            """Exception for data parsing failures."""
            pass

        class OrchestrationError(AgentError):
            """Exception for workflow orchestration failures."""
            pass
        ```
    -   **Refactoring Agents to Use Custom Exceptions:** Agents should catch broad exceptions from their internal logic or external calls but raise more specific, custom exceptions. This provides clearer error context to the calling orchestrator.

        **Example in `ParserAgent`:**
        ```python
        # In ParserAgent._parse_experience_section_with_llm
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to decode JSON from LLM: {e}")
            raise ParsingError(
                message="LLM returned malformed JSON for experience section.",
                component="ParserAgent"
            ) from e
        except Exception as e:
            self.logger.error(f"An unexpected error occurred during LLM experience parsing: {e}")
            raise LLMError(
                message="An unexpected LLM error occurred during experience parsing.",
                component="ParserAgent"
            ) from e
        ```
    -   **Orchestrator Error Handling:** The orchestrator (or LangGraph nodes) will catch these specific exceptions and update the state accordingly.

        **Example in a LangGraph node:**
        ```python
        # In a LangGraph node function
        def parser_node(state: AgentState) -> dict:
            try:
                # ... call parser agent ...
                return {"structured_cv": ..., "job_description": ...}
            except ParsingError as e:
                return {"error_messages": state['error_messages'] + [str(e)]}
            except LLMError as e:
                return {"error_messages": state['error_messages'] + [str(e)]}
        ```

-   **Testing Considerations:**
    -   Write unit tests for each agent that mock underlying dependencies to raise exceptions. Assert that the agent correctly catches the generic exception and raises the appropriate custom `AgentError`.
    -   Write integration tests for the LangGraph workflow where a node is forced to fail. Assert that the error is correctly added to the `AgentState` and that the graph can route to an error-handling state if one is defined.

---

### **Critical Gaps & Questions**

1.  **Orchestration Strategy Ambiguity:** The codebase contains `src/core/orchestrator.py` and `src/core/enhanced_orchestrator.py`, and the refactoring plan mentions a future migration to LangGraph.
    *   **Question:** What is the intended role of each orchestrator file during this MVP refactoring? Is `EnhancedOrchestrator` the target for implementing the new granular workflow, with LangGraph being a post-MVP goal?
    *   **Recommendation:** This plan assumes `EnhancedOrchestrator` is the primary target for the MVP refactoring. The logic should be built here, but with agent interfaces that are LangGraph-compatible (`run(state: dict) -> dict`) to simplify future migration. The older `orchestrator.py` should be marked for deprecation.

2.  **ParserAgent Experience Segmentation Logic:** The root cause of many data flow issues is the `ParserAgent`'s failure to segment experience roles. The analysis documents mention this, but the exact current logic is not fully detailed.
    *   **Question:** What is the current approach of `ParserAgent`'s `_parse_experience_section_with_llm`? Does it attempt segmentation, or does it treat the whole section as a block?
    *   **Recommendation:** This plan mandates that the `ParserAgent` *must* be refactored to output structured, individual roles. The engineer should prioritize using Gemini with a specific prompt to parse the experience section into a JSON array of roles. This is more reliable than complex regex and is a critical fix.

3.  **Database and VectorDB Usage:** The requirements mention `ChromaDB` and `SQLAlchemy`, but their concrete use in the MVP workflow (especially for `ResearchAgent` and `VectorStoreAgent`) is not fully defined in the SRS.
    *   **Question:** What is the precise role of `ResearchAgent` and `ChromaDB` in the MVP? Is it used for finding similar job descriptions, or for finding relevant parts of the user's *own* CV to highlight?
    *   **Recommendation:** For the MVP, the use of `ResearchAgent` should be simplified. It can be used to generate embeddings for the user's CV sections and the target job description. The `ContentWriterAgent` can then be provided with the top 3 most similar user CV bullet points as additional context when tailoring a new bullet point, improving relevance. This is a practical and high-value use case for the MVP.

4.  **UI/API Duality:** The project contains both Streamlit (`run_app.py`, `app.py`, `src/core/main.py`) and FastAPI (`src/api/main.py`) components.
    *   **Question:** Is the immediate MVP target a pure Streamlit application, with the FastAPI backend intended for a future production version?
    *   **Recommendation:** This plan focuses exclusively on the Streamlit application as the user-facing MVP. All workflow and state management logic should be implemented with the Streamlit front-end in mind. The FastAPI endpoints in `src/api/` should be considered out of scope for this immediate refactoring effort unless specified otherwise.

---

The technical blueprint is now complete. All phases, tasks, and appendices necessary for the MVP refactoring and implementation have been detailed. The plan provides a clear, actionable roadmap for the Senior Python/AI Engineer, addressing all requirements and critical issues identified in the provided context. No further sections are required.