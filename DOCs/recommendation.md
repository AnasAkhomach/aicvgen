



I'll search for relevant documentation using Context7 to help with the project.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 resolve-library-id [object Object]
 {"content":[{"type":"text","text":"Available Libraries (top matches):\n\nEach result includes:\n- Library ID: Context7-compatible identifier (format: /org/project)\n- Name: Library or package name\n- Description: Short summary\n- Code Snippets: Number of available code examples\n- Trust Score: Authority indicator\n- Versions: List of versions if available. Use one of those versions if and only if the user explicitly provides a version in their query.\n\nFor best results, select libraries based on name match, trust score, snippet coverage, and relevance to your use case.\n\n----------\n\n- Title: Pytest\n- Context7-compatible library ID: /pytest-dev/pytest\n- Description: The pytest framework makes it easy to write small tests, yet scales to support complex functional testing\n- Code Snippets: 975\n- Trust Score: 9.5\n----------\n- Title: pytest-grpc\n- Context7-compatible library ID: /kataev/pytest-grpc\n- Description: Allow test gRPC with pytest\n- Code Snippets: 8\n- Trust Score: 7.7\n----------\n- Title: pytest-qt\n- Context7-compatible library ID: /pytest-dev/pytest-qt\n- Description: pytest plugin for Qt (PyQt5/PyQt6 and PySide2/PySide6) application testing\n- Code Snippets: 66\n- Trust Score: 9.5\n----------\n- Title: Pytest Check\n- Context7-compatible library ID: /okken/pytest-check\n- Description: A pytest plugin that allows multiple failures per test.\n- Code Snippets: 11\n- Trust Score: 9.4\n----------\n- Title: Pytest DSL\n- Context7-compatible library ID: /felix-1991/pytest-dsl\n- Description: 基于pytest的测试框架，它使用自定义的领域特定语言(DSL)来编写测试用例，使测试更加直观、易读和易维护。\n- Code Snippets: 90\n- Trust Score: 2.2\n----------\n- Title: Pytest BDD\n- Context7-compatible library ID: /pytest-dev/pytest-bdd\n- Description: BDD library for the pytest runner\n- Code Snippets: 45\n- Trust Score: 9.5\n----------\n- Title: Pytest-Cov\n- Context7-compatible library ID: /pytest-dev/pytest-cov\n- Description: Coverage plugin for pytest.\n- Code Snippets: 47\n- Trust Score: 9.5\n----------\n- Title: Pytest Docker\n- Context7-compatible library ID: /avast/pytest-docker\n- Description: Docker-based integration tests\n- Code Snippets: 9\n- Trust Score: 8.2\n----------\n- Title: Pytest Celery\n- Context7-compatible library ID: /celery/pytest-celery\n- Description: Official pytest plugin for Celery.\n- Code Snippets: 191\n- Trust Score: 9.1\n----------\n- Title: Pytest HTML\n- Context7-compatible library ID: /pytest-dev/pytest-html\n- Description: Plugin for generating HTML reports for pytest results\n- Code Snippets: 38\n- Trust Score: 9.5\n----------\n- Title: Pytest Mock\n- Context7-compatible library ID: /pytest-dev/pytest-mock\n- Description: Thin-wrapper around the mock package for easier use with pytest\n- Code Snippets: 18\n- Trust Score: 9.5\n----------\n- Title: Pytest Loguru\n- Context7-compatible library ID: /mcarans/pytest-loguru\n- Description: \n- Code Snippets: 12\n- Trust Score: 7.4\n----------\n- Title: Pytest Xdist\n- Context7-compatible library ID: /pytest-dev/pytest-xdist\n- Description: pytest plugin for distributed testing and loop-on-failures testing modes. \n- Code Snippets: 32\n- Trust Score: 9.5\n----------\n- Title: Pytest DSL UI\n- Context7-compatible library ID: /felix-1991/pytest-dsl-ui\n- Description: 基于Playwright的UI自动化测试框架** - 为pytest-dsl提供强大的Web UI测试能力\n- Code Snippets: 21\n- Trust Score: 2.2\n----------\n- Title: Pytest-Django\n- Context7-compatible library ID: /pytest-dev/pytest-django\n- Description: A Django plugin for pytest.\n- Code Snippets: 79\n- Trust Score: 9.5\n----------\n- Title: Pytest Asyncio\n- Context7-compatible library ID: /pytest-dev/pytest-asyncio\n- Description: Asyncio support for pytest\n- Code Snippets: 25\n- Trust Score: 9.5\n----------\n- Title: Pytest Timeout\n- Context7-compatible library ID: /pytest-dev/pytest-timeout\n- Description: \n- Code Snippets: 9\n- Trust Score: 9.5\n----------\n- Title: Pytest-Recording\n- Context7-compatible library ID: /kiwicom/pytest-recording\n- Description: A pytest plugin powered by VCR.py to record and replay HTTP traffic.\n- Code Snippets: 21\n- Trust Score: 7.8\n----------\n- Title: Pytest Metadata\n- Context7-compatible library ID: /pytest-dev/pytest-metadata\n- Description: Plugin for accessing test session metadata\n- Code Snippets: 16\n- Trust Score: 9.5\n----------\n- Title: Pytest Unordered\n- Context7-compatible library ID: /utapyngo/pytest-unordered\n- Description: Test equality of unordered sequences\n- Code Snippets: 12\n- Trust Score: 7.3\n----------\n- Title: Pytest PostgreSQL\n- Context7-compatible library ID: /dbfixtures/pytest-postgresql\n- Description: This is a pytest plugin, that enables you to test your code that relies on a running PostgreSQL Database. It allows you to specify fixtures for PostgreSQL process and client.\n- Code Snippets: 21\n- Trust Score: 5.3\n----------\n- Title: Pytest Databases\n- Context7-compatible library ID: /litestar-org/pytest-databases\n- Description: Reusable database fixtures for any and all databases.\n- Code Snippets: 53\n- Trust Score: 8.9\n----------\n- Title: Pytest Rerunfailures\n- Context7-compatible library ID: /pytest-dev/pytest-rerunfailures\n- Description: a pytest plugin that re-runs failed tests up to -n times to eliminate flakey failures\n- Code Snippets: 24\n- Trust Score: 9.5\n----------\n- Title: Pytest OpenTelemetry\n- Context7-compatible library ID: /chrisguidry/pytest-opentelemetry\n- Description: A pytest plugin for instrumenting test runs via OpenTelemetry\n- Code Snippets: 8\n- Trust Score: 8.3\n----------\n- Title: pytest-benchmark\n- Context7-compatible library ID: /context7/pytest-benchmark_readthedocs_io-en-stable\n- Description: A pytest plugin that provides a benchmark fixture to measure the performance of functions. It offers features like automatic calibration, integration with pytest, comparison tracking, and detailed statistics.\n- Code Snippets: 74\n- Trust Score: 9"}],"isError":null}

I'll get pytest documentation to help with testing best practices for the project.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 get-library-docs [object Object]
 {"content":[{"type":"text","text":"TITLE: Sharing Mocked API Responses with Pytest Fixtures (Python)\nDESCRIPTION: This example refactors the API response mocking into a pytest fixture named `mock_response`. This allows the mock to be easily shared and reused across multiple tests, promoting cleaner, more organized, and maintainable test code by centralizing the mocking logic.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nimport requests\nimport app\n\n\nclass MockResponse:\n    @staticmethod\n    def json():\n        return {\"mock_key\": \"mock_response\"}\n\n\n@pytest.fixture\ndef mock_response(monkeypatch):\n    \"\"\"Requests.get() mocked to return {'mock_key':'mock_response'}.\"\"\"\n\n    def mock_get(*args, **kwargs):\n        return MockResponse()\n\n    monkeypatch.setattr(requests, \"get\", mock_get)\n\n\ndef test_get_json(mock_response):\n    result = app.get_json(\"https://fakeurl\")\n    assert result[\"mock_key\"] == \"mock_response\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Sync Test with Unawaited Async Fixture in Pytest\nDESCRIPTION: This snippet illustrates a deprecated pattern where a synchronous test function attempts to use an asynchronous fixture directly without awaiting it. This can lead to \"unawaited coroutine\" warnings or unpredictable behavior, especially with fixture caching. The `asyncio.run` call attempts to execute the coroutine returned by the fixture.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/deprecations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport pytest\n\n\n@pytest.fixture\nasync def unawaited_fixture():\n    return 1\n\n\ndef test_foo(unawaited_fixture):\n    assert 1 == asyncio.run(unawaited_fixture)\n```\n\n----------------------------------------\n\nTITLE: Testing Environment Variables with pytest-monkeypatch (Fixtures)\nDESCRIPTION: This example refactors environment variable monkeypatching into pytest fixtures (`mock_env_user`, `mock_env_missing`). This approach promotes reusability and modularity, allowing tests to simply reference the fixtures to set up specific environment states for their execution.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# contents of our test file e.g. test_code.py\nimport pytest\n\n\n@pytest.fixture\ndef mock_env_user(monkeypatch):\n    monkeypatch.setenv(\"USER\", \"TestingUser\")\n\n\n@pytest.fixture\ndef mock_env_missing(monkeypatch):\n    monkeypatch.delenv(\"USER\", raising=False)\n\n\n# notice the tests reference the fixtures for mocks\ndef test_upper_to_lower(mock_env_user):\n    assert get_os_user_lower() == \"testinguser\"\n\n\ndef test_raise_exception(mock_env_missing):\n    with pytest.raises(OSError):\n        _ = get_os_user_lower()\n```\n\n----------------------------------------\n\nTITLE: Correcting Sync Test Usage of Async Fixture in Pytest\nDESCRIPTION: This snippet demonstrates the recommended approach to allow a synchronous test to depend on an asynchronous fixture. The asynchronous logic is encapsulated within an `inner_fixture` function, which is then called and returned by a synchronous `unawaited_fixture`. This ensures the coroutine is properly handled before being passed to the synchronous test, preventing warnings and unpredictable behavior.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/deprecations.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport pytest\n\n\n@pytest.fixture\ndef unawaited_fixture():\n    async def inner_fixture():\n        return 1\n\n    return inner_fixture()\n\n\ndef test_foo(unawaited_fixture):\n    assert 1 == asyncio.run(unawaited_fixture)\n```\n\n----------------------------------------\n\nTITLE: Monkeypatching Dictionary Items with pytest-monkeypatch (Fixtures)\nDESCRIPTION: This snippet demonstrates organizing dictionary monkeypatching into pytest fixtures for better modularity and reusability. The `mock_test_user` fixture sets a specific 'user' value in `app.DEFAULT_CONFIG`, allowing tests to easily apply this mock by simply requesting the fixture.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# contents of test_app.py\nimport pytest\n\n# app.py with the connection string function\nimport app\n\n\n# all of the mocks are moved into separated fixtures\n@pytest.fixture\ndef mock_test_user(monkeypatch):\n    \"\"\"Set the DEFAULT_CONFIG user to test_user.\"\"\"\n    monkeypatch.setitem(app.DEFAULT_CONFIG, \"user\", \"test_user\")\n```\n\n----------------------------------------\n\nTITLE: Globally Preventing HTTP Requests with Autouse Fixture (Python)\nDESCRIPTION: This `autouse` pytest fixture, defined in `conftest.py`, uses `monkeypatch.delattr` to remove the `requests.sessions.Session.request` method. This effectively prevents any HTTP requests from being made by the `requests` library across all tests, ensuring test isolation and preventing unintended external network calls.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n\n@pytest.fixture(autouse=True)\ndef no_requests(monkeypatch):\n    \"\"\"Remove requests.sessions.Session.request for all tests.\"\"\"\n    monkeypatch.delattr(\"requests.sessions.Session.request\")\n```\n\n----------------------------------------\n\nTITLE: Defining Pytest Fixtures and Tests (Initial)\nDESCRIPTION: This snippet defines a parametrized `pytest` fixture named `non_parametrized_username` and two test functions. The fixture returns a parameter from its `request` object. The tests demonstrate how fixtures are used, with one test expecting an overridden fixture value and another testing the parametrized fixture's values.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\n@pytest.fixture(params=['one', 'two', 'three'])\ndef non_parametrized_username(request):\n    return request.param\n\ndef test_username(parametrized_username):\n    assert parametrized_username == 'overridden-username'\n\ndef test_parametrized_username(non_parametrized_username):\n    assert non_parametrized_username in ['one', 'two', 'three']\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Fixtures in Pytest INI\nDESCRIPTION: This INI configuration snippet for `pytest.ini` demonstrates how to globally apply a fixture named `cleandir` to all tests by default using the `usefixtures` option. This ensures the specified fixture runs before every test without explicit declaration in test functions.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_58\n\nLANGUAGE: INI\nCODE:\n```\n# content of pytest.ini\n[pytest]\nusefixtures = cleandir\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Pytest Fixture Reusability and Isolation in Python\nDESCRIPTION: This snippet highlights the reusability of pytest fixtures and their ability to provide isolated environments for each test. It shows two different tests (`test_string`, `test_int`) requesting the same `order` fixture, demonstrating that each test receives its own fresh instance of the fixture's result, preventing test interference and ensuring consistent results.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# contents of test_append.py\nimport pytest\n\n\n# Arrange\n@pytest.fixture\ndef first_entry():\n    return \"a\"\n\n\n# Arrange\n@pytest.fixture\ndef order(first_entry):\n    return [first_entry]\n\n\ndef test_string(order):\n    # Act\n    order.append(\"b\")\n\n    # Assert\n    assert order == [\"a\", \"b\"]\n\n\ndef test_int(order):\n    # Act\n    order.append(2)\n\n    # Assert\n    assert order == [\"a\", 2]\n```\n\n----------------------------------------\n\nTITLE: Applying `usefixtures` Marker to a Test Class (Python)\nDESCRIPTION: This Python snippet demonstrates applying the `@pytest.mark.usefixtures(\"cleandir\")` marker to a test class `TestDirectoryInit`. This automatically applies the `cleandir` fixture to every test method within the class, eliminating the need to explicitly pass `cleandir` as an argument to each test function. This is useful when tests don't need direct access to the fixture object but require its setup/teardown effects.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\n# content of test_setenv.py\nimport os\n\nimport pytest\n\n\n@pytest.mark.usefixtures(\"cleandir\")\nclass TestDirectoryInit:\n    def test_cwd_starts_empty(self):\n        assert os.listdir(os.getcwd()) == []\n        with open(\"myfile\", \"w\", encoding=\"utf-8\") as f:\n            f.write(\"hello\")\n\n    def test_cwd_again_starts_empty(self):\n        assert os.listdir(os.getcwd()) == []\n```\n\n----------------------------------------\n\nTITLE: Pytest Output: Running Interdependent Fixture Tests\nDESCRIPTION: This output shows the execution of `test_appsetup.py`, demonstrating that the test `test_smtp_connection_exists` runs twice. This is due to the `smtp_connection` fixture being parametrized, and pytest automatically running the dependent `app` fixture and its tests for each parameter, confirming proper dependency resolution.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_45\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest -v test_appsetup.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-8.x.y, pluggy-1.x.y -- $PYTHON_PREFIX/bin/python\ncachedir: .pytest_cache\nrootdir: /home/sweet/project\ncollecting ... collected 2 items\n\ntest_appsetup.py::test_smtp_connection_exists[smtp.gmail.com] PASSED [ 50%]\ntest_appsetup.py::test_smtp_connection_exists[mail.python.org] PASSED [100%]\n\n============================ 2 passed in 0.12s =============================\n```\n\n----------------------------------------\n\nTITLE: Using a Module-Scoped Pytest Fixture in Python Tests\nDESCRIPTION: This snippet demonstrates how test functions (`test_ehlo`, `test_noop`) consume the `smtp_connection` fixture. Pytest automatically injects the fixture instance as an argument to the test functions. Both tests use the same `smtp_connection` object, as indicated by the module scope, and include `assert 0` for demonstration purposes to show test failures.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# content of test_module.py\n\n\ndef test_ehlo(smtp_connection):\n    response, msg = smtp_connection.ehlo()\n    assert response == 250\n    assert b\"smtp.gmail.com\" in msg\n    assert 0  # for demo purposes\n\n\ndef test_noop(smtp_connection):\n    response, msg = smtp_connection.noop()\n    assert response == 250\n    assert 0  # for demo purposes\n```\n\n----------------------------------------\n\nTITLE: Executing Pytest with Fixture Parametrization (Pytest CLI)\nDESCRIPTION: This Pytest CLI command runs tests in verbose mode (`-v`) and displays print statements (`-s`) from `test_module.py`. The output clearly illustrates the setup and teardown order of parametrized fixtures with different scopes, showing how module-scoped fixtures are managed across multiple tests and how function-scoped fixtures are re-initialized for each test.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_52\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest -v -s test_module.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-8.x.y, pluggy-1.x.y -- $PYTHON_PREFIX/bin/python\ncachedir: .pytest_cache\nrootdir: /home/sweet/project\ncollecting ... collected 8 items\n\ntest_module.py::test_0[1]   SETUP otherarg 1\n  RUN test0 with otherarg 1\nPASSED  TEARDOWN otherarg 1\n\ntest_module.py::test_0[2]   SETUP otherarg 2\n  RUN test0 with otherarg 2\nPASSED  TEARDOWN otherarg 2\n\ntest_module.py::test_1[mod1]   SETUP modarg mod1\n  RUN test1 with modarg mod1\nPASSED\ntest_module.py::test_2[mod1-1]   SETUP otherarg 1\n  RUN test2 with otherarg 1 and modarg mod1\nPASSED  TEARDOWN otherarg 1\n\ntest_module.py::test_2[mod1-2]   SETUP otherarg 2\n  RUN test2 with otherarg 2 and modarg mod1\nPASSED  TEARDOWN otherarg 2\n\ntest_module.py::test_1[mod2]   TEARDOWN modarg mod1\n  SETUP modarg mod2\n  RUN test1 with modarg mod2\nPASSED\ntest_module.py::test_2[mod2-1]   SETUP otherarg 1\n  RUN test2 with otherarg 1 and modarg mod2\nPASSED  TEARDOWN otherarg 1\n\ntest_module.py::test_2[mod2-2]   SETUP otherarg 2\n  RUN test2 with otherarg 2 and modarg mod2\nPASSED  TEARDOWN otherarg 2\n  TEARDOWN modarg mod2\n\n\n============================ 8 passed in 0.12s =============================\n```\n\n----------------------------------------\n\nTITLE: Applying Global Fixtures (INI)\nDESCRIPTION: Specifies a list of fixtures that will be applied to all test functions in pytest. This configuration is semantically equivalent to applying the `@pytest.mark.usefixtures` marker to every test function.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/reference/reference.rst#_snippet_58\n\nLANGUAGE: ini\nCODE:\n```\n[pytest]\nusefixtures =\n    clean_db\n```\n\n----------------------------------------\n\nTITLE: Applying Marks to Parametrized Fixtures in Pytest\nDESCRIPTION: This snippet demonstrates how to use `pytest.param` to apply marks, specifically `pytest.mark.skip`, to individual values within a parametrized fixture. This allows selective skipping of test invocations based on specific fixture parameters, enhancing test control and flexibility.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# content of test_fixture_marks.py\nimport pytest\n\n\n@pytest.fixture(params=[0, 1, pytest.param(2, marks=pytest.mark.skip)])\ndef data_set(request):\n    return request.param\n\n\ndef test_data(data_set):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Basic Pytest Fixture in Python\nDESCRIPTION: This snippet demonstrates how to define a basic pytest fixture (`fruit_bowl`) and how a test function (`test_fruit_salad`) requests and uses it. It includes custom classes (`Fruit`, `FruitSalad`) to illustrate a practical scenario where the fixture provides initial data (a list of `Fruit` objects) for the test to operate on, ensuring a consistent setup.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n\nclass Fruit:\n    def __init__(self, name):\n        self.name = name\n        self.cubed = False\n\n    def cube(self):\n        self.cubed = True\n\n\nclass FruitSalad:\n    def __init__(self, *fruit_bowl):\n        self.fruit = fruit_bowl\n        self._cube_fruit()\n\n    def _cube_fruit(self):\n        for fruit in self.fruit:\n            fruit.cube()\n\n\n# Arrange\n@pytest.fixture\ndef fruit_bowl():\n    return [Fruit(\"apple\"), Fruit(\"banana\")]\n\n\ndef test_fruit_salad(fruit_bowl):\n    # Act\n    fruit_salad = FruitSalad(*fruit_bowl)\n\n    # Assert\n    assert all(fruit.cubed for fruit in fruit_salad.fruit)\n```\n\n----------------------------------------\n\nTITLE: Testing Database Connection String Creation (Python)\nDESCRIPTION: This test function verifies that `app.create_connection_string()` produces the expected connection string when `mock_test_user` and `mock_test_database` fixtures are applied. It asserts the generated string against a predefined `expected` value.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndef test_connection(mock_test_user, mock_test_database):\n    expected = \"User Id=test_user; Location=test_db;\"\n\n    result = app.create_connection_string()\n    assert result == expected\n```\n\n----------------------------------------\n\nTITLE: Pytest Fixtures for Optional Module Imports (conftest.py)\nDESCRIPTION: This `conftest.py` snippet defines session-scoped pytest fixtures (`basemod` and `optmod`) that use `pytest.importorskip` to conditionally import modules. This allows tests to compare different implementations of an API, skipping tests if an implementation is not available.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/parametrize.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# content of conftest.py\n\nimport pytest\n\n\n@pytest.fixture(scope=\"session\")\ndef basemod(request):\n    return pytest.importorskip(\"base\")\n\n\n@pytest.fixture(scope=\"session\", params=[\"opt1\", \"opt2\"])\ndef optmod(request):\n    return pytest.importorskip(request.param)\n```\n\n----------------------------------------\n\nTITLE: Parametrizing Test with `pytest.fixture_request` and `pytest.mark.parametrize`\nDESCRIPTION: This code demonstrates using the proposed `pytest.fixture_request` helper directly within `pytest.mark.parametrize`. It allows a test function to be parametrized with values yielded by existing fixtures, such as `default_context` and `extra_context`. This provides a flexible way to run a single test against various fixture-driven scenarios, ensuring proper handling of exit codes for each context.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/proposals/parametrize_with_fixtures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\n    \"context, expected_response_code\",\n    [\n        (pytest.fixture_request(\"default_context\"), 0),\n        (pytest.fixture_request(\"extra_context\"), 0),\n    ],\n)\ndef test_generate_project(cookies, context, exit_code):\n    \"\"\"Call the cookiecutter API to generate a new project from a\n    template.\n    \"\"\"\n    result = cookies.bake(extra_context=context)\n\n    assert result.exit_code == exit_code\n```\n\n----------------------------------------\n\nTITLE: Testing Global `username` Fixture Usage\nDESCRIPTION: This test function `test_username` demonstrates the consumption of the globally defined `username` fixture. It asserts that the value provided by the fixture is 'username', confirming the base fixture's functionality.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_61\n\nLANGUAGE: undefined\nCODE:\n```\nundefined\n```\n\n----------------------------------------\n\nTITLE: Chaining Pytest Fixtures (Fixture Dependencies) in Python\nDESCRIPTION: This snippet illustrates how one pytest fixture (`order`) can request and utilize another fixture (`first_entry`) as a dependency. It shows how `pytest` resolves these dependencies recursively, providing a structured way to build complex test setups from smaller, reusable components, and then how a test function consumes the final fixture.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# contents of test_append.py\nimport pytest\n\n\n# Arrange\n@pytest.fixture\ndef first_entry():\n    return \"a\"\n\n\n# Arrange\n@pytest.fixture\ndef order(first_entry):\n    return [first_entry]\n\n\ndef test_string(order):\n    # Act\n    order.append(\"b\")\n\n    # Assert\n    assert order == [\"a\", \"b\"]\n```\n\n----------------------------------------\n\nTITLE: Defining a Pytest Session-Scoped Fixture in Python\nDESCRIPTION: This Python snippet defines a `pytest` session-scoped fixture named `callattr_ahead_of_alltests` in `conftest.py`. Configured with `autouse=True`, it automatically runs once per test session before any tests. It iterates through all collected test items, identifies their parent test classes, and if a class defines a `callme` method, it invokes that method. This allows for pre-test setup or inspection across all test classes.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/special.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# content of conftest.py\n\nimport pytest\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef callattr_ahead_of_alltests(request):\n    print(\"callattr_ahead_of_alltests called\")\n    seen = {None}\n    session = request.node\n    for item in session.items:\n        cls = item.getparent(pytest.Class)\n        if cls not in seen:\n            if hasattr(cls.obj, \"callme\"):\n                cls.obj.callme()\n            seen.add(cls)\n```\n\n----------------------------------------\n\nTITLE: Testing pytest Plugins with Pytester Fixture (Python)\nDESCRIPTION: This snippet illustrates how to use the `pytester` fixture to thoroughly test a custom pytest plugin. It demonstrates creating temporary `conftest.py` and test files using `makeconftest` and `makepyfile`, running the tests with `runpytest`, and asserting the test outcomes using `assert_outcomes`. This specific test verifies the functionality of the `hello` fixture.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/writing_plugins.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef test_hello(pytester):\n    \"\"\"Make sure that our plugin works.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n            import pytest\n\n            @pytest.fixture(params=[\n                \"Brianna\",\n                \"Andreas\",\n                \"Floris\",\n            ])\n            def name(request):\n                return request.param\n        \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n            def test_hello_default(hello):\n                assert hello() == \"Hello World!\"\n\n            def test_hello_name(hello, name):\n                assert hello(name) == \"Hello {0}!\".format(name)\n        \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n\n    # check that all 4 tests passed\n    result.assert_outcomes(passed=4)\n```\n\n----------------------------------------\n\nTITLE: Pytest Fixture Introspection with Request Object (Python)\nDESCRIPTION: This snippet illustrates how pytest fixtures can introspect the requesting test context using the `request` object. The `smtp_connection` fixture reads an optional `smtpserver` attribute from the test module, allowing for dynamic configuration of the SMTP server based on the test environment. It also includes a teardown phase to close the connection.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# content of conftest.py\nimport smtplib\n\nimport pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef smtp_connection(request):\n    server = getattr(request.module, \"smtpserver\", \"smtp.gmail.com\")\n    smtp_connection = smtplib.SMTP(server, 587, timeout=5)\n    yield smtp_connection\n    print(f\"finalizing {smtp_connection} ({server})\")\n    smtp_connection.close()\n```\n\n----------------------------------------\n\nTITLE: Applying `usefixtures` at Module Level (Python)\nDESCRIPTION: This Python snippet demonstrates how to apply a fixture to all tests within an entire module by assigning `pytest.mark.usefixtures(\"cleandir\")` to the module-level variable `pytestmark`. This is a convenient way to ensure a specific fixture is active for all tests in a file without decorating each test function or class individually.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\npytestmark = pytest.mark.usefixtures(\"cleandir\")\n```\n\n----------------------------------------\n\nTITLE: Parametrizing Pytest Fixtures for Multiple Test Runs (Python)\nDESCRIPTION: This snippet demonstrates how to parametrize a `pytest` fixture using the `params` argument. The `smtp_connection` fixture will be instantiated twice, once for each parameter (`\"smtp.gmail.com\"` and `\"mail.python.org\"`), causing any dependent tests to run for each parameter value. The `request.param` attribute provides access to the current parameter value.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# content of conftest.py\nimport smtplib\n\nimport pytest\n\n\n@pytest.fixture(scope=\"module\", params=[\"smtp.gmail.com\", \"mail.python.org\"])\ndef smtp_connection(request):\n    smtp_connection = smtplib.SMTP(request.param, 587, timeout=5)\n    yield smtp_connection\n    print(f\"finalizing {smtp_connection}\")\n    smtp_connection.close()\n```\n\n----------------------------------------\n\nTITLE: Partial Manual Simulation of Fixture Reusability in Python\nDESCRIPTION: This snippet begins to illustrate the manual simulation of how pytest handles fixture reusability, where each test requesting a fixture receives its own independent instance. The provided code is incomplete but starts by defining the `first_entry` fixture, which would be part of a larger manual setup for multiple tests.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef first_entry():\n```\n\n----------------------------------------\n\nTITLE: Testing Email Functionality with Pytest Yield Fixtures\nDESCRIPTION: This test file demonstrates how to use pytest `yield` fixtures for setting up and tearing down test resources, specifically for an email sending scenario. It defines fixtures for `mail_admin`, `sending_user`, and `receiving_user`, with cleanup logic placed after the `yield` statement to ensure proper resource release, such as deleting users and clearing mailboxes.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n# content of test_emaillib.py\nfrom emaillib import Email, MailAdminClient\n\nimport pytest\n\n\n@pytest.fixture\ndef mail_admin():\n    return MailAdminClient()\n\n\n@pytest.fixture\ndef sending_user(mail_admin):\n    user = mail_admin.create_user()\n    yield user\n    mail_admin.delete_user(user)\n\n\n@pytest.fixture\ndef receiving_user(mail_admin):\n    user = mail_admin.create_user()\n    yield user\n    user.clear_mailbox()\n    mail_admin.delete_user(user)\n\n\ndef test_email_received(sending_user, receiving_user):\n    email = Email(subject=\"Hey!\", body=\"How's it going?\")\n    sending_user.send_email(email, receiving_user)\n    assert email in receiving_user.inbox\n```\n\n----------------------------------------\n\nTITLE: Use Package-Scoped Fixture in Test (Python)\nDESCRIPTION: A test function `test_a1` in `a/test_db.py` that depends on the `db` fixture defined in `a/conftest.py`. It asserts 0 to demonstrate the fixture value.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/simple.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n# content of a/test_db.py\ndef test_a1(db):\n    assert 0, db  # to show value\n```\n\n----------------------------------------\n\nTITLE: Implementing Pytest Fixtures with addfinalizer for Email Testing (Python)\nDESCRIPTION: This Python code demonstrates how to use `pytest.fixture` in conjunction with `request.addfinalizer` to ensure proper resource cleanup (e.g., deleting users, clearing mailboxes) after tests. It provides a structured way to manage setup and teardown, offering an alternative to `yield` fixtures for explicit finalization.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# content of test_emaillib.py\nfrom emaillib import Email, MailAdminClient\n\nimport pytest\n\n\n@pytest.fixture\ndef mail_admin():\n    return MailAdminClient()\n\n\n@pytest.fixture\ndef sending_user(mail_admin):\n    user = mail_admin.create_user()\n    yield user\n    mail_admin.delete_user(user)\n\n\n@pytest.fixture\ndef receiving_user(mail_admin, request):\n    user = mail_admin.create_user()\n\n    def delete_user():\n        mail_admin.delete_user(user)\n\n    request.addfinalizer(delete_user)\n    return user\n\n\n@pytest.fixture\ndef email(sending_user, receiving_user, request):\n    _email = Email(subject=\"Hey!\", body=\"How's it going?\")\n    sending_user.send_email(_email, receiving_user)\n\n    def empty_mailbox():\n        receiving_user.clear_mailbox()\n\n    request.addfinalizer(empty_mailbox)\n    return _email\n\n\ndef test_email_received(receiving_user, email):\n    assert email in receiving_user.inbox\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with Session Fixture and Observing Output\nDESCRIPTION: This snippet shows the command to execute the `pytest` tests (`pytest -q -s test_module.py`) and the expected console output. The `-q` flag provides quiet output, and `-s` disables output capturing, allowing `print` statements to be seen. The output demonstrates the execution order: the session fixture runs first, then the `callme` methods from each test class, followed by the individual test methods, confirming the fixture's pre-test execution capability.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/special.rst#_snippet_2\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest -q -s test_module.py\ncallattr_ahead_of_alltests called\ncallme called!\ncallme other called\nSomeTest callme called\ntest_method1 called\n.test_method2 called\n.test other\n.test_unit1 method called\n.\n4 passed in 0.12s\n```\n\n----------------------------------------\n\nTITLE: Mocking API Response Objects with Monkeypatch (Python)\nDESCRIPTION: This test demonstrates how to use `monkeypatch.setattr` to mock the `requests.get` function. It defines a `MockResponse` class with a `json()` method to simulate an API response, allowing tests to run without actual network calls and ensuring predictable test outcomes.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport app\n\n\nclass MockResponse:\n    @staticmethod\n    def json():\n        return {\"mock_key\": \"mock_response\"}\n\n\ndef test_get_json(monkeypatch):\n    def mock_get(*args, **kwargs):\n        return MockResponse()\n\n    monkeypatch.setattr(requests, \"get\", mock_get)\n\n    result = app.get_json(\"https://fakeurl\")\n    assert result[\"mock_key\"] == \"mock_response\"\n```\n\n----------------------------------------\n\nTITLE: Requesting Multiple Pytest Fixtures in Python\nDESCRIPTION: This example demonstrates how pytest tests and fixtures can request multiple other fixtures. The `order` fixture depends on `first_entry` and `second_entry`, while `test_string` requests both `order` and `expected_list` to perform its assertions.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n\n@pytest.fixture\ndef first_entry():\n    return \"a\"\n\n\n@pytest.fixture\ndef second_entry():\n    return 2\n\n\n@pytest.fixture\ndef order(first_entry, second_entry):\n    return [first_entry, second_entry]\n\n\n@pytest.fixture\ndef expected_list():\n    return [\"a\", 2, 3.0]\n\n\ndef test_string(order, expected_list):\n    # Act\n    order.append(3.0)\n\n    # Assert\n    assert order == expected_list\n```\n\n----------------------------------------\n\nTITLE: End-to-End Login and Landing Page Tests with Pytest Fixtures (Python)\nDESCRIPTION: This snippet defines a suite of end-to-end tests for a login and landing page using pytest. It demonstrates the use of class-scoped fixtures for setting up an AdminApiClient, creating/deleting test users, managing a Selenium Chrome driver, and handling the login process. It includes tests for verifying user name in the header, sign-out button visibility, and profile link correctness.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# contents of tests/end_to_end/test_login.py\nfrom uuid import uuid4\nfrom urllib.parse import urljoin\n\nfrom selenium.webdriver import Chrome\nimport pytest\n\nfrom src.utils.pages import LoginPage, LandingPage\nfrom src.utils import AdminApiClient\nfrom src.utils.data_types import User\n\n\n@pytest.fixture(scope=\"class\")\ndef admin_client(base_url, admin_credentials):\n    return AdminApiClient(base_url, **admin_credentials)\n\n\n@pytest.fixture(scope=\"class\")\ndef user(admin_client):\n    _user = User(name=\"Susan\", username=f\"testuser-{uuid4()}\", password=\"P4$$word\")\n    admin_client.create_user(_user)\n    yield _user\n    admin_client.delete_user(_user)\n\n\n@pytest.fixture(scope=\"class\")\ndef driver():\n    _driver = Chrome()\n    yield _driver\n    _driver.quit()\n\n\n@pytest.fixture(scope=\"class\")\ndef landing_page(driver, login):\n    return LandingPage(driver)\n\n\nclass TestLandingPageSuccess:\n    @pytest.fixture(scope=\"class\", autouse=True)\n    def login(self, driver, base_url, user):\n        driver.get(urljoin(base_url, \"/login\"))\n        page = LoginPage(driver)\n        page.login(user)\n\n    def test_name_in_header(self, landing_page, user):\n        assert landing_page.header == f\"Welcome, {user.name}!\"\n\n    def test_sign_out_button(self, landing_page):\n        assert landing_page.sign_out_button.is_displayed()\n\n    def test_profile_link(self, landing_page, user):\n        profile_href = urljoin(base_url, f\"/profile?id={user.profile_id}\")\n        assert landing_page.profile_link.get_attribute(\"href\") == profile_href\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with Parametrized Fixtures\nDESCRIPTION: This snippet shows the output of running `pytest` on `test_module.py`, demonstrating how parametrized fixtures cause tests to execute multiple times for different values. It highlights the detailed failure reports for each parametrized test, including the specific parameter value in the test ID, and the setup/teardown output for fixture finalization. The output indicates that `test_ehlo` and `test_noop` each ran twice, once for `smtp.gmail.com` and once for `mail.python.org`.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_39\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest -q test_module.py\nFFFF                                                                 [100%]\n================================= FAILURES =================================\n________________________ test_ehlo[smtp.gmail.com] _________________________\n\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0004>\n\n    def test_ehlo(smtp_connection):\n        response, msg = smtp_connection.ehlo()\n        assert response == 250\n        assert b\"smtp.gmail.com\" in msg\n>       assert 0  # for demo purposes\nE       assert 0\n\ntest_module.py:7: AssertionError\n________________________ test_noop[smtp.gmail.com] _________________________\n\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0004>\n\n    def test_noop(smtp_connection):\n        response, msg = smtp_connection.noop()\n        assert response == 250\n>       assert 0  # for demo purposes\nE       assert 0\n\ntest_module.py:13: AssertionError\n________________________ test_ehlo[mail.python.org] ________________________\n\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0005>\n\n    def test_ehlo(smtp_connection):\n        response, msg = smtp_connection.ehlo()\n        assert response == 250\n>       assert b\"smtp.gmail.com\" in msg\nE       AssertionError: assert b'smtp.gmail.com' in b'mail.python.org\\nPIPELINING\\nSIZE 51200000\\nETRN\\nSTARTTLS\\nAUTH DIGEST-MD5 NTLM CRAM-MD5\\nENHANCEDSTATUSCODES\\n8BITMIME\\nDSN\\nSMTPUTF8\\nCHUNKING'\n\ntest_module.py:6: AssertionError\n-------------------------- Captured stdout setup ---------------------------\nfinalizing <smtplib.SMTP object at 0xdeadbeef0004>\n________________________ test_noop[mail.python.org] ________________________\n\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0005>\n\n    def test_noop(smtp_connection):\n        response, msg = smtp_connection.noop()\n        assert response == 250\n>       assert 0  # for demo purposes\nE       assert 0\n\ntest_module.py:13: AssertionError\n------------------------- Captured stdout teardown -------------------------\nfinalizing <smtplib.SMTP object at 0xdeadbeef0005>\n========================= short test summary info ==========================\nFAILED test_module.py::test_ehlo[smtp.gmail.com] - assert 0\nFAILED test_module.py::test_noop[smtp.gmail.com] - assert 0\nFAILED test_module.py::test_ehlo[mail.python.org] - AssertionError: asser...\nFAILED test_module.py::test_noop[mail.python.org] - assert 0\n4 failed in 0.12s\n```\n\n----------------------------------------\n\nTITLE: Pytest Function Using Multiple Fixtures (Python)\nDESCRIPTION: This Python snippet defines `test_2`, a Pytest test function that simultaneously uses both the `otherarg` (function-scoped) and `modarg` (module-scoped) fixtures. This demonstrates how Pytest handles the interaction and ordering of fixtures with different scopes when multiple are required by a single test.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndef test_2(otherarg, modarg):\n    print(f\"  RUN test2 with otherarg {otherarg} and modarg {modarg}\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Basic Fixture-like Functions and Tests in Python\nDESCRIPTION: This conceptual example illustrates how functions can act as simple 'fixtures' (`first_entry`, `order`) providing setup data for 'tests' (`test_string`, `test_int`). It shows how a list can be modified by a test and then asserted against an expected state.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef first_entry():\n    return \"a\"\n\n\ndef order(first_entry):\n    return [first_entry]\n\n\ndef test_string(order):\n    # Act\n    order.append(\"b\")\n\n    # Assert\n    assert order == [\"a\", \"b\"]\n\n\ndef test_int(order):\n    # Act\n    order.append(2)\n\n    # Assert\n    assert order == [\"a\", 2]\n\n\nentry = first_entry()\nthe_list = order(first_entry=entry)\ntest_string(order=the_list)\n\nentry = first_entry()\nthe_list = order(first_entry=entry)\ntest_int(order=the_list)\n```\n\n----------------------------------------\n\nTITLE: Partial Pytest Module for Fixture Grouping Example\nDESCRIPTION: This snippet provides the initial lines of a Python test module (`test_module.py`) intended to demonstrate pytest's automatic grouping of tests by fixture instances. It sets up the basic structure for further examples involving parametrized fixtures and their setup/teardown flow.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# content of test_module.py\n    import pytest\n```\n\n----------------------------------------\n\nTITLE: Controlling Fixture Order with Autouse in pytest (Python)\nDESCRIPTION: This snippet demonstrates how `autouse` fixtures influence execution order. Autouse fixtures are executed first within their scope, and any fixtures they depend on effectively become autouse for the tests where the primary autouse fixture applies, helping to resolve dependency ambiguities.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/reference/fixtures.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef a():\n    print(\"fixture a\")\n    return \"a\"\n\n@pytest.fixture\ndef b(a):\n    print(\"fixture b\")\n    return \"b\"\n\n@pytest.fixture(autouse=True)\ndef c(b):\n    print(\"fixture c (autouse)\")\n    return \"c\"\n\n@pytest.fixture\ndef d():\n    print(\"fixture d\")\n    return \"d\"\n\ndef test_autouse_order(d):\n    print(\"test_autouse_order executed\")\n    assert True\n```\n\n----------------------------------------\n\nTITLE: Illustrating Pytest Fixture Error Handling and Execution Order in Python\nDESCRIPTION: This example showcases pytest's behavior when a fixture raises an exception. It defines a series of dependent fixtures (`order`, `append_first`, `append_second`, `append_third`) that modify a shared list. If an earlier fixture, like `append_first`, fails, pytest will stop executing subsequent fixtures and the test itself, marking the test as an error rather than a failure, indicating it couldn't be attempted.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/explanation/fixtures.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n\n@pytest.fixture\ndef order():\n    return []\n\n\n@pytest.fixture\ndef append_first(order):\n    order.append(1)\n\n\n@pytest.fixture\ndef append_second(order, append_first):\n    order.extend([2])\n\n\n@pytest.fixture(autouse=True)\ndef append_third(order, append_second):\n    order += [3]\n\n\ndef test_order(order):\n    assert order == [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Parametrizing Fixture with `pytest.fixture_request` (Alternative Proposal)\nDESCRIPTION: This alternative proposal uses a `pytest.fixture_request` helper within the `params` argument of a fixture. The `context` fixture is parametrized to yield values from `default_context` and `extra_context` sequentially. This approach allows a fixture to directly consume the values generated by other fixtures, providing a more explicit and compatible way to combine parametrized fixtures.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/proposals/parametrize_with_fixtures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture(\n    params=[\n        pytest.fixture_request(\"default_context\"),\n        pytest.fixture_request(\"extra_context\"),\n    ]\n)\ndef context(request):\n    \"\"\"Returns all values for ``default_context``, one-by-one before it\n    does the same for ``extra_context``.\n\n    request.param:\n        - {}\n        - {'author': 'alice'}\n        - {'project_slug': 'helloworld'}\n        - {'author': 'bob', 'project_slug': 'foobar'}\n    \"\"\"\n    return request.param\n```\n\n----------------------------------------\n\nTITLE: Defining Module-Scoped Pytest Fixtures in Python\nDESCRIPTION: This conceptual snippet describes how to define a pytest fixture with `scope=\"module\"`. Such a fixture, like `smtp_connection`, is invoked only once per test module, allowing multiple test functions within that module to share the same instance and save setup time for expensive resources.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n@pytest.fixture(scope=\"module\")\ndef smtp_connection():\n    # Establish a connection to an SMTP server\n    connection = \"SMTP_CONNECTION_OBJECT\" # Placeholder for actual connection logic\n    yield connection\n    # Teardown: Close the connection\n    # connection.close()\n```\n\n----------------------------------------\n\nTITLE: Deferring Pytest Fixture Setup with `indirect` Parametrization in Python\nDESCRIPTION: This `conftest.py` file defines a `pytest_generate_tests` hook and a `db` fixture to demonstrate deferred setup of parametrized resources. The hook parametrizes the `db` fixture with values `d1` and `d2` using `indirect=True`, which tells pytest to pass the values to the `db` fixture for instantiation. The `db` fixture then creates either a `DB1` or `DB2` object based on the requested parameter, ensuring expensive resource setup occurs only when the test runs.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/parametrize.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# content of conftest.py\nimport pytest\n\n\ndef pytest_generate_tests(metafunc):\n    if \"db\" in metafunc.fixturenames:\n        metafunc.parametrize(\"db\", [\"d1\", \"d2\"], indirect=True)\n\n\nclass DB1:\n    \"one database object\"\n\n\nclass DB2:\n    \"alternative database object\"\n\n\n@pytest.fixture\ndef db(request):\n    if request.param == \"d1\":\n        return DB1()\n    elif request.param == \"d2\":\n        return DB2()\n    else:\n        raise ValueError(\"invalid internal test config\")\n```\n\n----------------------------------------\n\nTITLE: Verifying `usefixtures` Application (Pytest CLI)\nDESCRIPTION: This Pytest CLI command runs tests in quiet mode (`-q`). The output `..` followed by `2 passed` confirms that both test methods within `TestDirectoryInit` were executed successfully, implying that the `cleandir` fixture was correctly applied via the `usefixtures` marker, ensuring a clean directory for each test.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_55\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest -q\n..                                                                   [100%]\n2 passed in 0.12s\n```\n\n----------------------------------------\n\nTITLE: Testing Missing User Configuration Error Handling (Python)\nDESCRIPTION: This test ensures that `app.create_connection_string()` raises a `KeyError` when the `user` configuration is missing, as simulated by the `mock_missing_default_user` fixture. It uses `pytest.raises` to assert the exception.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/monkeypatch.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ndef test_missing_user(mock_missing_default_user):\n    with pytest.raises(KeyError):\n        _ = app.create_connection_string()\n```\n\n----------------------------------------\n\nTITLE: Defining Pytest and Unittest Test Classes with `callme` Methods in Python\nDESCRIPTION: This Python code defines multiple test classes (`TestHello`, `TestOther`, `SomeTest`) for `pytest` and `unittest` frameworks in `test_module.py`. Each class includes a `callme` class method, which is designed to be invoked by the session-scoped fixture before any tests in the class run. This demonstrates how test classes can expose hooks for pre-test execution logic, compatible with both `pytest` and standard `unittest` frameworks.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/special.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# content of test_module.py\n\n\nclass TestHello:\n    @classmethod\n    def callme(cls):\n        print(\"callme called!\")\n\n    def test_method1(self):\n        print(\"test_method1 called\")\n\n    def test_method2(self):\n        print(\"test_method2 called\")\n\n\nclass TestOther:\n    @classmethod\n    def callme(cls):\n        print(\"callme other called\")\n\n    def test_other(self):\n        print(\"test other\")\n\n\n# works with unittest as well ...\nimport unittest\n\n\nclass SomeTest(unittest.TestCase):\n    @classmethod\n    def callme(self):\n        print(\"SomeTest callme called\")\n\n    def test_unit1(self):\n        print(\"test_unit1 method called\")\n```\n\n----------------------------------------\n\nTITLE: Defining Interdependent Pytest Fixtures\nDESCRIPTION: This Python snippet defines an `App` class and an `app` fixture that depends on another fixture, `smtp_connection`. It illustrates how fixtures can consume other fixtures, promoting modularity and reusability in test setups, and how `App` instances are created with the injected `smtp_connection`.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# content of test_appsetup.py\n\n    import pytest\n\n\n    class App:\n        def __init__(self, smtp_connection):\n            self.smtp_connection = smtp_connection\n\n\n    @pytest.fixture(scope=\"module\")\n    def app(smtp_connection):\n        return App(smtp_connection)\n\n\n    def test_smtp_connection_exists(app):\n        assert app.smtp_connection\n```\n\n----------------------------------------\n\nTITLE: Defining 'inner' Fixture and 'test_order' with Plugin Dependencies in pytest (tests/subpackage/test_subpackage.py)\nDESCRIPTION: This snippet from `tests/subpackage/test_subpackage.py` defines the `inner` fixture, which appends 'inner subpackage' to `order` and depends on `order`, `mid`, and `a_fix` (a plugin fixture). The `test_order` function asserts the final execution order, illustrating the complex interplay of local, `conftest.py`, and plugin-provided fixtures.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/reference/fixtures.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef inner(order, mid, a_fix):\n    order.append(\"inner subpackage\")\n\ndef test_order(order, inner):\n    assert order == [\"b_fix\", \"mid subpackage\", \"a_fix\", \"inner subpackage\"]\n```\n\n----------------------------------------\n\nTITLE: Pytest Fixture Caching and Side Effects in Python\nDESCRIPTION: This snippet illustrates pytest's fixture caching mechanism, where a fixture is executed only once per test, and its return value is cached. The `append_first` fixture modifies the `order` list, and `test_string_only` observes this modification because both reference the same cached `order` object.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n\n@pytest.fixture\ndef first_entry():\n    return \"a\"\n\n\n@pytest.fixture\ndef order():\n    return []\n\n\n@pytest.fixture\ndef append_first(order, first_entry):\n    return order.append(first_entry)\n\n\ndef test_string_only(append_first, order, first_entry):\n    # Assert\n    assert order == [first_entry]\n```\n\n----------------------------------------\n\nTITLE: Executing Pytest with Autouse Fixture Test Module\nDESCRIPTION: This snippet shows the command-line execution of a Python test file containing a unittest.TestCase with an autouse pytest fixture. The output confirms that the test passes, demonstrating the successful application of the initdir fixture before the test method runs.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/unittest.rst#_snippet_7\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest -q test_unittest_cleandir.py\n.                                                                    [100%]\n1 passed in 0.12s\n```\n\n----------------------------------------\n\nTITLE: Pytest Function Using a Single Fixture (Python)\nDESCRIPTION: This Python snippet defines a simple Pytest test function `test_0` that consumes the `otherarg` fixture. The fixture's value is passed as an argument to the test function. This demonstrates the basic mechanism of injecting fixture values into tests.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef test_0(otherarg):\n    print(\"  RUN test0 with otherarg\", otherarg)\n```\n\n----------------------------------------\n\nTITLE: Controlling Fixture Order by Scope in pytest (Python)\nDESCRIPTION: This snippet demonstrates how pytest executes higher-scoped fixtures (e.g., `session`) before lower-scoped fixtures (e.g., `function` or `class`). The test will pass because the `session`-scoped fixture runs before the `function`-scoped one, ensuring dependencies are met based on scope.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/reference/fixtures.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef session_fixture():\n    print(\"session_fixture executed\")\n    return \"session_data\"\n\n@pytest.fixture(scope=\"function\")\ndef function_fixture(session_fixture):\n    print(f\"function_fixture executed, received: {session_fixture}\")\n    return \"function_data\"\n\ndef test_order(function_fixture):\n    print(f\"test_order executed, received: {function_fixture}\")\n    assert function_fixture == \"function_data\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Test Function with a Fixture in Python\nDESCRIPTION: This Python snippet defines a simple test function `test_valid_string` that accepts a `stringinput` fixture. The test asserts that the provided string input consists only of alphabetic characters, serving as a basic example for dynamic parametrization.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/parametrize.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# content of test_strings.py\n\n\ndef test_valid_string(stringinput):\n    assert stringinput.isalpha()\n```\n\n----------------------------------------\n\nTITLE: Pytest Test Execution Output with Module-Scoped Fixture\nDESCRIPTION: This snippet shows the console output when running `pytest test_module.py`. It demonstrates that both `test_ehlo` and `test_noop` fail due to the `assert 0` statements. Crucially, the traceback confirms that the *same* `smtp_connection` object instance was passed to both test functions, highlighting the effect of the `module` scope.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/how-to/fixtures.rst#_snippet_13\n\nLANGUAGE: pytest\nCODE:\n```\n$ pytest test_module.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-8.x.y, pluggy-1.x.y\nrootdir: /home/sweet/project\ncollected 2 items\n\ntest_module.py FF                                                    [100%]\n\n================================= FAILURES =================================\n________________________________ test_ehlo _________________________________\n\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0001>\n\n    def test_ehlo(smtp_connection):\n        response, msg = smtp_connection.ehlo()\n        assert response == 250\n        assert b\"smtp.gmail.com\" in msg\n>       assert 0  # for demo purposes\nE       assert 0\n\ntest_module.py:7: AssertionError\n________________________________ test_noop _________________________________\n\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0001>\n\n    def test_noop(smtp_connection):\n        response, msg = smtp_connection.noop()\n        assert response == 250\n>       assert 0  # for demo purposes\nE       assert 0\n\ntest_module.py:13: AssertionError\n========================= short test summary info ==========================\nFAILED test_module.py::test_ehlo - assert 0\nFAILED test_module.py::test_noop - assert 0\n============================ 2 failed in 0.12s =============================\n```\n\n----------------------------------------\n\nTITLE: Defining Tests for Fixture Finalizer Example\nDESCRIPTION: Defines tests in `test_module.py` to be used with the `conftest.py` plugin. It includes a fixture `other` that fails during setup, a test `test_setup_fails` that uses this fixture, a test `test_call_fails` that fails during the call phase, and another failing test `test_fail2`, all designed to trigger different failure scenarios.\nSOURCE: https://github.com/pytest-dev/pytest/blob/main/doc/en/example/simple.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# content of test_module.py\n\nimport pytest\n\n\n@pytest.fixture\ndef other():\n    assert 0\n\n\ndef test_setup_fails(something, other):\n    pass\n\n\ndef test_call_fails(something):\n    assert 0\n\n\ndef test_fail2():\n    assert 0\n```"}],"isError":null}

Now I'll search for Streamlit documentation to help with the UI components and best practices.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 resolve-library-id [object Object]
 {"content":[{"type":"text","text":"Available Libraries (top matches):\n\nEach result includes:\n- Library ID: Context7-compatible identifier (format: /org/project)\n- Name: Library or package name\n- Description: Short summary\n- Code Snippets: Number of available code examples\n- Trust Score: Authority indicator\n- Versions: List of versions if available. Use one of those versions if and only if the user explicitly provides a version in their query.\n\nFor best results, select libraries based on name match, trust score, snippet coverage, and relevance to your use case.\n\n----------\n\n- Title: Streamlit\n- Context7-compatible library ID: /streamlit/docs\n- Description: Source code for the Streamlit Python library documentation\n- Code Snippets: 2799\n- Trust Score: 8.9\n----------\n- Title: Streamlit\n- Context7-compatible library ID: /context7/streamlit_io\n- Description: Streamlit is an open-source Python framework for data scientists and AI/ML engineers to deliver dynamic data apps with only a few lines of code, enabling rapid building and deployment.\n- Code Snippets: 3789\n- Trust Score: 10\n----------\n- Title: Streamlit WebRTC\n- Context7-compatible library ID: /whitphx/streamlit-webrtc\n- Description: Real-time video and audio processing on Streamlit\n- Code Snippets: 18\n- Trust Score: 9.7\n----------\n- Title: Streamlit Elements\n- Context7-compatible library ID: /okld/streamlit-elements\n- Description: Create a draggable and resizable dashboard in Streamlit, featuring Material UI widgets, Monaco editor (Visual Studio Code), Nivo charts, and more!\n- Code Snippets: 19\n- Trust Score: 7.2\n----------\n- Title: Streamlit Monaco\n- Context7-compatible library ID: /marcusschiesser/streamlit-monaco\n- Description: Monaco editor (Visual Studio Code) for Streamlit\n- Code Snippets: 2\n- Trust Score: 9.8\n----------\n- Title: Streamlit Option Menu\n- Context7-compatible library ID: /victoryhb/streamlit-option-menu\n- Description: streamlit-option-menu is a simple Streamlit component that allows users to select a single item from a list of options in a menu. \n- Code Snippets: 8\n- Trust Score: 7.3\n----------\n- Title: Streamlit Authenticator\n- Context7-compatible library ID: /mkhorasani/streamlit-authenticator\n- Description: A secure authentication module to manage user access in a Streamlit application.\n- Code Snippets: 15\n- Trust Score: 9.4\n----------\n- Title: Streamlit Condition Tree\n- Context7-compatible library ID: /cedricvlt/streamlit-condition-tree\n- Description: A Streamlit component to build a condition tree\n- Code Snippets: 5\n- Trust Score: 5.4\n----------\n- Title: Streamlit Browser Storage\n- Context7-compatible library ID: /kosfera/streamlit-browser-storage\n- Description: \n- Code Snippets: 3\n- Trust Score: 2.1\n----------\n- Title: Streamlit Antd Components\n- Context7-compatible library ID: /nicedouble/streamlitantdcomponents\n- Description: A Streamlit component to display Antd-Design\n- Code Snippets: 2\n- Trust Score: 7.7\n----------\n- Title: Streamlit Shadcn UI\n- Context7-compatible library ID: /observedobserver/streamlit-shadcn-ui-docs\n- Description: docs and examples of streamlit-shadcn-ui\n- Code Snippets: 23\n----------\n- Title: Streamlit Navigation Bar\n- Context7-compatible library ID: /gabrieltempass/streamlit-navigation-bar\n- Description: A component that allows you to place a navigation bar in your Streamlit app.\n- Code Snippets: 13\n- Trust Score: 8.3\n----------\n- Title: LongChain Streamlit PDFReader\n- Context7-compatible library ID: /lenkazuma/pdfreader\n- Description: LongChain Streamlit PDFReader\n- Code Snippets: 22\n- Trust Score: 5"}],"isError":null}

I'll get Streamlit documentation to help with UI best practices and component usage.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 get-library-docs [object Object]
 {"content":[{"type":"text","text":"TITLE: Streamlit Forms with Session State and Callbacks\nDESCRIPTION: Shows how to combine Streamlit forms (`st.form`) with session state and callbacks to update multiple state variables (count and last updated time) upon form submission.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nimport datetime\nst.title('Counter Example')\nif 'count' not in st.session_state:\nst.session_state.count = 0\nst.session_state.last_updated = datetime.time(0,0)\ndef update_counter():\n  st.session_state.count += st.session_state.increment_value\n  st.session_state.last_updated = st.session_state.update_time\nwith st.form(key='my_form'):\n  st.time_input(label='Enter the time', value=datetime.datetime.now().time(), key='update_time')\n  st.number_input('Enter a value', value=0, step=1, key='increment_value')\n  submit = st.form_submit_button(label='Update', on_click=update_counter)\nst.write('Current Count = ', st.session_state.count)\nst.write('Last Updated = ', st.session_state.last_updated)\n```\n\n----------------------------------------\n\nTITLE: Streamlit API: `st.file_uploader` - Session State Reset Fix\nDESCRIPTION: A bug fix addresses an issue where `st.file_uploader` would cause session state to reset when a websocket connection was dropped and reconnected. This ensures state persistence and a more robust user experience during file uploads.\nSOURCE: https://docs.streamlit.io/develop/quick-reference/release-notes/2023\n\nLANGUAGE: APIDOC\nCODE:\n```\nst.file_uploader\n  Bug Fix: No longer causes session state to reset when websocket connection is dropped and reconnected.\n```\n\n----------------------------------------\n\nTITLE: Handle Uninitialized Streamlit Session State Access\nDESCRIPTION: Shows that attempting to access an uninitialized variable in Streamlit's session state will result in an exception.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nst.write(st.session_state['value'])\n# Throws an exception!\n```\n\n----------------------------------------\n\nTITLE: Attempting to Set Streamlit Button State via Session State (Error)\nDESCRIPTION: This snippet illustrates a known limitation in Streamlit where attempting to set the state of an `st.button` widget directly via `st.session_state` will raise a `StreamlitAPIException`. This behavior is specific to `st.button` and `st.file_uploader`.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nif 'my_button' not in st.session_state:\nst.session_state.my_button = True\n# Streamlit will raise an Exception on trying to set the state of button\nst.button('Submit', key='my_button')\n```\n\n----------------------------------------\n\nTITLE: Streamlit API: `st.file_uploader` Multi-part File Extension Handling Bug Fix\nDESCRIPTION: Fixed a bug allowing `st.file_uploader` to correctly handle multi-part file extensions specified in its `type` parameter, improving file type validation and upload reliability.\nSOURCE: https://docs.streamlit.io/develop/quick-reference/release-notes/2025\n\nLANGUAGE: APIDOC\nCODE:\n```\nst.file_uploader(label: str, ..., type: list[str])\n  Behavior: Correctly handles multi-part file extensions (e.g., ['tar.gz']) in the 'type' parameter (bug fix).\n```\n\n----------------------------------------\n\nTITLE: Process Streamlit Form with Callback and Session State\nDESCRIPTION: This code demonstrates how to process form submissions in Streamlit by using an `on_click` callback function with `st.form_submit_button`. It leverages `st.session_state` to store and retrieve input values (`a`, `b`) and the calculated sum. Widgets within the form are assigned keys, allowing their updated values to be accessed directly from `st.session_state` within the callback, ensuring immediate reflection of changes.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/forms\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nif 'sum' not in st.session_state:\nst.session_state.sum = ''\ndef sum():\nresult = st.session_state.a + st.session_state.b\nst.session_state.sum = result\ncol1,col2 = st.columns(2)\ncol1.title('Sum:')\nif isinstance(st.session_state.sum, float):\ncol2.title(f'{st.session_state.sum:.2f}')\nwith st.form('addition'):\nst.number_input('a', key = 'a')\nst.number_input('b', key = 'b')\nst.form_submit_button('add', on_click=sum)\n```\n\n----------------------------------------\n\nTITLE: Handle Unserializable Data with Enforced Serializable Session State\nDESCRIPTION: This Python code shows an example where storing a non-pickle-serializable object (like a lambda function) in `st.session_state` will result in an exception. This occurs when the `runner.enforceSerializableSessionState` configuration option is enabled, enforcing strict serialization.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\ndef unserializable_data():\nreturn lambda x: x\n#👇 results in an exception when enforceSerializableSessionState is on\nst.session_state.unserializable = unserializable_data()\n```\n\n----------------------------------------\n\nTITLE: Improved Error Handling for st.secrets\nDESCRIPTION: This update includes fixes for `st.secrets`, ensuring more robust error handling. A `FileNotFoundError` is now correctly raised if the `secrets.toml` file is missing, and a clear `TypeError` is provided when attempting to assign a value to an attribute, improving debugging.\nSOURCE: https://docs.streamlit.io/develop/quick-reference/release-notes/2025\n\nLANGUAGE: APIDOC\nCODE:\n```\nst.secrets\n```\n\n----------------------------------------\n\nTITLE: Check Streamlit Session State for 'correct' Stage\nDESCRIPTION: Checks if the current application stage, stored in `st.session_state.stage`, is set to 'correct' to activate the error correction UI and logic.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nelif st.session_state.stage == \"correct\":\n```\n\n----------------------------------------\n\nTITLE: Handle 'Correct errors' Button Click\nDESCRIPTION: This block defines a primary button in the first column. When clicked, it saves the current validation state (sentences and validity) into `st.session_state`, updates the application stage to 'correct', and triggers a rerun of the Streamlit app. The button is disabled if all items in `validation_list` are true, indicating no errors.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nif cols[0].button(\n    \"Correct errors\", type=\"primary\", disabled=all(validation_list)\n):\n    st.session_state.validation = {\n        \"sentences\": response_sentences,\n        \"valid\": validation_list\n    }\n    st.session_state.stage = \"correct\"\n    st.rerun()\n```\n\n----------------------------------------\n\nTITLE: Initialize and Update Streamlit Session State\nDESCRIPTION: Illustrates how to initialize a session state variable if it doesn't exist and subsequently update its value using both attribute-based and dictionary-like API access.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nif 'key' not in st.session_state:\nst.session_state['key'] = 'value'\n# Updates\nst.session_state.key = 'value2' # Attribute API\nst.session_state['key'] = 'value2' # Dictionary like API\n```\n\n----------------------------------------\n\nTITLE: Streamlit Counter with Session State and Callbacks\nDESCRIPTION: Demonstrates how to update Streamlit session state using a callback function, `increment_counter()`, which is triggered by a button click via the `on_click` parameter.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nst.title('Counter Example using Callbacks')\nif 'count' not in st.session_state:\nst.session_state.count = 0\ndef increment_counter():\n  st.session_state.count += 1\nst.button('Increment', on_click=increment_counter)\nst.write('Count = ', st.session_state.count)\n```\n\n----------------------------------------\n\nTITLE: Handle 'Update' Button Click in Streamlit\nDESCRIPTION: When the 'Update' button is clicked, this code appends the newly rewritten response to the chat history. It then clears the pending message and validation information from the session state.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: python\nCODE:\n```\nst.session_state.history.append({\"role\": \"assistant\", \"content\": new})\nst.session_state.pending = None\nst.session_state.validation = {}\n```\n\n----------------------------------------\n\nTITLE: Read Streamlit Session State Value\nDESCRIPTION: Demonstrates how to read a value from Streamlit's session state using `st.write`.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: Python\nCODE:\n```\nst.write(st.session_state.key)\n```\n\n----------------------------------------\n\nTITLE: Initialize Streamlit Session State for Chat Application\nDESCRIPTION: This snippet initializes critical session state variables for a Streamlit chat application, including `stage` for tracking user flow, `history` for conversation logs, `pending` for unapproved responses, and `validation` for storing validation details. It ensures persistent state across reruns of the application.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nif \"stage\" not in st.session_state:\n    st.session_state.stage = \"user\"\n    st.session_state.history = []\n    st.session_state.pending = None\n    st.session_state.validation = {}\n```\n\n----------------------------------------\n\nTITLE: Streamlit Widget Update with Session State and Callbacks\nDESCRIPTION: Demonstrates how to correctly update a Streamlit slider widget using `st.session_state` and a callback function with `st.button`'s `on_click` parameter. It highlights the importance of referencing the widget's unique key in session state rather than its assigned variable to ensure immediate updates.\nSOURCE: https://docs.streamlit.io/knowledge-base/using-streamlit/widget-updating-session-state\n\nLANGUAGE: Python\nCODE:\n```\n# the callback function for the button will add 1 to the\n# slider value up to 10\ndef plus_one():\n    if st.session_state[\"slider\"] < 10:\n        st.session_state.slider += 1\n    else:\n        pass\n    return\n\n# when creating the button, assign the name of your callback\n# function to the on_click parameter\nadd_one = st.button(\"Add one to the slider\", on_click=plus_one, key=\"add_one\")\n\n# create the slider\nslide_val = st.slider(\"Pick a number\", 0, 10, key=\"slider\")\n```\n\n----------------------------------------\n\nTITLE: Manage Streamlit Session State\nDESCRIPTION: Session state is a way to share variables between reruns, for each user session.\nSOURCE: https://docs.streamlit.io/en/0.71.0/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Streamlit Anti-pattern: Ephemeral Process Results from Buttons\nDESCRIPTION: This snippet demonstrates an anti-pattern where a process (like reading a file or saving data) is executed within a button's conditional block without explicitly saving the results to `st.session_state`. Consequently, any displayed data or computed results will disappear on the next user interaction, as Streamlit reruns the script and the ephemeral state is lost. This can also lead to errors if subsequent button clicks depend on the lost state.\nSOURCE: https://docs.streamlit.io/develop/concepts/design/buttons\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nimport pandas as pd\nfile = st.file_uploader(\"Upload a file\", type=\"csv\")\nif st.button('Get data'):\ndf = pd.read_csv(file)\n# This display will go away with the user's next action.\nst.write(df)\nif st.button('Save'):\n# This will always error.\ndf.to_csv('data.csv')\n```\n\n----------------------------------------\n\nTITLE: Managing Session State with st.session_state\nDESCRIPTION: Streamlit's `st.session_state` allows developers to save and persist data between reruns of the script and across different pages, enabling the creation of stateful applications.\nSOURCE: https://docs.streamlit.io/develop/api-reference/caching-and-state\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state[\"foo\"] = \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Manage Session State in Streamlit\nDESCRIPTION: Session state is a way to share variables between reruns, for each user session. This allows maintaining stateful information across user interactions, making applications more dynamic and interactive.\nSOURCE: https://docs.streamlit.io/en/0.69.0/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Manage State with Streamlit Session State\nDESCRIPTION: Illustrates how Streamlit's session state can be used to share variables between reruns for each user session. This allows persistent data storage across interactions, maintaining application state.\nSOURCE: https://docs.streamlit.io/en/0.68.0/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Manage Session State in Streamlit\nDESCRIPTION: Explains how to use `st.session_state` to store and share variables across reruns for a specific user session. This mechanism is crucial for maintaining state and interactivity in Streamlit applications.\nSOURCE: https://docs.streamlit.io/en/0.75.0/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Allow File Uploads with Streamlit Chat Input\nDESCRIPTION: This example demonstrates how to configure `st.chat_input` to accept file attachments using the `accept_file` and `file_type` parameters. It shows how to access both the text message and the list of uploaded files from the dictionary-like object returned by the widget.\nSOURCE: https://docs.streamlit.io/develop/api-reference/chat/st\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\n\nprompt = st.chat_input(\n    \"Say something and/or attach an image\",\n    accept_file=True,\n    file_type=[\"jpg\", \"jpeg\", \"png\"]\n)\nif prompt and prompt.text:\n    st.markdown(prompt.text)\nif prompt and prompt[\"files\"]:\n    st.image(prompt[\"files\"][0])\n```\n\n----------------------------------------\n\nTITLE: Set and Get Streamlit Slider Value with Session State\nDESCRIPTION: This example demonstrates how to initialize and access the value of a Streamlit slider widget using `st.session_state` and the `key` argument. It illustrates the unification of widget state with the application's session state, allowing easy read/write access.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nif \"celsius\" not in st.session_state:\n# set the initial default value of the slider widget\nst.session_state.celsius = 50.0\nst.slider(\n\"Temperature in Celsius\",\nmin_value=-100.0,\nmax_value=100.0,\nkey=\"celsius\"\n)\n# This will get the value of the slider widget\nst.write(st.session_state.celsius)\n```\n\n----------------------------------------\n\nTITLE: Persisting Streamlit Widget Values with Session State (Generalized)\nDESCRIPTION: This Python code provides a generalized solution for persisting multiple Streamlit widget values using `st.session_state`. It includes `store_value` and `load_value` functions to manage the copying of values between temporary widget keys and permanent session state keys, allowing values to be retained even when widgets are not continually rendered.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/widget-behavior\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\ndef store_value(key):\nst.session_state[key] = st.session_state[\"_\"+key]\ndef load_value(key):\nst.session_state[\"_\"+key] = st.session_state[key]\nload_value(\"my_key\")\nst.number_input(\"Number of filters\", key=\"_my_key\", on_change=store_value, args=[\"my_key\"])\n```\n\n----------------------------------------\n\nTITLE: Handle 'Accept' Button Click in Streamlit\nDESCRIPTION: This snippet defines the logic executed when the 'Accept' button is clicked. It appends the pending message to the chat history and clears temporary session state variables related to pending messages and validation.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: python\nCODE:\n```\nif cols[0].button(\"Accept\", type=\"primary\"):\n    st.session_state.history.append(\n    {\"role\": \"assistant\", \"content\": st.session_state.pending}\n    )\n    st.session_state.pending = None\n    st.session_state.validation = {}\n```\n\n----------------------------------------\n\nTITLE: Manage Session State in Streamlit\nDESCRIPTION: Illustrates how to use `st.session_state` to share variables between reruns for each user session, allowing the application to maintain state across interactions.\nSOURCE: https://docs.streamlit.io/en/0.70.0/api\n\nLANGUAGE: python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Manage Session State in Streamlit\nDESCRIPTION: Illustrates how to use `st.session_state` to store and share variables across reruns for a specific user session. This is crucial for maintaining stateful interactions in Streamlit applications.\nSOURCE: https://docs.streamlit.io/en/0.65.0/api\n\nLANGUAGE: python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Enable Serializable Session State in Streamlit Configuration\nDESCRIPTION: This configuration snippet for `.streamlit/config.toml` demonstrates how to enable `enforceSerializableSessionState`. When set to `true`, Streamlit will only allow pickle-serializable objects to be stored in Session State, which can be useful for certain execution environments or for detecting incompatibility.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: toml\nCODE:\n```\n# .streamlit/config.toml\n[runner]\nenforceSerializableSessionState = true\n```\n\n----------------------------------------\n\nTITLE: Retrieve Filenames for Multiple File Uploads in Streamlit\nDESCRIPTION: This snippet shows how to retrieve filenames when multiple files are uploaded using `st.file_uploader` with `accept_multiple_files=True`. It iterates through the list of `UploadedFile` objects and accesses the `.name` attribute for each.\nSOURCE: https://docs.streamlit.io/knowledge-base/using-streamlit/retrieve-filename-uploaded\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nuploaded_files = st.file_uploader(\"Upload multiple files\", accept_multiple_files=True)\nif uploaded_files:\nfor uploaded_file in uploaded_files:\nst.write(\"Filename: \", uploaded_file.name)\n```\n\n----------------------------------------\n\nTITLE: Manage Session State in Streamlit\nDESCRIPTION: Illustrates how to use `st.session_state` to store and share variables across reruns for a specific user session. This enables persistence of data and UI elements.\nSOURCE: https://docs.streamlit.io/en/latest/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Python Example: Storing Unserializable Data in Streamlit Session State\nDESCRIPTION: This Python code demonstrates an attempt to store an unserializable object (a lambda function) into `st.session_state`. If Streamlit's `runner.enforceSerializableSessionState` configuration option is set to `true`, this operation will raise an exception, indicating that the data is not pickle-serializable and thus incompatible with the enforced serialization requirement.\nSOURCE: https://docs.streamlit.io/knowledge-base/using-streamlit/serializable-session-state\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\ndef unserializable_data():\n    return lambda x: x\n#👇 results in an exception when enforceSerializableSessionState is on\nst.session_state.unserializable = unserializable_data()\n```\n\n----------------------------------------\n\nTITLE: Retrieve Filename for Single File Upload in Streamlit\nDESCRIPTION: This snippet demonstrates how to get the filename when a single file is uploaded using `st.file_uploader`. The `UploadedFile` object returned by `st.file_uploader` has a `.name` attribute that contains the filename.\nSOURCE: https://docs.streamlit.io/knowledge-base/using-streamlit/retrieve-filename-uploaded\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nuploaded_file = st.file_uploader(\"Upload a file\")\nif uploaded_file:\nst.write(\"Filename: \", uploaded_file.name)\n```\n\n----------------------------------------\n\nTITLE: Implement Basic Counter with Streamlit Session State\nDESCRIPTION: A simple Streamlit application demonstrating a counter that persists its value across reruns using `st.session_state` to store the count.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nst.title('Counter Example')\nif 'count' not in st.session_state:\nst.session_state.count = 0\nincrement = st.button('Increment')\nif increment:\nst.session_state.count += 1\nst.write('Count = ', st.session_state.count)\n```\n\n----------------------------------------\n\nTITLE: Read Value from Streamlit Session State\nDESCRIPTION: Demonstrates how to read a value from Streamlit's `st.session_state` after it has been initialized. This snippet shows that once a variable is stored in session state, its value can be retrieved and used in subsequent app reruns.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nif 'key' not in st.session_state:\n    st.session_state['key'] = 'value'\n```\n\n----------------------------------------\n\nTITLE: Manage Streamlit Session State\nDESCRIPTION: Illustrates how to use `st.session_state` to store and share variables across reruns for a specific user session. This is crucial for maintaining stateful interactions and preserving user input in a Streamlit app.\nSOURCE: https://docs.streamlit.io/en/0.76.0/api\n\nLANGUAGE: python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Manage Session State in Streamlit\nDESCRIPTION: Shows how to use `st.session_state` to share variables between reruns for each user session. This mechanism enables persistent data across user interactions, allowing applications to maintain stateful information.\nSOURCE: https://docs.streamlit.io/develop/api-reference\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Manage Streamlit Session State Variables\nDESCRIPTION: Shows how to use `st.session_state` to store and share variables across reruns for each user session. This is crucial for building interactive and stateful Streamlit applications.\nSOURCE: https://docs.streamlit.io/en/0.74.0/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Initialize Streamlit Session State (Dictionary Syntax)\nDESCRIPTION: Illustrates how to initialize a key-value pair in Streamlit's `st.session_state` using dictionary-like syntax. This ensures the 'value' associated with 'key' persists across multiple reruns of the Streamlit app for a given user session.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\n# Check if 'key' already exists in session_state\n# If not, then initialize it\nif 'key' not in st.session_state:\n    st.session_state['key'] = 'value'\n```\n\n----------------------------------------\n\nTITLE: Initialize Streamlit Session State (Attribute Syntax)\nDESCRIPTION: Shows an alternative, more concise way to initialize a key-value pair in Streamlit's `st.session_state` using attribute-based syntax. This method achieves the same persistence as the dictionary syntax, making state management more readable.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\n# Session State also supports the attribute based syntax\nif 'key' not in st.session_state:\n    st.session_state.key = 'value'\n```\n\n----------------------------------------\n\nTITLE: Display a file uploader widget in Streamlit\nDESCRIPTION: Display a file uploader widget. This enables users to upload files from their local system to the Streamlit application. It supports various file types and can handle multiple uploads.\nSOURCE: https://docs.streamlit.io/en/0.69.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Manage Chat History in Session State\nDESCRIPTION: Demonstrates how to initialize chat history in Streamlit's Session State and iterate through it to render messages. It includes logic to retrieve and display user feedback for assistant messages, preparing for the interactive feedback widget.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/chat-response-feedback\n\nLANGUAGE: python\nCODE:\n```\nif \"history\" not in st.session_state:\n    st.session_state.history = []\n```\n\nLANGUAGE: python\nCODE:\n```\nfor i, message in enumerate(st.session_state.history):\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n        if message[\"role\"] == \"assistant\":\n            feedback = message.get(\"feedback\", None)\n            st.session_state[f\"feedback_{i}\"] = feedback\n            st.feedback(\n                \"thumbs\",\n                key=f\"feedback_{i}\",\n                disabled=feedback is not None,\n            )\n```\n\n----------------------------------------\n\nTITLE: Process Streamlit Form with st.rerun for UI Update\nDESCRIPTION: This example illustrates an alternative method for processing Streamlit form submissions, particularly when updates affect content displayed above the form. After the form is submitted, the script calculates the sum and stores it in `st.session_state`. If the form was submitted, `st.rerun()` is called to trigger a full script re-execution, which then updates the displayed sum. This approach can be less resource-efficient than using callbacks.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/forms\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nif 'sum' not in st.session_state:\nst.session_state.sum = ''\ncol1,col2 = st.columns(2)\ncol1.title('Sum:')\nif isinstance(st.session_state.sum, float):\ncol2.title(f'{st.session_state.sum:.2f}')\nwith st.form('addition'):\na = st.number_input('a')\nb = st.number_input('b')\nsubmit = st.form_submit_button('add')\n# The value of st.session_state.sum is updated at the end of the script rerun,\n# so the displayed value at the top in col2 does not show the new sum. Trigger\n# a second rerun when the form is submitted to update the value above.\nst.session_state.sum = a + b\nif submit:\nst.rerun()\n```\n\n----------------------------------------\n\nTITLE: Implementing Streamlit Form with Callback Function for Attendance Tracking\nDESCRIPTION: This comprehensive Python snippet demonstrates how to use a Streamlit form (`st.form`) in conjunction with a callback function (`on_click`) to manage user input and update `st.session_state`. It implements a simple attendance tracking system, showing how to check for existing entries and add new ones, while also illustrating the use of `st.text_input` and `st.form_submit_button` within a form context.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/widget-behavior\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nif \"attendance\" not in st.session_state:\nst.session_state.attendance = set()\ndef take_attendance():\nif st.session_state.name in st.session_state.attendance:\nst.info(f\"{st.session_state.name} has already been counted.\")\nelse:\nst.session_state.attendance.add(st.session_state.name)\nwith st.form(key=\"my_form\"):\nst.text_input(\"Name\", key=\"name\")\nst.form_submit_button(\"I'm here!\", on_click=take_attendance)\n```\n\n----------------------------------------\n\nTITLE: Display a File Uploader Widget in Streamlit\nDESCRIPTION: Display a file uploader widget, allowing users to upload files from their local system. The uploaded file's data is returned.\nSOURCE: https://docs.streamlit.io/en/0.70.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Display a File Uploader Widget in Streamlit\nDESCRIPTION: Display a file uploader widget using `st.file_uploader`. This allows users to upload files from their local system, such as CSV files.\nSOURCE: https://docs.streamlit.io/en/0.71.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: File uploader\nDESCRIPTION: Display a file uploader widget. This widget enables users to upload files from their local system to the Streamlit application.\nSOURCE: https://docs.streamlit.io/develop/api-reference\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Streamlit Session State Lag with Direct Button Updates\nDESCRIPTION: This example demonstrates a common issue when directly modifying `st.session_state` with buttons. The display of `st.session_state.name` before the buttons lags behind the actual state after the buttons are clicked, as the script reruns and updates the state only after the button's conditional code executes.\nSOURCE: https://docs.streamlit.io/develop/concepts/design/buttons\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nimport pandas as pd\nif 'name' not in st.session_state:\nst.session_state['name'] = 'John Doe'\nst.header(st.session_state['name'])\nif st.button('Jane'):\nst.session_state['name'] = 'Jane Doe'\nif st.button('John'):\nst.session_state['name'] = 'John Doe'\nst.header(st.session_state['name'])\n```\n\n----------------------------------------\n\nTITLE: Display a file uploader widget in Streamlit\nDESCRIPTION: Display a file uploader widget using `st.file_uploader`. Users can upload files, such as CSVs, to the application.\nSOURCE: https://docs.streamlit.io/en/0.65.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Processing Streamlit Form Submission After Form Definition\nDESCRIPTION: This snippet demonstrates a common pattern for processing form submissions in Streamlit. It defines a simple addition form with two number inputs and a submit button. The calculation and display of the result are conditioned on the form's submission status, occurring after the form definition. It also uses `st.columns` to control layout.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/forms\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\ncol1,col2 = st.columns([1,2])\ncol1.title('Sum:')\nwith st.form('addition'):\n    a = st.number_input('a')\n    b = st.number_input('b')\n    submit = st.form_submit_button('add')\nif submit:\n    col2.title(f'{a+b:.2f}')\n```\n\n----------------------------------------\n\nTITLE: Display a File Uploader Widget in Streamlit\nDESCRIPTION: Display a file uploader widget. This component enables users to upload files from their local system to the Streamlit application.\nSOURCE: https://docs.streamlit.io/en/0.75.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Implement Streamlit File Uploader\nDESCRIPTION: Display a file uploader widget. This enables users to upload files from their local system to the Streamlit application for processing.\nSOURCE: https://docs.streamlit.io/en/0.74.0/api\n\nLANGUAGE: python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Handle No Errors: Create Columns for Final Action Buttons\nDESCRIPTION: Initiates an `else` block that executes when no errors remain in the `validation_list`. It creates two columns to display final action buttons like 'Accept' and 'Re-validate', indicating the user has resolved all identified issues.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nelse:\ncols = st.columns(2)\n```\n\n----------------------------------------\n\nTITLE: Streamlit File Uploader Widget\nDESCRIPTION: Display a file uploader widget, allowing users to upload files from their local system to the Streamlit application.\nSOURCE: https://docs.streamlit.io/develop/api-reference/widgets\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Create a File Uploader Widget in Streamlit\nDESCRIPTION: Shows how to display a file uploader widget in Streamlit, enabling users to upload files like CSVs to the application.\nSOURCE: https://docs.streamlit.io/en/0.72.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Display a file uploader widget in Streamlit\nDESCRIPTION: Display a file uploader widget using `st.file_uploader`. Users can upload files from their local system.\nSOURCE: https://docs.streamlit.io/en/0.76.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Create a File Uploader Widget in Streamlit\nDESCRIPTION: Display a file uploader widget using `st.file_uploader`. This enables users to upload files from their local system to the Streamlit application for processing.\nSOURCE: https://docs.streamlit.io/en/0.68.0/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Retrieve Validation Data from Streamlit Session State\nDESCRIPTION: Accesses and assigns the 'sentences' (list of text segments) and 'valid' (list of boolean validation statuses) from the `st.session_state.validation` dictionary to local variables for easier manipulation.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nresponse_sentences = st.session_state.validation[\"sentences\"]\nvalidation_list = st.session_state.validation[\"valid\"]\n```\n\n----------------------------------------\n\nTITLE: Set Fallback Value for Error Focus Index\nDESCRIPTION: Establishes a default `None` value for the `focus` variable when no errors are present in the `validation_list`, ensuring proper flow when all issues are resolved and no specific error needs highlighting.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nelse:\nfocus = None\n```\n\n----------------------------------------\n\nTITLE: Incorrect Streamlit Widget State Modification After Render\nDESCRIPTION: This snippet illustrates an incorrect way to modify a widget's state using buttons. Attempting to change `st.session_state` for a widget (like `st.text_input`) after it has already been rendered in the current script run will lead to an error. This highlights a crucial limitation in Streamlit's state management.\nSOURCE: https://docs.streamlit.io/develop/concepts/design/buttons\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nst.text_input('Name', key='name')\n# These buttons will error because their nested code changes\n# a widget's state after that widget within the script.\nif st.button('Clear name'):\nst.session_state.name = ''\nif st.button('Streamlit!'):\nst.session_state.name = ('Streamlit')\n```\n\n----------------------------------------\n\nTITLE: Implement a Streamlit File Uploader Widget\nDESCRIPTION: This code snippet illustrates how to display a file uploader widget in Streamlit. It enables users to upload various file types, such as CSVs or images, to the application.\nSOURCE: https://docs.streamlit.io/en/latest/api\n\nLANGUAGE: Python\nCODE:\n```\ndata = st.file_uploader(\"Upload a CSV\")\n```\n\n----------------------------------------\n\nTITLE: Streamlit Session State Usage\nDESCRIPTION: Illustrates how to use Streamlit's session state to store and share variables across reruns for each user session. This is crucial for maintaining stateful interactions in a web application.\nSOURCE: https://docs.streamlit.io/en/0.72.0/api\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state['key'] = value\n```\n\n----------------------------------------\n\nTITLE: Update Sentence and Validation Status in Session State\nDESCRIPTION: Within the 'Update' button's conditional block, this code modifies the specific sentence and its validation status in `st.session_state.validation`. The corrected sentence is updated, and its validation status is set to `True`.\nSOURCE: https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/validate-and-edit-chat-responses\n\nLANGUAGE: Python\nCODE:\n```\nst.session_state.validation[\"sentences\"][focus] = (\nnew_sentence.strip(\". \") + \".\"\n)\nst.session_state.validation[\"valid\"][focus] = True\n```\n\n----------------------------------------\n\nTITLE: Basic Streamlit Counter App (Problematic)\nDESCRIPTION: This Streamlit example demonstrates a basic counter where the `count` variable is re-initialized to 0 on every app rerun, causing the counter to always display 1. It highlights the problem Session State solves by showing how variables reset without persistence.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/session-state\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\nst.title('Counter Example')\ncount = 0\nincrement = st.button('Increment')\nif increment:\n    count += 1\nst.write('Count = ', count)\n```\n\n----------------------------------------\n\nTITLE: Streamlit Client Error and UI Configuration\nDESCRIPTION: Defines how Streamlit handles and displays application exceptions, deprecation warnings, and controls the visibility of developer tools in the client-side UI.\nSOURCE: https://docs.streamlit.io/develop/api-reference/configuration/config\n\nLANGUAGE: TOML\nCODE:\n```\n[client]\n# Controls whether uncaught app exceptions and deprecation warnings\n# are displayed in the browser. This can be one of the following:\n# - \"full\" : In the browser, Streamlit displays app deprecation\n# warnings and exceptions, including exception types,\n# exception messages, and associated tracebacks.\n# - \"stacktrace\" : In the browser, Streamlit displays exceptions,\n# including exception types, generic exception messages,\n# and associated tracebacks. Deprecation warnings and\n# full exception messages will only print to the\n# console.\n# - \"type\" : In the browser, Streamlit displays exception types and\n# generic exception messages. Deprecation warnings, full\n# exception messages, and associated tracebacks only\n# print to the console.\n# - \"none\" : In the browser, Streamlit displays generic exception\n# messages. Deprecation warnings, full\n# exception messages, associated tracebacks, and exception types\n# will only print to the console.\n# - True : This is deprecated. Streamlit displays \"full\"\n# error details.\n# - False : This is deprecated. Streamlit displays \"stacktrace\"\n# error details.\n# Default: \"full\"\nshowErrorDetails = \"full\"\n# Change the visibility of items in the toolbar, options menu,\n# and settings dialog (top right of the app).\n# Allowed values:\n# - \"auto\" : Show the developer options if the app is accessed through\n# localhost or through Streamlit Community Cloud as a developer.\n# Hide them otherwise.\n# - \"developer\" : Show the developer options.\n# - \"viewer\" : Hide the developer options.\n# - \"minimal\" : Show only options set externally (e.g. through\n# Streamlit Community Cloud) or through st.set_page_config.\n# If there are no options left, hide the menu.\n# Default: \"auto\"\ntoolbarMode = \"auto\"\n# Controls whether to display the default sidebar page navigation in a\n# multi-page app. This only applies when app's pages are defined by the\n# `pages/` directory.\n# Default: true\nshowSidebarNavigation = true\n```\n\n----------------------------------------\n\nTITLE: Persisting Streamlit Widget Value with Session State (Single)\nDESCRIPTION: This Python example shows how to prevent a Streamlit widget's value from being lost when it's not continually rendered. It uses `st.session_state` to copy the widget's value from a temporary key (e.g., `_my_key`) to a permanent key (`my_key`) on change, ensuring persistence across page navigations.\nSOURCE: https://docs.streamlit.io/develop/concepts/architecture/widget-behavior\n\nLANGUAGE: python\nCODE:\n```\nimport streamlit as st\ndef store_value():\n# Copy the value to the permanent key\nst.session_state[\"my_key\"] = st.session_state[\"_my_key\"]\n# Copy the saved value to the temporary key\nst.session_state[\"_my_key\"] = st.session_state[\"my_key\"]\nst.number_input(\"Number of filters\", key=\"_my_key\", on_change=store_value)\n```\n\n----------------------------------------\n\nTITLE: Streamlit Runner Behavior Configuration\nDESCRIPTION: Configures core application runner behaviors such as magic commands, immediate reruns, session state serialization enforcement, and Enum coercion for widgets.\nSOURCE: https://docs.streamlit.io/develop/api-reference/configuration/config\n\nLANGUAGE: TOML\nCODE:\n```\n[runner]\n# Allows you to type a variable or string by itself in a single line of\n# Python code to write it to the app.\n# Default: true\nmagicEnabled = true\n# Handle script rerun requests immediately, rather than waiting for\n# script execution to reach a yield point.\n# This makes Streamlit much more responsive to user interaction, but it\n# can lead to race conditions in apps that mutate session_state data\n# outside of explicit session_state assignment statements.\n# Default: true\nfastReruns = true\n# Raise an exception after adding unserializable data to Session State.\n# Some execution environments may require serializing all data in Session\n# State, so it may be useful to detect incompatibility during development,\n# or when the execution environment will stop supporting it in the future.\n# Default: false\nenforceSerializableSessionState = false\n# Adjust how certain 'options' widgets like radio, selectbox, and\n# multiselect coerce Enum members.\n# This is useful when the Enum class gets re-defined during a script\n# re-run. For more information, check out the docs:\n# https://docs.streamlit.io/develop/concepts/design/custom-classes#enums\n# Allowed values:\n# - \"off\" : Disables Enum coercion.\n# - \"nameOnly\" : Enum classes can be coerced if their member names match.\n# - \"nameAndValue\" : Enum classes can be coerced if their member names AND\n# member values match.\n# Default: \"nameOnly\"\nenumCoercion = \"nameOnly\"\n```\n\n----------------------------------------\n\nTITLE: Streamlit Widget Object Comparison Pitfall with Session State\nDESCRIPTION: This pathological example demonstrates a common pitfall in Streamlit where objects stored in `st.session_state` and then used in widgets like `st.selectbox` can lead to unexpected comparison results. This occurs because class definitions are re-evaluated on each rerun, potentially comparing an instance from a previous run's class definition with an instance from the current run's definition.\nSOURCE: https://docs.streamlit.io/develop/concepts/design/custom-classes\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nfrom dataclasses import dataclass\n@dataclass\nclass Student:\n    student_id: int\n    name: str\nMarshall_A = Student(1, \"Marshall\")\nif \"B\" not in st.session_state:\n    st.session_state.B = Student(1, \"Marshall\")\nMarshall_B = st.session_state.B\noptions = [Marshall_A,Marshall_B]\nselected = st.selectbox(\"Pick\", options)\n# This comparison does not return expected results:\nselected == Marshall_A\n```\n\n----------------------------------------\n\nTITLE: Streamlit Main Application File (app.py)\nDESCRIPTION: This `app.py` file serves as the main entry point and pseudo-login page for the Streamlit application. It initializes `st.session_state.role`, allows users to select a role via a `st.selectbox`, and uses a callback to persist the role in session state, finally rendering the dynamic menu.\nSOURCE: https://docs.streamlit.io/develop/tutorials/multipage/st\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nfrom menu import menu\n# Initialize st.session_state.role to None\nif \"role\" not in st.session_state:\n    st.session_state.role = None\n# Retrieve the role from Session State to initialize the widget\nst.session_state._role = st.session_state.role\ndef set_role():\n    # Callback function to save the role selection to Session State\n    st.session_state.role = st.session_state._role\n# Selectbox to choose role\nst.selectbox(\n    \"Select your role:\",\n    [None, \"user\", \"admin\", \"super-admin\"],\n    key=\"_role\",\n    on_change=set_role,\n)\nmenu() # Render the dynamic menu!\n```\n\n----------------------------------------\n\nTITLE: Streamlit Session State Counter Example\nDESCRIPTION: This Streamlit application demonstrates how to use `st.session_state` to maintain a counter across script reruns. It initializes a 'counter' key if not present and increments it on each run, displaying the current count. A button triggers a rerun, showcasing state persistence.\nSOURCE: https://docs.streamlit.io/get-started/fundamentals/advanced-concepts\n\nLANGUAGE: Python\nCODE:\n```\nimport streamlit as st\nif \"counter\" not in st.session_state:\n    st.session_state.counter = 0\nst.session_state.counter += 1\nst.header(f\"This page has run {st.session_state.counter} times.\")\nst.button(\"Run it again\")\n```"}],"isError":null}

Now I'll search for Pydantic documentation to help with data validation and model best practices.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 resolve-library-id [object Object]
 {"content":[{"type":"text","text":"Available Libraries (top matches):\n\nEach result includes:\n- Library ID: Context7-compatible identifier (format: /org/project)\n- Name: Library or package name\n- Description: Short summary\n- Code Snippets: Number of available code examples\n- Trust Score: Authority indicator\n- Versions: List of versions if available. Use one of those versions if and only if the user explicitly provides a version in their query.\n\nFor best results, select libraries based on name match, trust score, snippet coverage, and relevance to your use case.\n\n----------\n\n- Title: Pydantic\n- Context7-compatible library ID: /pydantic/pydantic\n- Description: Data validation using Python type hints\n- Code Snippets: 681\n- Trust Score: 9.6\n----------\n- Title: Pydantic AI\n- Context7-compatible library ID: /pydantic/pydantic-ai\n- Description: Agent Framework / shim to use Pydantic with LLMs\n- Code Snippets: 354\n- Trust Score: 9.6\n----------\n- Title: Pydantic RDF\n- Context7-compatible library ID: /omegaice/pydantic-rdf\n- Description: A bridge between Pydantic V2 models and RDF graphs\n- Code Snippets: 24\n- Trust Score: 7.2\n----------\n- Title: Pydantic AI\n- Context7-compatible library ID: /context7/ai_pydantic_dev\n- Description: PydanticAI is a Python agent framework that brings the ergonomic design of FastAPI to Generative AI application development. It leverages Pydantic for type-safe, structured responses and offers model-agnostic support for building robust LLM-powered applications.\n- Code Snippets: 1554\n- Trust Score: 10\n----------\n- Title: Pydantic Kedro\n- Context7-compatible library ID: /nowanilfideme/pydantic-kedro\n- Description: Integration of Pydantic with Kedro.\n- Code Snippets: 13\n- Trust Score: 8.5\n----------\n- Title: Pydantic Settings\n- Context7-compatible library ID: /pydantic/pydantic-settings\n- Description: Settings management using pydantic\n- Code Snippets: 71\n- Trust Score: 9.6\n----------\n- Title: Pydantic SQLAlchemy\n- Context7-compatible library ID: /tiangolo/pydantic-sqlalchemy\n- Description: Tools to convert SQLAlchemy models to Pydantic models\n- Code Snippets: 3\n- Trust Score: 9.9\n----------\n- Title: Pydantic AI GitHub Agent\n- Context7-compatible library ID: /coleam00/pydantic-ai-github-agent\n- Description: The agent I built for my AI Agents series on YouTube - breaking down my entire process of building AI agents from idea to production!\n- Code Snippets: 9\n- Trust Score: 8.5\n----------\n- Title: Pydantic to TypeScript\n- Context7-compatible library ID: /phillipdupuis/pydantic-to-typescript\n- Description: CLI Tool for converting pydantic models into typescript definitions\n- Code Snippets: 7\n- Trust Score: 6.7\n----------\n- Title: Logfire\n- Context7-compatible library ID: /pydantic/logfire\n- Description: Uncomplicated Observability for Python and beyond! 🪵🔥\n- Code Snippets: 298\n- Trust Score: 9.6\n----------\n- Title: OpenAPI Pydantic\n- Context7-compatible library ID: /mike-oakley/openapi-pydantic\n- Description: Modern, type-safe OpenAPI schemas in Python using Pydantic 1.8+ and 2.x\n- Code Snippets: 11\n- Trust Score: 5.7\n----------\n- Title: Django Pydantic Field\n- Context7-compatible library ID: /surenkov/django-pydantic-field\n- Description: Django JSONField with Pydantic models as a Schema\n- Code Snippets: 9\n- Trust Score: 8.3\n----------\n- Title: Json Schema To Pydantic\n- Context7-compatible library ID: /richard-gyiko/json-schema-to-pydantic\n- Description: \n- Code Snippets: 13\n- Trust Score: 6.2"}],"isError":null}

I'll get Pydantic documentation to help with data validation and model best practices.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 get-library-docs [object Object]
 {"content":[{"type":"text","text":"TITLE: Demonstrating Pydantic ValidationError Handling\nDESCRIPTION: This Python code defines a Pydantic `Model` with various field types and custom validation. It then attempts to instantiate the model with invalid data, catches the `ValidationError`, and demonstrates printing the error object directly and accessing detailed error information using `e.errors()`.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/errors.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\n\n\nclass Location(BaseModel):\n    lat: float = 0.1\n    lng: float = 10.1\n\n\nclass Model(BaseModel):\n    is_required: float\n    gt_int: int = Field(gt=42)\n    list_of_ints: list[int]\n    a_float: float\n    recursive_model: Location\n\n    @field_validator('a_float', mode='after')\n    @classmethod\n    def validate_float(cls, value: float) -> float:\n        if value > 2.0:\n            raise ValueError('Invalid float value')\n        return value\n\n\ndata = {\n    'list_of_ints': ['1', 2, 'bad'],\n    'a_float': 3.0,\n    'recursive_model': {'lat': 4.2, 'lng': 'New York'},\n    'gt_int': 21,\n}\n\ntry:\n    Model(**data)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    5 validation errors for Model\n    is_required\n      Field required [type=missing, input_value={'list_of_ints': ['1', 2,...ew York'}, 'gt_int': 21}, input_type=dict]\n    gt_int\n      Input should be greater than 42 [type=greater_than, input_value=21, input_type=int]\n    list_of_ints.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bad', input_type=str]\n    a_float\n      Value error, Invalid float value [type=value_error, input_value=3.0, input_type=float]\n    recursive_model.lng\n      Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='New York', input_type=str]\n    \"\"\"\n\ntry:\n    Model(**data)\nexcept ValidationError as e:\n    print(e.errors())\n    \"\"\"\n    [\n        {\n            'type': 'missing',\n            'loc': ('is_required',),\n            'msg': 'Field required',\n            'input': {\n                'list_of_ints': ['1', 2, 'bad'],\n                'a_float': 3.0,\n                'recursive_model': {'lat': 4.2, 'lng': 'New York'},\n                'gt_int': 21,\n            },\n\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `model_type` Validation Error\nDESCRIPTION: This error is raised when the input to a Pydantic model is not an instance of the model itself or a dictionary. The example illustrates successful validation from a dictionary and an existing model instance, then shows how a string input leads to a 'model_type' error.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    a: int\n    b: int\n\n\n# simply validating a dict\nm = Model.model_validate({'a': 1, 'b': 2})\nprint(m)\n#> a=1 b=2\n\n# validating an existing model instance\nprint(Model.model_validate(m))\n#> a=1 b=2\n\ntry:\n    Model.model_validate('not an object')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'model_type'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic Validation Errors with ValidationError\nDESCRIPTION: This snippet demonstrates how Pydantic raises a `ValidationError` exception when input data fails validation against a `BaseModel` schema. It defines a simple model with a list of integers and a float, then attempts to validate invalid input. The `try-except` block catches the `ValidationError` and prints its detailed message, which includes information about all detected errors, their types, and input values.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    list_of_ints: list[int]\n    a_float: float\n\n\ndata = dict(\n    list_of_ints=['1', 2, 'bad'],\n    a_float='not a float',\n)\n\ntry:\n    Model(**data)\nexce\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    list_of_ints.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bad', input_type=str]\n    a_float\n      Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='not a float', input_type=str]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `missing` Validation Error\nDESCRIPTION: This error is raised when required fields are missing from the input value during Pydantic model validation. The example shows a `Model` with a required `x` field and how `Model()` without `x` raises a `ValidationError` with type 'missing'.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: str\n\n\ntry:\n    Model()\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'missing'\n```\n\n----------------------------------------\n\nTITLE: Pydantic Model Validation Error Handling\nDESCRIPTION: This snippet illustrates how Pydantic raises a `ValidationError` when input data does not conform to the model's schema, providing a detailed list of errors. It demonstrates how to catch and inspect validation errors, showing the specific error types, locations, messages, and problematic input values for debugging.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/index.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# continuing the above example...\n\nfrom datetime import datetime\nfrom pydantic import BaseModel, PositiveInt, ValidationError\n\n\nclass User(BaseModel):\n    id: int\n    name: str = 'John Doe'\n    signup_ts: datetime | None\n    tastes: dict[str, PositiveInt]\n\n\nexternal_data = {'id': 'not an int', 'tastes': {}}\n\ntry:\n    User(**external_data)\nexcept ValidationError as e:\n    print(e.errors())\n    \"\"\"\n    [\n        {\n            'type': 'int_parsing',\n            'loc': ('id',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not an int',\n            'url': 'https://errors.pydantic.dev/2/v/int_parsing',\n        },\n        {\n            'type': 'missing',\n            'loc': ('signup_ts',),\n            'msg': 'Field required',\n            'input': {'id': 'not an int', 'tastes': {}},\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        },\n    ]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Validate Data with Pydantic Model Methods\nDESCRIPTION: This snippet demonstrates how to use Pydantic's `model_validate`, `model_validate_json`, and `model_validate_strings` methods to parse and validate data. It includes examples of successful validation and error handling for invalid inputs, showcasing how different input types are processed and how `ValidationError` is raised.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass User(BaseModel):\n    id: int\n    name: str = 'John Doe'\n    signup_ts: Optional[datetime] = None\n\n\nm = User.model_validate({'id': 123, 'name': 'James'})\nprint(m)\n# id=123 name='James' signup_ts=None\n\ntry:\n    User.model_validate(['not', 'a', 'dict'])\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n      Input should be a valid dictionary or instance of User [type=model_type, input_value=['not', 'a', 'dict'], input_type=list]\n    \"\"\"\n\nm = User.model_validate_json('{\"id\": 123, \"name\": \"James\"}')\nprint(m)\n# id=123 name='James' signup_ts=None\n\ntry:\n    m = User.model_validate_json('{\"id\": 123, \"name\": 123}')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    \"\"\"\n\ntry:\n    m = User.model_validate_json('invalid JSON')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n      Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='invalid JSON', input_type=str]\n    \"\"\"\n\nm = User.model_validate_strings({'id': '123', 'name': 'James'})\nprint(m)\n# id=123 name='James' signup_ts=None\n\nm = User.model_validate_strings(\n    {'id': '123', 'name': 'James', 'signup_ts': '2024-04-01T12:00:00'}\n)\nprint(m)\n# id=123 name='James' signup_ts=datetime.datetime(2024, 4, 1, 12, 0)\n\ntry:\n    m = User.model_validate_strings(\n        {'id': '123', 'name': 'James', 'signup_ts': '2024-04-01'}, strict=True\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    signup_ts\n      Input should be a valid datetime, invalid datetime separator, expected `T`, `t`, `_` or space [type=datetime_parsing, input_value='2024-04-01', input_type=str]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `recursion_loop` Error\nDESCRIPTION: This error is raised by Pydantic when a cyclic reference is detected within a model's data structure, preventing infinite recursion during validation. The example shows a self-referencing list that triggers this error.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: list['Model']\n\n\nd = {'x': []}\nd['x'].append(d)\ntry:\n    Model(**d)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'recursion_loop'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `model_attributes_type` Validation Error\nDESCRIPTION: This error is raised when the input value is not a valid dictionary, model instance, or an instance from which fields can be extracted using `from_attributes=True`. The example shows successful validation from a dictionary and a custom object, then demonstrates how a non-object string triggers the 'model_attributes_type' error.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    a: int\n    b: int\n\n\n# simply validating a dict\nprint(Model.model_validate({'a': 1, 'b': 2}))\n#> a=1 b=2\n\n\nclass CustomObj:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n\n# using from attributes to extract fields from an objects\nprint(Model.model_validate(CustomObj(3, 4), from_attributes=True))\n#> a=3 b=4\n\ntry:\n    Model.model_validate('not an object', from_attributes=True)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'model_attributes_type'\n```\n\n----------------------------------------\n\nTITLE: Handling and Suppressing Pydantic Cyclic Validation Errors\nDESCRIPTION: Provides a robust method to handle and optionally suppress `ValidationError` caused by cyclic references in Pydantic models. It includes a utility function to check for `recursion_loop` errors and a `field_validator` that demonstrates how to drop cyclic references during validation, allowing partial validation of recursive structures.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/forward_annotations.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom contextlib import contextmanager\nfrom dataclasses import field\nfrom typing import Iterator\n\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\ndef is_recursion_validation_error(exc: ValidationError) -> bool:\n    errors = exc.errors()\n    return len(errors) == 1 and errors[0]['type'] == 'recursion_loop'\n\n\n@contextmanager\ndef suppress_recursion_validation_error() -> Iterator[None]:\n    try:\n        yield\n    except ValidationError as exc:\n        if not is_recursion_validation_error(exc):\n            raise exc\n\n\nclass Node(BaseModel):\n    id: int\n    children: list['Node'] = field(default_factory=list)\n\n    @field_validator('children', mode='wrap')\n    @classmethod\n    def drop_cyclic_references(cls, children, h):\n        try:\n            return h(children)\n        except ValidationError as exc:\n            if not (\n                is_recursion_validation_error(exc)\n                and isinstance(children, list)\n            ):\n                raise exc\n\n            value_without_cyclic_refs = []\n            for child in children:\n                with suppress_recursion_validation_error():\n                    value_without_cyclic_refs.extend(h([child]))\n            return h(value_without_cyclic_refs)\n\n\n# Create data with cyclic references representing the graph 1 -> 2 -> 3 -> 1\nnode_data = {'id': 1, 'children': [{'id': 2, 'children': [{'id': 3}]}]}\nnode_data['children'][0]['children'][0]['children'] = [node_data]\n\nprint(Node.model_validate(node_data))\n#> id=1 children=[Node(id=2, children=[Node(id=3, children=[])])]\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `frozen_instance` Validation Error\nDESCRIPTION: This error is raised when `model_config['frozen'] == True` and you attempt to delete or assign a new value to any of the fields. This configuration makes the entire model instance immutable after creation.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nm = Model(x=1)\n\ntry:\n    m.x = 2\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_instance'\n\ntry:\n    del m.x\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_instance'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `multiple_of` Validation Error\nDESCRIPTION: This error is raised when the input value for a field is not a multiple of its defined `multiple_of` constraint. The example defines a `Model` where `x` must be a multiple of 5, and attempting to initialize it with `x=1` triggers a 'multiple_of' validation error.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(multiple_of=5)\n\n\ntry:\n    Model(x=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'multiple_of'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic ValidationError for Model Field Assignment\nDESCRIPTION: This snippet demonstrates how Pydantic raises a `ValidationError` when an invalid value is assigned to a model field. It shows a `try-except` block to catch the error and prints the detailed validation message, illustrating the type and input value issues.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n        m.a = 'not an int'\n\n        try:\n            m2 = Model.model_validate(m)\n        except ValidationError as e:\n            print(e)\n            \"\"\"\n            1 validation error for Model\n            a\n              Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='not an int', input_type=str]\n            \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define Generic Pydantic Model (Python 3.9+)\nDESCRIPTION: This example demonstrates how to create a generic Pydantic model using `typing.Generic` and `TypeVar` for Python 3.9 and later. It shows how to define a `Response` model that can wrap any data type, and includes examples of instantiation with `int`, `str`, and another `BaseModel`, along with validation error handling.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Generic, TypeVar\n\nfrom pydantic import BaseModel, ValidationError\n\nDataT = TypeVar('DataT')\n\n\nclass DataModel(BaseModel):\n    number: int\n\n\nclass Response(BaseModel, Generic[DataT]):\n    data: DataT\n\n\nprint(Response[int](data=1))\n#> data=1\nprint(Response[str](data='value'))\n#> data='value'\nprint(Response[str](data='value').model_dump())\n#> {'data': 'value'}\n\ndata = DataModel(number=1)\nprint(Response[DataModel](data=data).model_dump())\n#> {'data': {'number': 1}}\ntry:\n    Response[int](data='value')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response[int]\n    data\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='value', input_type=str]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic value_error from Custom Validator\nDESCRIPTION: This error is raised when a `ValueError` is explicitly thrown within a Pydantic field validator or a custom model validator. Pydantic catches these standard Python errors and converts them into its own validation error type. The example demonstrates defining a custom `field_validator` that unconditionally raises a `ValueError`, showing how Pydantic processes this into a `value_error`.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_99\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\nclass Model(BaseModel):\n    x: str\n\n    @field_validator('x')\n    @classmethod\n    def repeat_b(cls, v):\n        raise ValueError()\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'value_error'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `callable_type` Validation Error\nDESCRIPTION: This error is raised when the input value is not valid as a `Callable`.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Callable\n\nfrom pydantic import BaseModel, ImportString, ValidationError\n\n\nclass Model(BaseModel):\n    x: ImportString[Callable[[Any], Any]]\n\n\nModel(x='math:cos')  # OK\n\ntry:\n    Model(x='os.path')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'callable_type'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic dataclass_type Validation Error\nDESCRIPTION: This error is raised when the input value is not valid for a `dataclass` field, meaning it cannot be coerced into the expected dataclass type.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import ValidationError, dataclasses\n\n\n@dataclasses.dataclass\nclass Inner:\n    x: int\n\n\n@dataclasses.dataclass\nclass Outer:\n    y: Inner\n\n\nOuter(y=Inner(x=1))  # OK\n\ntry:\n    Outer(y=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'dataclass_type'\n```\n\n----------------------------------------\n\nTITLE: Handling Errors in Last Element During Partial Validation\nDESCRIPTION: This Python example demonstrates how Pydantic's `experimental_allow_partial` feature, when used with `TypeAdapter`, automatically ignores validation errors that occur in the last element of an incomplete JSON input. This prevents the entire parsing process from failing due to partial data, allowing successful validation of the complete parts.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/experimental.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\n\nfrom annotated_types import MinLen\n\nfrom pydantic import BaseModel, TypeAdapter\n\n\nclass MyModel(BaseModel):\n    a: int\n    b: Annotated[str, MinLen(5)]\n\n\nta = TypeAdapter(list[MyModel])\nv = ta.validate_json(\n    '[{\"a\": 1, \"b\": \"12345\"}, {\"a\": 1,',\n    experimental_allow_partial=True,\n)\nprint(v)\n#> [MyModel(a=1, b='12345')]\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `mapping_type` Validation Error\nDESCRIPTION: This error is raised when a problem occurs during validation due to a failure in a call to the methods from the `Mapping` protocol, such as `.items()`. The example demonstrates how to trigger and catch this error when a custom `BadMapping` class fails during dictionary validation.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Mapping\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass BadMapping(Mapping):\n    def items(self):\n        raise ValueError()\n\n    def __iter__(self):\n        raise ValueError()\n\n    def __getitem__(self, key):\n        raise ValueError()\n\n    def __len__(self):\n        return 1\n\n\nclass Model(BaseModel):\n    x: dict[str, str]\n\n\ntry:\n    Model(x=BadMapping())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'mapping_type'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `bytes_too_short` Validation Error\nDESCRIPTION: This error is raised when the length of a `bytes` value is less than the field's `min_length` constraint.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: bytes = Field(min_length=3)\n\n\ntry:\n    Model(x=b't')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bytes_too_short'\n```\n\n----------------------------------------\n\nTITLE: Integrate Pydantic Models with ARQ for Asynchronous Job Processing\nDESCRIPTION: This comprehensive Python example demonstrates defining a Pydantic `User` model, enqueuing user data as jobs using ARQ, and processing/validating the data upon job execution. It covers `BaseModel` definition, `model_dump()` for serialization, and `model_validate()` for deserialization and validation within an asynchronous ARQ worker, showcasing a complete job lifecycle.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/examples/queues.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom typing import Any\n\nfrom arq import create_pool\nfrom arq.connections import RedisSettings\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\nREDIS_SETTINGS = RedisSettings()\n\n\nasync def process_user(ctx: dict[str, Any], user_data: dict[str, Any]) -> None:\n    user = User.model_validate(user_data)\n    print(f'Processing user: {repr(user)}')\n\n\nasync def enqueue_jobs(redis):\n    user1 = User(id=1, name='John Doe', email='john@example.com')\n    user2 = User(id=2, name='Jane Doe', email='jane@example.com')\n\n    await redis.enqueue_job('process_user', user1.model_dump())\n    print(f'Enqueued user: {repr(user1)}')\n\n    await redis.enqueue_job('process_user', user2.model_dump())\n    print(f'Enqueued user: {repr(user2)}')\n\n\nclass WorkerSettings:\n    functions = [process_user]\n    redis_settings = REDIS_SETTINGS\n\n\nasync def main():\n    redis = await create_pool(REDIS_SETTINGS)\n    await enqueue_jobs(redis)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Handle Missing Fields with Default Values in Partial JSON Parsing\nDESCRIPTION: Provides an advanced example demonstrating how to use a custom `WrapValidator` (`default_on_error`) to set default values for fields that are missing due to partial JSON parsing. This prevents `ValidationError` for missing fields, allowing successful model validation even with truncated input.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/json.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, Any, Optional\n\nimport pydantic_core\n\nfrom pydantic import BaseModel, ValidationError, WrapValidator\n\n\ndef default_on_error(v, handler) -> Any:\n    \"\"\"\n    Raise a PydanticUseDefault exception if the value is missing.\n\n    This is useful for avoiding errors from partial\n    JSON preventing successful validation.\n    \"\"\"\n    try:\n        return handler(v)\n    except ValidationError as exc:\n        # there might be other types of errors resulting from partial JSON parsing\n        # that you allow here, feel free to customize as needed\n        if all(e['type'] == 'missing' for e in exc.errors()):\n            raise pydantic_core.PydanticUseDefault()\n        else:\n            raise\n\n\nclass NestedModel(BaseModel):\n    x: int\n    y: str\n\n\nclass MyModel(BaseModel):\n    foo: Optional[str] = None\n    bar: Annotated[\n        Optional[tuple[str, int]], WrapValidator(default_on_error)\n    ] = None\n    nested: Annotated[\n        Optional[NestedModel], WrapValidator(default_on_error)\n    ] = None\n\n\nm = MyModel.model_validate(\n    pydantic_core.from_json('{\"foo\": \"x\", \"bar\": [\"world\",', allow_partial=True)\n)\nprint(repr(m))\n# > MyModel(foo='x', bar=None, nested=None)\n\n\nm = MyModel.model_validate(\n    pydantic_core.from_json(\n        '{\"foo\": \"x\", \"bar\": [\"world\", 1], \"nested\": {\"x\":', allow_partial=True\n    )\n)\nprint(repr(m))\n# > MyModel(foo='x', bar=('world', 1), nested=None)\n```\n\n----------------------------------------\n\nTITLE: Pydantic validate_call with Asynchronous Functions\nDESCRIPTION: Demonstrates that Pydantic's `validate_call` decorator can also be applied to asynchronous functions, ensuring validation works seamlessly with `async def` methods.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/validation_decorator.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Connection:\n    async def execute(self, sql, *args):\n        return 'testing@example.com'\n\n\nconn = Connection()\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic date_parsing Validation Error for JSON\nDESCRIPTION: This error is raised when validating JSON data where the input string for a `date` field cannot be parsed into a valid date, especially when `strict=True` is applied to the field.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom datetime import date\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: date = Field(strict=True)\n\n\ntry:\n    Model.model_validate_json(json.dumps({'x': '1'}))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_parsing'\n```\n\n----------------------------------------\n\nTITLE: Pydantic Generic Model Validation with `model_validator`\nDESCRIPTION: This snippet demonstrates the execution of a `model_validator` on a generic Pydantic model. It shows that the `after` validator runs even when the generic type parameter is `Any`, illustrating the validation flow for generic instances.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Generic, Self, TypeVar\n\nfrom pydantic import BaseModel, model_validator\n\nT = TypeVar('T')\n\n\nclass GenericModel(BaseModel, Generic[T]):\n    a: T\n\n    @model_validator(mode='after')\n    def validate_after(self: Self) -> Self:\n        print('after validator running custom validation...')\n        return self\n\n\nclass Model(BaseModel):\n    inner: GenericModel[Any]\n\n\nm = Model.model_validate(Model(inner=GenericModel[int](a=1)))\n#> after validator running custom validation...\n#> after validator running custom validation...\nprint(repr(m))\n#> Model(inner=GenericModel[Any](a=1))\n```\n\n----------------------------------------\n\nTITLE: Pydantic Cyclic Reference Detection in Validation\nDESCRIPTION: Demonstrates Pydantic's ability to detect and handle cyclic references during model validation, preventing `RecursionError` and instead raising a specific `ValidationError`. The example defines two mutually recursive models and shows how a cyclic data structure triggers the `recursion_loop` error.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/forward_annotations.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass ModelA(BaseModel):\n    b: 'Optional[ModelB]' = None\n\n\nclass ModelB(BaseModel):\n    a: Optional[ModelA] = None\n\n\ncyclic_data = {}\ncyclic_data['a'] = {'b': cyclic_data}\nprint(cyclic_data)\n#> {'a': {'b': {...}}}\n\ntry:\n    ModelB.model_validate(cyclic_data)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for ModelB\n    a.b\n      Recursion error - cyclic reference detected [type=recursion_loop, input_value={'a': {'b': {...}}}, input_type=dict]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Validation with Nested Parametrized Generic Models\nDESCRIPTION: Explores how Pydantic handles validation when using the same type variable across nested generic models, highlighting how type mismatches in nested fields lead to `ValidationError`.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Generic, TypeVar\n\nfrom pydantic import BaseModel, ValidationError\n\nT = TypeVar('T')\n\n\nclass InnerT(BaseModel, Generic[T]):\n    inner: T\n\n\nclass OuterT(BaseModel, Generic[T]):\n    outer: T\n    nested: InnerT[T]\n\n\nnested = InnerT[int](inner=1)\nprint(OuterT[int](outer=1, nested=nested))\ntry:\n    print(OuterT[int](outer='a', nested=InnerT(inner='a')))\nexcept ValidationError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Validate Pydantic Models with TypedDict and TypeAdapter\nDESCRIPTION: Demonstrates how to define Pydantic models using `TypedDict` for structured data, including optional fields (`total=False`) and strict extra field handling (`ConfigDict(extra='forbid')`). It shows how to use `TypeAdapter` for validation and illustrates common validation errors for incorrect types or forbidden extra fields.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/api/standard_library_types.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, ValidationError\n\n\n# `total=False` means keys are non-required\nclass UserIdentity(TypedDict, total=False):\n    name: Optional[str]\n    surname: str\n\n\nclass User(TypedDict):\n    __pydantic_config__ = ConfigDict(extra='forbid')\n\n    identity: UserIdentity\n    age: int\n\nta = TypeAdapter(User)\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n    )\n)\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n    )\n)\n\nprint(ta.validate_python({'identity': {}, 'age': 37}))\n\n\ntry:\n    ta.validate_python(\n        {'identity': {'name': ['Smith'], 'surname': 'John'}, 'age': 24}\n    )\nexcept ValidationError as e:\n    print(e)\n\ntry:\n    ta.validate_python(\n        {\n            'identity': {'name': 'Smith', 'surname': 'John'},\n            'age': '37',\n            'email': 'john.smith@me.com',\n        }\n    )\nexcept ValidationError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic datetime_future Validation Error\nDESCRIPTION: This error is raised when a `FutureDatetime` field receives an input value that is not in the future relative to the current date and time.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom pydantic import BaseModel, FutureDatetime, ValidationError\n\n\nclass Model(BaseModel):\n    x: FutureDatetime\n\n\ntry:\n    Model(x=datetime(2000, 1, 1))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_future'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `needs_python_object` Validation Error\nDESCRIPTION: This error is raised when validation is attempted from a format that cannot be converted to a Python object, such as when `isinstance` or `issubclass` checks are performed on JSON input. The example shows `Model.model_validate_json` failing when trying to validate a non-class string as a `type[BaseModel]`.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    bm: type[BaseModel]\n\n\ntry:\n    Model.model_validate_json(json.dumps({'bm': 'not a basemodel class'}))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'needs_python_object'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `time_delta_parsing` Error\nDESCRIPTION: This error occurs when a string input for a `timedelta` field cannot be successfully parsed into a timedelta object. The example demonstrates how to catch this `ValidationError` and inspect the error type.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_78\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: timedelta\n\n\ntry:\n    Model(x='t')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'time_delta_parsing'\n```\n\n----------------------------------------\n\nTITLE: Pydantic Before Model Validator Example\nDESCRIPTION: Illustrates a 'before' model validator in Pydantic, executed prior to model instantiation. This validator checks the raw input data for the presence of a 'card_number' field, raising a `ValueError` if found. It highlights the need to handle `Any` type input.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/validators.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\nfrom pydantic import BaseModel, model_validator\n\n\nclass UserModel(BaseModel):\n    username: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def check_card_number_not_present(cls, data: Any) -> Any:\n        if isinstance(data, dict):\n            if 'card_number' in data:\n                raise ValueError(\"'card_number' should not be included\")\n        return data\n```\n\n----------------------------------------\n\nTITLE: Define Generic Pydantic Model (Python 3.12+ New Syntax)\nDESCRIPTION: This example illustrates how to create a generic Pydantic model using the new type parameter syntax introduced in Python 3.12. It defines a `Response` model that can wrap various data types, showcasing instantiation with `int`, `str`, and another `BaseModel`, along with validation error handling, similar to the older syntax but with a more concise declaration.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\nclass DataModel(BaseModel):\n    number: int\n\n\nclass Response[DataT](BaseModel):\n    data: DataT\n\n\nprint(Response[int](data=1))\n#> data=1\nprint(Response[str](data='value'))\n#> data='value'\nprint(Response[str](data='value').model_dump())\n#> {'data': 'value'}\n\ndata = DataModel(number=1)\nprint(Response[DataModel](data=data).model_dump())\n#> {'data': {'number': 1}}\ntry:\n    Response[int](data='value')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response[int]\n    data\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='value', input_type=str]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define Nested Discriminated Unions in Pydantic\nDESCRIPTION: This example demonstrates creating hierarchical data models using Pydantic's nested discriminated unions. It defines `BlackCat`, `WhiteCat`, `Cat`, `Dog`, and `Pet` models, showcasing how to use `Annotated` and `Field(discriminator='...')` to validate complex structures and handle specific validation errors for missing fields or incorrect discriminator values.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/unions.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BlackCat(BaseModel):\n    pet_type: Literal['cat']\n    color: Literal['black']\n    black_name: str\n\n\nclass WhiteCat(BaseModel):\n    pet_type: Literal['cat']\n    color: Literal['white']\n    white_name: str\n\n\nCat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    name: str\n\n\nPet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n\nclass Model(BaseModel):\n    pet: Pet\n    n: int\n\n\nm = Model(pet={'pet_type': 'cat', 'color': 'black', 'black_name': 'felix'}, n=1)\nprint(m)\n# pet=BlackCat(pet_type='cat', color='black', black_name='felix') n=1\ntry:\n    Model(pet={'pet_type': 'cat', 'color': 'red'}, n='1')\nexcept ValidationError as e:\n    print(e)\n    # 1 validation error for Model\n    # pet.cat\n    #   Input tag 'red' found using 'color' does not match any of the expected tags: 'black', 'white' [type=union_tag_invalid, input_value={'pet_type': 'cat', 'color': 'red'}, input_type=dict]\ntry:\n    Model(pet={'pet_type': 'cat', 'color': 'black'}, n='1')\nexcept ValidationError as e:\n    print(e)\n    # 1 validation error for Model\n    # pet.cat.black.black_name\n    #   Field required [type=missing, input_value={'pet_type': 'cat', 'color': 'black'}, input_type=dict]\n```\n\n----------------------------------------\n\nTITLE: Pydantic Model Default Extra Data Handling\nDESCRIPTION: Demonstrates Pydantic's default behavior where extra fields passed to a BaseModel constructor are silently ignored and not included in the model's data.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    x: int\n\nm = Model(x=1, y='a')\nassert m.model_dump() == {'x': 1}\n```\n\n----------------------------------------\n\nTITLE: Validating Asynchronous Function Arguments with Pydantic `validate_call`\nDESCRIPTION: This example demonstrates how to apply the `@validate_call` decorator to an asynchronous function (`get_user_email`) to automatically validate its arguments using Pydantic types like `PositiveInt`. It also shows how to handle `ValidationError` exceptions for invalid inputs.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/validation_decorator.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom pydantic import PositiveInt, ValidationError, validate_call\n\n\n@validate_call\nasync def get_user_email(user_id: PositiveInt):\n    # `conn` is some fictional connection to a database\n    email = await conn.execute('select email from users where id=$1', user_id)\n    if email is None:\n        raise RuntimeError('user not found')\n    else:\n        return email\n\n\nasync def main():\n    email = await get_user_email(123)\n    print(email)\n    #> testing@example.com\n    try:\n        await get_user_email(-4)\n    except ValidationError as exc:\n        print(exc.errors())\n        \"\"\"\n        [\n            {\n                'type': 'greater_than',\n                'loc': (0,),\n                'msg': 'Input should be greater than 0',\n                'input': -4,\n                'ctx': {'gt': 0},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        \"\"\"\n\n\nasyncio.run(main())\n# requires: `conn.execute()` that will return `'testing@example.com'`\n```\n\n----------------------------------------\n\nTITLE: Validate and Serialize Data with TypeAdapter using TypedDict\nDESCRIPTION: This example demonstrates how to use `TypeAdapter` to validate and serialize a list of `TypedDict` objects. It shows successful validation, error handling for invalid input, and JSON serialization of the validated data.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/type_adapter.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\n\nfrom pydantic import TypeAdapter, ValidationError\n\n\nclass User(TypedDict):\n    name: str\n    id: int\n\n\nuser_list_adapter = TypeAdapter(list[User])\nuser_list = user_list_adapter.validate_python([{'name': 'Fred', 'id': '3'}])\nprint(repr(user_list))\n#> [{'name': 'Fred', 'id': 3}]\n\ntry:\n    user_list_adapter.validate_python(\n        [{'name': 'Fred', 'id': 'wrong', 'other': 'no'}]\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for list[User]\n    0.id\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='wrong', input_type=str]\n    \"\"\"\n\nprint(repr(user_list_adapter.dump_json(user_list)))\n#> b'[{\"name\":\"Fred\",\"id\":3}]'\n```\n\n----------------------------------------\n\nTITLE: Demonstrate `iteration_error` Pydantic Validation Error\nDESCRIPTION: This error is raised when an exception occurs during the iteration of a value intended for a collection field. The example uses a generator that raises a `RuntimeError` during iteration, and when this generator is passed to a Pydantic model expecting a list, it correctly raises an `iteration_error`.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\n\n\ndef gen():\n    yield 1\n    raise RuntimeError('error')\n\n\nclass Model(BaseModel):\n    x: list[int]\n\n\ntry:\n    Model(x=gen())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'iteration_error'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `extra_forbidden` Validation Error\nDESCRIPTION: This error is raised when the input value contains extra fields, but `model_config['extra'] == 'forbid'`. It indicates that the model configuration explicitly disallows additional fields not defined in the schema.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: str\n\n    model_config = ConfigDict(extra='forbid')\n\n\ntry:\n    Model(x='test', y='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'extra_forbidden'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic `get_attribute_error` Validation Error\nDESCRIPTION: This error is raised when `model_config['from_attributes'] == True` and an error occurs while Pydantic attempts to read attributes from the input object. This typically happens if an attribute or property raises an exception during access.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Foobar:\n    def __init__(self):\n        self.x = 1\n\n    @property\n    def y(self):\n        raise RuntimeError('intentional error')\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\ntry:\n    Model.model_validate(Foobar())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'get_attribute_error'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic date_future Validation Error\nDESCRIPTION: This error is raised when a `FutureDate` field receives an input value that is not in the future relative to the current date.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nfrom pydantic import BaseModel, FutureDate, ValidationError\n\n\nclass Model(BaseModel):\n    x: FutureDate\n\n\ntry:\n    Model(x=date(2000, 1, 1))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_future'\n```\n\n----------------------------------------\n\nTITLE: Handle Pydantic Validation Errors for Invalid JSON\nDESCRIPTION: This Python code demonstrates how Pydantic raises a `ValidationError` when attempting to validate invalid JSON input. It shows how to use a `try-except` block to catch the error and print the detailed validation issues reported by Pydantic.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/examples/files.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pathlib\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt, ValidationError\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\njson_string = pathlib.Path('person.json').read_text()\ntry:\n    person = Person.model_validate_json(json_string)\nexcept ValidationError as err:\n    print(err)\n    \"\"\"\n    3 validation errors for Person\n    name\n    Field required [type=missing, input_value={'age': -30, 'email': 'not-an-email-address'}, input_type=dict]\n        For further information visit https://errors.pydantic.dev/2.10/v/missing\n    age\n    Input should be greater than 0 [type=greater_than, input_value=-30, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.10/v/greater_than\n    email\n    value is not a valid email address: An email address must have an @-sign. [type=value_error, input_value='not-an-email-address', input_type=str]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Validation of Unparametrized Type Variables in Pydantic\nDESCRIPTION: This example illustrates how Pydantic validates generic models when type variables are left unparametrized. It demonstrates the behavior with `TypeVar` instances that are unbound, bound to a specific type, or have a default type, showcasing how `ValidationError` is raised for type mismatches.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/concepts/models.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Generic\n\nfrom typing_extensions import TypeVar\n\nfrom pydantic import BaseModel, ValidationError\n\nT = TypeVar('T')\nU = TypeVar('U', bound=int)\nV = TypeVar('V', default=str)\n\n\nclass Model(BaseModel, Generic[T, U, V]):\n    t: T\n    u: U\n    v: V\n\n\nprint(Model(t='t', u=1, v='v'))\n#> t='t' u=1 v='v'\n\ntry:\n    Model(t='t', u='u', v=1)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    2 validation errors for Model\n    u\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='u', input_type=str]\n    v\n      Input should be a valid string [type=string_type, input_value=1, input_type=int]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrate `iterable_type` Pydantic Validation Error\nDESCRIPTION: This error is raised when an input value is not valid for an `Iterable` field. The snippet shows a Pydantic model with an `Iterable` field and attempts to assign an integer (`123`) to it, which is not iterable, resulting in an `iterable_type` error.\nSOURCE: https://github.com/pydantic/pydantic/blob/main/docs/errors/validation_errors.md#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    y: Iterable\n\n\ntry:\n    Model(y=123)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'iterable_type'\n```"}],"isError":null}

Now I'll get LangGraph documentation to understand workflow orchestration patterns and error handling.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 resolve-library-id [object Object]
 {"content":[{"type":"text","text":"Available Libraries (top matches):\n\nEach result includes:\n- Library ID: Context7-compatible identifier (format: /org/project)\n- Name: Library or package name\n- Description: Short summary\n- Code Snippets: Number of available code examples\n- Trust Score: Authority indicator\n- Versions: List of versions if available. Use one of those versions if and only if the user explicitly provides a version in their query.\n\nFor best results, select libraries based on name match, trust score, snippet coverage, and relevance to your use case.\n\n----------\n\n- Title: LangGraph\n- Context7-compatible library ID: /langchain-ai/langgraph\n- Description: Build resilient language agents as graphs.\n- Code Snippets: 1425\n- Trust Score: 9.2\n- Versions: 0.2.74, 0.4.8\n----------\n- Title: LangGraph4j\n- Context7-compatible library ID: /bsorrentino/langgraph4j\n- Description: 🚀 LangGraph for Java. A library for building stateful, multi-actor applications with LLMs, built for work jointly with langchain4j\n- Code Snippets: 620\n- Trust Score: 10\n----------\n- Title: LangGraphJS\n- Context7-compatible library ID: /langchain-ai/langgraphjs\n- Description: Framework to build resilient language agents as graphs.\n- Code Snippets: 1123\n- Trust Score: 9.2\n----------\n- Title: LangGraph.js API\n- Context7-compatible library ID: /langchain-ai/langgraphjs-api\n- Description: LangGraph.js CLI and in-memory server implementation\n- Code Snippets: 12\n- Trust Score: 9.2\n----------\n- Title: LangGraph Studio\n- Context7-compatible library ID: /langchain-ai/langgraph-studio\n- Description: Desktop app for prototyping and debugging LangGraph applications locally.\n- Code Snippets: 6\n- Trust Score: 9.2\n----------\n- Title: Langgraph Swarm\n- Context7-compatible library ID: /langchain-ai/langgraph-swarm-py\n- Description: \n- Code Snippets: 40\n- Trust Score: 9.2\n----------\n- Title: LangGraph CodeAct\n- Context7-compatible library ID: /langchain-ai/langgraph-codeact\n- Description: \n- Code Snippets: 6\n- Trust Score: 9.2\n----------\n- Title: Langgraph 4 Context 7\n- Context7-compatible library ID: /shak-shat/langgraph4context7\n- Description: Langgraph 4 Context 7 repository\n- Code Snippets: 11964\n- Trust Score: 9\n----------\n- Title: LangGraph.js (llmstxt)\n- Context7-compatible library ID: /langchain-ai.github.io/llmstxt\n- Description: LangGraph.js is an open-source JavaScript/TypeScript framework for building, orchestrating, and deploying AI agent workflows and complex LLM applications via composable graphs, with integrated support for persistence, streaming, human-in-the-loop, and memory.\n- Code Snippets: 2783\n- Trust Score: 7\n----------\n- Title: LangGraph Reflection\n- Context7-compatible library ID: /langchain-ai/langgraph-reflection\n- Description: \n- Code Snippets: 5\n- Trust Score: 9.2\n----------\n- Title: LangGraph MCP Agents\n- Context7-compatible library ID: /teddynote-lab/langgraph-mcp-agents\n- Description: LangGraph-powered ReAct agent with Model Context Protocol (MCP) integration. A Streamlit web interface for dynamically configuring, deploying, and interacting with AI agents capable of accessing various data sources and APIs through MCP tools.\n- Code Snippets: 45\n- Trust Score: 7.3\n----------\n- Title: LangGraph Supervisor\n- Context7-compatible library ID: /langchain-ai/langgraph-supervisor-py\n- Description: \n- Code Snippets: 14\n- Trust Score: 9.2\n----------\n- Title: LangGraph (llmstxt)\n- Context7-compatible library ID: /llmstxt/langchain-ai_github_io-langgraph-llms-full.txt\n- Description: LangGraph is a framework for building, deploying, and managing agentic and multi-agent applications with support for persistence, streaming, memory, and human-in-the-loop workflows.\n- Code Snippets: 2590\n- Trust Score: 8\n----------\n- Title: LangGraph.js Generative UI Examples\n- Context7-compatible library ID: /langchain-ai/langgraphjs-gen-ui-examples\n- Description: A collection of generative UI agents written with LangGraph.js\n- Code Snippets: 11\n- Trust Score: 9.2\n----------\n- Title: Assistant UI LangGraph FastAPI\n- Context7-compatible library ID: /yonom/assistant-ui-langgraph-fastapi\n- Description: \n- Code Snippets: 2\n- Trust Score: 8.5\n----------\n- Title: FastAPI MCP LangGraph Template\n- Context7-compatible library ID: /nicholasgoh/fastapi-mcp-langgraph-template\n- Description: A modern template for agentic orchestration — built for rapid iteration and scalable deployment using highly customizable, community-supported tools like MCP, LangGraph, and more.\n- Code Snippets: 16\n- Trust Score: 8.3\n----------\n- Title: Gemini Fullstack LangGraph Quickstart\n- Context7-compatible library ID: /google-gemini/gemini-fullstack-langgraph-quickstart\n- Description: Get started with building Fullstack Agents using Gemini 2.5 and LangGraph\n- Code Snippets: 12\n- Trust Score: 7.8\n----------\n- Title: FastAPI LangGraph Agent Production Ready Template\n- Context7-compatible library ID: /wassim249/fastapi-langgraph-agent-production-ready-template\n- Description: A production-ready FastAPI template for building AI agent applications with LangGraph integration. This template provides a robust foundation for building scalable, secure, and maintainable AI agent services.\n- Code Snippets: 11\n- Trust Score: 8.8"}],"isError":null}

I'll get LangGraph documentation focusing on error handling, state management, and workflow patterns.


toolName: run_mcp

status: success

mcp: mcp.config.usrlocalmcp.context7 get-library-docs [object Object]
 {"content":[{"type":"text","text":"TITLE: Define GraphState TypedDict for Workflow State Management\nDESCRIPTION: Defines `GraphState` as a `TypedDict` to represent the state of the graph in the code generation workflow. It specifies the types and descriptions for key attributes such as `error` (a binary flag), `messages` (for user questions and error messages), `generation` (the generated code solution), and `iterations` (the number of retry attempts).\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/code_assistant/langgraph_code_assistant.ipynb#_snippet_14\n\nLANGUAGE: APIDOC\nCODE:\n```\nfrom typing import List\nfrom typing_extensions import TypedDict\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        error : Binary flag for control flow to indicate whether test error was tripped\n        messages : With user question, error messages, reasoning\n        generation : Code solution\n        iterations : Number of tries\n    \"\"\"\n\n    error: str\n    messages: List\n    generation: str\n    iterations: int\n```\n\n----------------------------------------\n\nTITLE: Define LangGraph Workflow with Error Handling and Checkpointing\nDESCRIPTION: This Python snippet defines a LangGraph workflow that simulates a task failure and demonstrates how `MemorySaver` can checkpoint the state. It includes `get_info` which fails once, `slow_task` for delay, and `main` as the entrypoint. The initial invocation is expected to fail, showcasing the setup for error recovery.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/use-functional-api.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import StreamWriter\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n@task()\ndef get_info():\n    \"\"\"\n    Simulates a task that fails once before succeeding.\n    Raises an exception on the first attempt, then returns \"OK\" on subsequent tries.\n    \"\"\"\n    global attempts\n    attempts += 1\n\n    if attempts < 2:\n        raise ValueError(\"Failure\")  # Simulate a failure on the first attempt\n    return \"OK\"\n\n# Initialize an in-memory checkpointer for persistence\ncheckpointer = MemorySaver()\n\n@task\ndef slow_task():\n    \"\"\"\n    Simulates a slow-running task by introducing a 1-second delay.\n    \"\"\"\n    time.sleep(1)\n    return \"Ran slow task.\"\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer: StreamWriter):\n    \"\"\"\n    Main workflow function that runs the slow_task and get_info tasks sequentially.\n\n    Parameters:\n    - inputs: Dictionary containing workflow input values.\n    - writer: StreamWriter for streaming custom data.\n\n    The workflow first executes `slow_task` and then attempts to execute `get_info`,\n    which will fail on the first invocation.\n    \"\"\"\n    slow_task_result = slow_task().result()  # Blocking call to slow_task\n    get_info().result()  # Exception will be raised here on the first attempt\n    return slow_task_result\n\n# Workflow execution configuration with a unique thread identifier\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"  # Unique identifier to track workflow execution\n    }\n}\n\n# This invocation will take ~1 second due to the slow_task execution\ntry:\n    # First invocation will raise an exception due to the `get_info` task failing\n    main.invoke({'any_input': 'foobar'}, config=config)\nexcept ValueError:\n    pass  # Handle the failure gracefully\n```\n\n----------------------------------------\n\nTITLE: LangGraph @entrypoint Decorator API Reference\nDESCRIPTION: Documentation for the `@entrypoint` decorator in LangGraph, used to define and manage workflow functions. It encapsulates workflow logic, handles execution flow, long-running tasks, and integrates with interruption mechanisms, often with a `checkpointer` for state management.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/functional_api.md#_snippet_3\n\nLANGUAGE: APIDOC\nCODE:\n```\n@entrypoint(checkpointer: Checkpointer = MemorySaver())\ndef workflow_function(*args, **kwargs) -> Any:\n  \"\"\"\n  Decorator to define the entry point of a LangGraph workflow.\n\n  Args:\n    checkpointer (Checkpointer, optional): An instance of a Checkpointer\n                                           (e.g., MemorySaver) to manage\n                                           workflow state and enable\n                                           resumption. Defaults to MemorySaver().\n\n  Usage:\n    - Encapsulates workflow logic.\n    - Manages execution flow.\n    - Handles long-running tasks and interruptions.\n    - Allows state persistence and resumption via checkpointer.\n  \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Customize Error Handling in LangGraph create_react_agent\nDESCRIPTION: Explains how to override the default error handling in `create_react_agent` by explicitly passing a pre-configured `ToolNode` instance. This allows for custom error messages or specific error management strategies within the agent.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/tool-calling.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ncustom_tool_node = ToolNode(\n    [multiply],\n    handle_tool_errors=\"Cannot use 42 as a first operand!\"\n)\n\nagent_custom = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=custom_tool_node\n)\n\nagent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n```\n\n----------------------------------------\n\nTITLE: Resume LangGraph Workflow Execution After Error\nDESCRIPTION: Illustrates how to resume a LangGraph workflow after an error by invoking the workflow with 'None' as the input, using the same 'thread_id' in the configuration. This assumes the underlying error has been resolved, allowing execution to proceed.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/functional_api.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(None, config)\n```\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nawait my_workflow.ainvoke(None, config)\n```\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nfor chunk in my_workflow.stream(None, config):\n    print(chunk)\n```\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nasync for chunk in my_workflow.astream(None, config):\n    print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Resume LangGraph Workflow from Checkpoint\nDESCRIPTION: This Python snippet demonstrates how to resume a LangGraph workflow after an error. By invoking `main.invoke(None, config=config)`, the workflow continues from its last checkpointed state, avoiding re-execution of already completed tasks like `slow_task`.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/use-functional-api.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmain.invoke(None, config=config)\n```\n\n----------------------------------------\n\nTITLE: Define a LangGraph with concurrent updates to a non-reducible state\nDESCRIPTION: This Python code demonstrates a LangGraph `StateGraph` setup where multiple nodes (`node`, `other_node`) attempt to update the same state key (`some_key`) concurrently without a defined reducer. This configuration can lead to `INVALID_CONCURRENT_GRAPH_UPDATE` errors when executed in parallel, as the graph cannot determine how to merge conflicting updates to `some_key`.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass State(TypedDict):\n    some_key: str\n\ndef node(state: State):\n    return {\"some_key\": \"some_string_value\"}\n\ndef other_node(state: State):\n    return {\"some_key\": \"some_string_value\"}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(node)\nbuilder.add_node(other_node)\nbuilder.add_edge(START, \"node\")\nbuilder.add_edge(START, \"other_node\")\ngraph = builder.compile()\n```\n\n----------------------------------------\n\nTITLE: LangGraph Workflow Custom State and Node Updates\nDESCRIPTION: Illustrates defining a custom state using TypedDict for a LangGraph workflow. It shows how a node function accesses and updates this state, with the return value of the node representing the state modifications to be applied by the graph.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/agents/context.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import StateGraph\n\nclass CustomState(TypedDict):\n    messages: list[AnyMessage]\n    extra_field: int\n\ndef node(state: CustomState):\n    messages = state[\"messages\"]\n    ...\n    return {\n        \"extra_field\": state[\"extra_field\"] + 1\n    }\n\nbuilder = StateGraph(State)\nbuilder.add_node(node)\nbuilder.set_entry_point(\"node\")\ngraph = builder.compile()\n```\n\n----------------------------------------\n\nTITLE: Disable Error Handling in LangGraph's ToolNode\nDESCRIPTION: Demonstrates how to disable the default error handling behavior of `ToolNode` by setting `handle_tool_errors=False`. When disabled, exceptions raised by tools will propagate directly, requiring explicit management in the calling code.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/tool-calling.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ntool_node = ToolNode([multiply], handle_tool_errors=False)\n```\n\n----------------------------------------\n\nTITLE: Python: Construct and Compile LangGraph Workflow with Nodes and Edges\nDESCRIPTION: This section demonstrates how to assemble the LangGraph workflow. It initializes a `StateGraph` with `MessagesState`, adds the previously defined nodes (`agent`, `action`, `ask_human`), and establishes the flow using `add_edge` and `add_conditional_edges`. Finally, the workflow is compiled into a runnable LangChain application, integrating a `MemorySaver` for state persistence and visualizing the graph.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/human_in_the_loop/wait-user-input.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom langgraph.graph import END, StateGraph\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the three nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\nworkflow.add_node(\"ask_human\", ask_human)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n    path_map=[\"ask_human\", \"action\", END],\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"action\", \"agent\")\n\n# After we get back the human response, we go back to the agent\nworkflow.add_edge(\"ask_human\", \"agent\")\n\n# Set up memory\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable\napp = workflow.compile(checkpointer=memory)\n\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n```\n\n----------------------------------------\n\nTITLE: LangGraph State Management with Postgres (Sync and Async)\nDESCRIPTION: Illustrates comprehensive examples of building a LangGraph `StateGraph` with `PostgresStore` and `PostgresSaver` for state persistence and checkpointing. It includes a `call_model` node that interacts with the store to remember user information and demonstrates streaming graph execution in both synchronous and asynchronous contexts.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/memory/add-memory.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom langgraph.store.postgres import PostgresStore\nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\nwith (\n    PostgresStore.from_conn_string(DB_URI) as store,\n    PostgresSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # store.setup()\n    # checkpointer.setup()\n\n    def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        store: BaseStore,\n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n    \n        # Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n    \n        response = model.invoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n    \n    graph = builder.compile(\n        checkpointer=checkpointer,\n        store=store,\n    )\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",\n            \"user_id\": \"1\",\n        }\n    }\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n    \n    config = {\n        \"configurable\": {\n            \"thread_id\": \"2\",\n            \"user_id\": \"1\",\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom langgraph.store.postgres.aio import AsyncPostgresStore\nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\nasync with (\n    AsyncPostgresStore.from_conn_string(DB_URI) as store,\n    AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # await store.setup()\n    # await checkpointer.setup()\n\n    async def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n```\n\n----------------------------------------\n\nTITLE: Initialize and Add Nodes to LangGraph Workflow\nDESCRIPTION: This code snippet demonstrates the basic setup for a LangGraph workflow. It imports necessary components, initializes a StateGraph with a defined state, and then adds the previously defined functions as named nodes to the workflow, establishing the building blocks of the graph.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/rag/langgraph_self_rag.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # generate\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\n```\n\n----------------------------------------\n\nTITLE: Implement Prompt Chaining with LangGraph's Graph API\nDESCRIPTION: This Python code demonstrates how to build a prompt chaining workflow using LangGraph's StateGraph. It defines a state, multiple LLM-based nodes for joke generation and refinement, and a conditional gate to check for a punchline. The workflow explicitly defines edges and conditional transitions, allowing for complex, stateful LLM orchestrations.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/workflows.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom IPython.display import Image, display\n\n\n# Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    improved_joke: str\n    final_joke: str\n\n\n# Nodes\ndef generate_joke(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef check_punchline(state: State):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n\n    # Simple check - does the joke contain \"?\" or \"!\"\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n        return \"Pass\"\n    return \"Fail\"\n\n\ndef improve_joke(state: State):\n    \"\"\"Second LLM call to improve the joke\"\"\"\n\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n    return {\"improved_joke\": msg.content}\n\n\ndef polish_joke(state: State):\n    \"\"\"Third LLM call for final polish\"\"\"\n\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n    return {\"final_joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_joke\", generate_joke)\nworkflow.add_node(\"improve_joke\", improve_joke)\nworkflow.add_node(\"polish_joke\", polish_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_joke\")\nworkflow.add_conditional_edges(\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n)\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\nworkflow.add_edge(\"polish_joke\", END)\n\n# Compile\nchain = workflow.compile()\n\n# Show workflow\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = chain.invoke({\"topic\": \"cats\"})\nprint(\"Initial joke:\")\nprint(state[\"joke\"])\nprint(\"\\n--- --- ---\\n\")\nif \"improved_joke\" in state:\n    print(\"Improved joke:\")\n    print(state[\"improved_joke\"])\n    print(\"\\n--- --- ---\\n\")\n\n    print(\"Final joke:\")\n    print(state[\"final_joke\"])\nelse:\n    print(\"Joke failed quality gate - no punchline detected!\")\n```\n\n----------------------------------------\n\nTITLE: Define and Invoke a Simple LangGraph Workflow with Checkpointing\nDESCRIPTION: This Python example demonstrates how to define a basic `StateGraph` with custom state, add nodes, and connect them. It then compiles the workflow with an `InMemorySaver` checkpointer and invokes the graph, illustrating how state is persisted across steps.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/persistence.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n```\n\n----------------------------------------\n\nTITLE: Define Parallel LLM Workflow using LangGraph's Graph API\nDESCRIPTION: This Python code demonstrates how to build a parallel workflow using LangGraph's StateGraph. It defines a shared state with TypedDict and separate nodes for parallel LLM calls (generating a joke, story, and poem) and an aggregator node to combine their outputs. Edges are added to connect the start node to the parallel LLM calls, and then all LLM calls to the aggregator, finally leading to the end node. This approach provides explicit control over the graph structure.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/workflows.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    story: str\n    poem: str\n    combined_output: str\n\n\n# Nodes\ndef call_llm_1(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef call_llm_2(state: State):\n    \"\"\"Second LLM call to generate story\"\"\"\n\n    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n    return {\"story\": msg.content}\n\n\ndef call_llm_3(state: State):\n    \"\"\"Third LLM call to generate poem\"\"\"\n\n    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n    return {\"poem\": msg.content}\n\n\ndef aggregator(state: State):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n    combined += f\"POEM:\\n{state['poem']}\"\n    return {\"combined_output\": combined}\n\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\nparallel_builder.add_node(\"aggregator\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"call_llm_1\")\nparallel_builder.add_edge(START, \"call_llm_2\")\nparallel_builder.add_edge(START, \"call_llm_3\")\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\nparallel_builder.add_edge(\"aggregator\", END)\nparallel_workflow = parallel_builder.compile()\n\n# Show workflow\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\nprint(state[\"combined_output\"])\n```\n\n----------------------------------------\n\nTITLE: Define LangGraph Workflow for Joke Generation\nDESCRIPTION: Defines the state, initializes an Anthropic LLM, and sets up a simple LangGraph workflow. This workflow includes nodes for generating a joke topic and writing a joke, connected sequentially, and is compiled with an in-memory checkpointer for state management.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/human_in_the_loop/time-travel.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nfrom typing_extensions import TypedDict, NotRequired\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    topic: NotRequired[str]\n    joke: NotRequired[str]\n\n\nllm = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    temperature=0,\n)\n\n\ndef generate_topic(state: State):\n    \"\"\"LLM call to generate a topic for the joke\"\"\"\n    msg = llm.invoke(\"Give me a funny topic for a joke\")\n    return {\"topic\": msg.content}\n\n\ndef write_joke(state: State):\n    \"\"\"LLM call to write a joke based on the topic\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_topic\", generate_topic)\nworkflow.add_node(\"write_joke\", write_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_topic\")\nworkflow.add_edge(\"generate_topic\", \"write_joke\")\nworkflow.add_edge(\"write_joke\", END)\n\n# Compile\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\ngraph\n```\n\n----------------------------------------\n\nTITLE: Default Error Handling with LangGraph's ToolNode\nDESCRIPTION: Shows how LangGraph's `ToolNode` component, by default, catches exceptions raised during tool execution. It then returns these exceptions as `ToolMessage` objects with a 'status' indicating an error, preventing the workflow from crashing.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/tool-calling.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\ndef multiply(a: int, b: int) -> int:\n    if a == 42:\n        raise ValueError(\"The ultimate error\")\n    return a * b\n\n# Default error handling (enabled by default)\ntool_node = ToolNode([multiply])\n\nmessage = AIMessage(\n    content=\"\",\n    tool_calls=[{\n        \"name\": \"multiply\",\n        \"args\": {\"a\": 42, \"b\": 7},\n        \"id\": \"tool_call_id\",\n        \"type\": \"tool_call\"\n    }]\n)\n\nresult = tool_node.invoke({\"messages\": [message]})\n```\n\n----------------------------------------\n\nTITLE: Implement a Custom Conversation Summarization Node in LangGraph Workflow\nDESCRIPTION: This Python function, `summarize_conversation`, acts as a custom node within a LangGraph workflow. It generates a summary of the chat history, leveraging any existing summary as context for new messages. The node dynamically constructs a summarization prompt, invokes a language model, and then updates the state with the new summary while managing message history by deleting older messages.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/memory/add-memory.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n```\n\n----------------------------------------\n\nTITLE: LangGraph Workflow Resumption API and Concepts\nDESCRIPTION: This section details the mechanisms for achieving durable execution in LangGraph workflows, including how to pause and resume using `interrupt` and `Command`, and how to recover from failures by resuming from the last successful checkpoint. It also clarifies the specific starting points for resumption across different LangGraph API types.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/durable_execution.md#_snippet_1\n\nLANGUAGE: APIDOC\nCODE:\n```\nResuming Workflows:\n  Pausing and Resuming Workflows:\n    Function: langgraph.types.interrupt\n    Purpose: Pause a workflow at specific points.\n    Primitive: langgraph.types.Command\n    Purpose: Resume a workflow with updated state.\n    See Also: Human-in-the-Loop documentation.\n  Recovering from Failures:\n    Mechanism: Automatically resume from last successful checkpoint after an exception (e.g., LLM provider outage).\n    Usage: Execute the workflow with the same thread identifier by providing None as the input value.\n    Example: See example with the functional API.\n\nStarting Points for Resuming Workflows:\n  StateGraph (Graph API):\n    Starting Point: Beginning of the node where execution stopped.\n  Subgraph Call inside a Node:\n    Parent Node: The parent node that called the subgraph that was halted.\n    Inside Subgraph: The specific node where execution stopped.\n  Functional API:\n    Starting Point: Beginning of the entrypoint where execution stopped.\n```\n\n----------------------------------------\n\nTITLE: LangGraph Task Invocation Examples\nDESCRIPTION: Demonstrates both synchronous and asynchronous methods for invoking tasks within a LangGraph workflow, highlighting how to retrieve task results using `future.result()` or `await`.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/functional_api.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(some_input: int) -> int:\n    future = slow_computation(some_input)\n    return future.result()  # Wait for the result synchronously\n```\n\nLANGUAGE: python\nCODE:\n```\n@entrypoint(checkpointer=checkpointer)\nasync def my_workflow(some_input: int) -> int:\n    return await slow_computation(some_input)  # Await result asynchronously\n```\n\n----------------------------------------\n\nTITLE: Building a LangChain StateGraph Workflow\nDESCRIPTION: This Python code constructs a LangChain StateGraph, defining its nodes (e.g., `web_search`, `retrieve`, `generate`) and establishing the flow of execution through conditional and unconditional edges. It sets up the routing from the `START` node and defines transitions between various processing steps, culminating in the graph's compilation.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # generate\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\n\n# Build graph\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"web_search\": \"web_search\",\n        \"vectorstore\": \"retrieve\",\n    },\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"transform_query\": \"transform_query\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"transform_query\", \"retrieve\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"transform_query\",\n    },\n)\n\n# Compile\napp = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Trim LLM Messages in a LangGraph Workflow Node\nDESCRIPTION: This example shows how to apply message trimming directly within a custom node function in a LangGraph `StateGraph` workflow. The `trim_messages` utility is called on the `messages` state before invoking the LLM, allowing fine-grained control over the context passed to the model within a specific workflow step. It uses `count_tokens_approximately` and a 'last' strategy.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/memory/add-memory.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages.utils import (\n    trim_messages,\n    count_tokens_approximately\n)\n\ndef call_model(state: MessagesState):\n    messages = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\n```\n\n----------------------------------------\n\nTITLE: Define Custom State for Message Summarization in LangGraph Workflow\nDESCRIPTION: This snippet illustrates how to extend LangGraph's `MessagesState` to include a `summary` key. This custom state schema allows a LangGraph workflow to store and manage the ongoing conversation summary as part of its state.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/memory/add-memory.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\n```\n\n----------------------------------------\n\nTITLE: Define LangGraph StateGraph Workflow\nDESCRIPTION: This snippet defines the core LangGraph workflow, setting up nodes for code generation, checking, and reflection. It establishes the flow using direct edges and conditional edges, allowing the graph to dynamically decide its next step based on the outcome of code checks.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/code_assistant/langgraph_code_assistant.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Define a LangGraph Workflow with Memory Management\nDESCRIPTION: This Python code defines a LangGraph workflow that integrates an Anthropic LLM with user-specific memory management. It includes a `call_model` task for LLM interaction and memory persistence, and an `entrypoint` function that orchestrates message handling, memory retrieval, and response generation. The workflow utilizes `MemorySaver` and a `BaseStore` for state persistence, and requires `langchain-anthropic` and `langgraph` dependencies.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/cross-thread-persistence-functional.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.func import entrypoint, task\nfrom langgraph.graph import add_messages\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.store.base import BaseStore\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n\n@task\ndef call_model(messages: list[BaseMessage], memory_store: BaseStore, user_id: str):\n    namespace = (\"memories\", user_id)\n    last_message = messages[-1]\n    memories = memory_store.search(namespace, query=str(last_message.content))\n    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # Store new memories if the user asks the model to remember\n    if \"remember\" in last_message.content.lower():\n        memory = \"User name is Bob\"\n        memory_store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n    response = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + messages)\n    return response\n\n\n# NOTE: we're passing the store object here when creating a workflow via entrypoint()\n@entrypoint(checkpointer=MemorySaver(), store=in_memory_store)\ndef workflow(\n    inputs: list[BaseMessage],\n    *,\n    previous: list[BaseMessage],\n    config: RunnableConfig,\n    store: BaseStore,\n):\n    user_id = config[\"configurable\"][\"user_id\"]\n    previous = previous or []\n    inputs = add_messages(previous, inputs)\n    response = call_model(inputs, store, user_id).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n```\n\n----------------------------------------\n\nTITLE: Define Asynchronous Checkpointed Tasks with '@task' Decorator\nDESCRIPTION: Demonstrates how to define an asynchronous unit of work using the '@task' decorator in LangGraph. Tasks are designed for concurrent execution and automatically save their results to a checkpoint, requiring their outputs to be JSON-serializable.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/functional_api.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.func import task\n\n@task()\ndef slow_computation(input_value):\n    # Simulate a long-running operation\n    ...\n    return result\n```\n\n----------------------------------------\n\nTITLE: Visualize Langgraph Workflow with Mermaid\nDESCRIPTION: This Python snippet demonstrates how to visualize the compiled Langgraph workflow using `IPython.display` and `graph.get_graph().draw_mermaid_png()`. It helps in understanding the graph's structure visually, with error handling included to gracefully manage cases where optional visualization dependencies might be missing.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n```\n\n----------------------------------------\n\nTITLE: LangGraph ToolNode API Reference\nDESCRIPTION: Detailed API documentation for `ToolNode`, a specialized node in LangGraph for executing tools within workflows. It outlines its features, input/output requirements, and error handling capabilities.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/tool-calling.md#_snippet_5\n\nLANGUAGE: APIDOC\nCODE:\n```\nToolNode:\n  Description: A specialized node for executing tools in a workflow.\n  Features:\n    - Supports both synchronous and asynchronous tools.\n    - Executes multiple tools concurrently.\n    - Handles errors during tool execution (handle_tool_errors=True, enabled by default).\n  Operates on: MessagesState\n  Input: MessagesState, where the last message is an AIMessage containing the tool_calls parameter.\n  Output: MessagesState updated with the resulting ToolMessage from executed tools.\n```\n\n----------------------------------------\n\nTITLE: Asynchronous LangGraph Checkpoint Management with SQLite\nDESCRIPTION: Illustrates the asynchronous usage of `AsyncSqliteSaver` for managing LangGraph checkpoints. It shows how to store, load, and list checkpoints using `await` for non-blocking I/O operations with an in-memory SQLite database.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/libs/checkpoint-sqlite/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n\nasync with AsyncSqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n    checkpoint = {\n        \"v\": 4,\n        \"ts\": \"2024-07-31T20:14:19.804150+00:00\",\n        \"id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\",\n        \"channel_values\": {\n            \"my_key\": \"meow\",\n            \"node\": \"node\"\n        },\n        \"channel_versions\": {\n            \"__start__\": 2,\n            \"my_key\": 3,\n            \"start:node\": 3,\n            \"node\": 3\n        },\n        \"versions_seen\": {\n            \"__input__\": {},\n            \"__start__\": {\n                \"__start__\": 1\n            },\n            \"node\": {\n                \"start:node\": 2\n            }\n        },\n    }\n\n    # store checkpoint\n    await checkpointer.aput(write_config, checkpoint, {}, {})\n\n    # load checkpoint\n    await checkpointer.aget(read_config)\n\n    # list checkpoints\n    [c async for c in checkpointer.alist(read_config)]\n```\n\n----------------------------------------\n\nTITLE: Default Error Handling in LangGraph create_react_agent\nDESCRIPTION: Shows the basic setup and invocation of a `create_react_agent` with default error handling. This example highlights how agents process user queries and implicitly manage tool-related errors.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/tool-calling.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[multiply]\n)\n\n# Default error handling\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n```\n\n----------------------------------------\n\nTITLE: Define a LangGraph state with a reducer for concurrent updates\nDESCRIPTION: This Python code illustrates how to modify a `TypedDict` state definition in LangGraph to handle concurrent updates to a specific key (`some_key`). By using `Annotated[list, operator.add]`, the `some_key` becomes append-only, allowing multiple nodes to contribute to it without causing concurrent update errors. This approach ensures that values from parallel executions are combined rather than overwritten.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport operator\nfrom typing import Annotated\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    some_key: Annotated[list, operator.add]\n```\n\n----------------------------------------\n\nTITLE: Define LangGraph Agent with Dynamic Tool Binding and Selection\nDESCRIPTION: This comprehensive snippet defines the core LangGraph agent workflow. It introduces a `State` TypedDict to manage messages and dynamically selected tool IDs. It includes an `agent` function that binds selected tools to a `ChatOpenAI` LLM and a `select_tools` function that uses the previously initialized `vector_store` to semantically select tools based on the last user message. The graph is constructed with nodes for agent, tool selection, and tool execution, defining conditional edges for a flexible workflow.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/many-tools.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\n\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\n\n# Define the state structure using TypedDict.\n# It includes a list of messages (processed by add_messages)\n# and a list of selected tool IDs.\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n    selected_tools: list[str]\n\n\nbuilder = StateGraph(State)\n\n# Retrieve all available tools from the tool registry.\ntools = list(tool_registry.values())\nllm = ChatOpenAI()\n\n\n# The agent function processes the current state\n# by binding selected tools to the LLM.\ndef agent(state: State):\n    # Map tool IDs to actual tools\n    # based on the state's selected_tools list.\n    selected_tools = [tool_registry[id] for id in state[\"selected_tools\"]]\n    # Bind the selected tools to the LLM for the current interaction.\n    llm_with_tools = llm.bind_tools(selected_tools)\n    # Invoke the LLM with the current messages and return the updated message list.\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n\n# The select_tools function selects tools based on the user's last message content.\ndef select_tools(state: State):\n    last_user_message = state[\"messages\"][-1]\n    query = last_user_message.content\n    tool_documents = vector_store.similarity_search(query)\n    return {\"selected_tools\": [document.id for document in tool_documents]}\n\n\nbuilder.add_node(\"agent\", agent)\nbuilder.add_node(\"select_tools\", select_tools)\n\ntool_node = ToolNode(tools=tools)\nbuilder.add_node(\"tools\", tool_node)\n\nbuilder.add_conditional_edges(\"agent\", tools_condition, path_map=[\"tools\", \"__end__\"])\nbuilder.add_edge(\"tools\", \"agent\")\nbuilder.add_edge(\"select_tools\", \"agent\")\nbuilder.add_edge(START, \"select_tools\")\ngraph = builder.compile()\n```\n\n----------------------------------------\n\nTITLE: langgraph.types Module API Reference\nDESCRIPTION: API documentation for the `langgraph.types` module, detailing its public members which include various classes and enums related to streaming, error handling, caching, and state management within the LangGraph framework.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/reference/types.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\nlanggraph.types Module:\n  Description: Contains core types and classes for the langgraph library.\n  Members:\n    - All\n    - StreamMode\n    - StreamWriter\n    - RetryPolicy\n    - CachePolicy\n    - Interrupt\n    - PregelTask\n    - StateSnapshot\n    - Send\n    - Command\n    - interrupt\n```\n\n----------------------------------------\n\nTITLE: Resume LangGraph Workflow Execution After Interrupt\nDESCRIPTION: Demonstrates how to resume a LangGraph workflow execution after an interrupt by passing a 'Command(resume=some_resume_value)' to the workflow's invocation methods. This requires a 'thread_id' in the configuration to identify the specific thread.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/functional_api.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(Command(resume=some_resume_value), config)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nawait my_workflow.ainvoke(Command(resume=some_resume_value), config)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nfor chunk in my_workflow.stream(Command(resume=some_resume_value), config):\n    print(chunk)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nasync for chunk in my_workflow.astream(Command(resume=some_resume_value), config):\n    print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Define Graph State for Self-RAG Workflow\nDESCRIPTION: Defines the `GraphState` TypedDict, which represents the state passed between nodes in the LangGraph workflow for Self-RAG. It includes fields for the question, LLM generation, and a list of documents.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_pinecone_movies.ipynb#_snippet_14\n\nLANGUAGE: APIDOC\nCODE:\n```\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    documents: List[str]\n```\n\n----------------------------------------\n\nTITLE: Correct Side Effect Handling with LangGraph Tasks\nDESCRIPTION: This example illustrates the correct method for managing side effects in LangGraph by encapsulating them within a `@task` decorated function. By doing so, the side effect is executed only once and its result is cached, ensuring consistent execution and preventing re-execution upon workflow resumption.\nSOURCE: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/concepts/functional_api.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.func import task\n\n@task\ndef write_to_file():\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Side effect executed\")\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    # The side effect is now encapsulated in a task.\n    write_to_file().result()\n    value = interrupt(\"question\")\n    return value\n```"}],"isError":null}

# Code Quality and Maintainability Enhancement Recommendations

Based on the comprehensive documentation analysis using Context7 MCP, here are actionable insights and recommendations for enhancing the `anasakhomach-aicvgen` project:

## 1. **Error Handling & Validation Improvements**

### Pydantic ValidationError Best Practices
- **Structured Error Handling**: Implement comprehensive `ValidationError` catching with detailed error inspection using `e.errors()` method
- **Custom Validators**: Add `@field_validator` decorators for domain-specific validation logic
- **Error Recovery**: Use try-catch blocks around model validation with fallback strategies

```python
try:
    validated_data = CVDataModel(**input_data)
except ValidationError as e:
    # Log detailed errors for debugging
    logger.error(f"Validation errors: {e.errors()}")
    # Implement graceful fallback or user-friendly error messages
    return handle_validation_error(e)
```

### LangGraph Error Handling Patterns
- **State-Based Error Recovery**: Implement error states in your workflow graphs with recovery nodes
- **Checkpointing**: Use `MemorySaver` or `PostgresSaver` for workflow state persistence during errors
- **Custom Error Nodes**: Create dedicated error handling nodes that can retry or redirect workflow execution

## 2. **Async Testing & Mocking Enhancements**

### Pytest Async Best Practices
- **AsyncMock Usage**: Replace `MagicMock` with `AsyncMock` for all async function mocking
- **Fixture Isolation**: Ensure test fixtures are properly isolated to prevent state bleeding
- **Parametrized Testing**: Use `@pytest.mark.parametrize` for testing multiple scenarios

```python
@pytest.mark.asyncio
async def test_async_workflow():
    with patch('module.AsyncService') as mock_service:
        mock_service.return_value.process.return_value = AsyncMock(return_value="success")
        result = await workflow_function()
        assert result == "expected"
```

## 3. **State Management Architecture**

### LangGraph State Patterns
- **TypedDict State Definitions**: Use strongly-typed state classes for better IDE support and validation
- **State Reducers**: Implement proper state reduction functions for concurrent updates
- **Immutable State Updates**: Return new state dictionaries rather than mutating existing state

```python
class WorkflowState(TypedDict):
    current_content_type: ContentType
    error_messages: List[str]
    processing_status: str
    trace_id: str

def error_handler_node(state: WorkflowState) -> WorkflowState:
    return {
        "error_messages": [],  # Clear errors after handling
        "processing_status": "recovered"
    }
```

## 4. **Streamlit UI Robustness**

### Session State Management
- **Error State Persistence**: Store error states in `st.session_state` for user feedback
- **Form Validation**: Use `st.form` with proper validation before processing
- **File Upload Safety**: Implement comprehensive file validation and error handling

```python
if 'error_state' not in st.session_state:
    st.session_state.error_state = None

with st.form("cv_upload_form"):
    uploaded_file = st.file_uploader("Upload CV", type=['pdf', 'docx'])
    if st.form_submit_button("Process"):
        try:
            result = process_cv(uploaded_file)
            st.session_state.error_state = None
        except ValidationError as e:
            st.session_state.error_state = str(e)
            st.error("Please check your input and try again")
```

## 5. **Testing Strategy Enhancements**

### Comprehensive Test Coverage
- **Integration Tests**: Test complete workflow paths, not just individual components
- **Error Path Testing**: Explicitly test error conditions and recovery mechanisms
- **Mock Strategy**: Use dependency injection to make mocking more reliable

### Test Organization
- **Fixture Hierarchies**: Create reusable fixtures for common test setup
- **Test Categories**: Separate unit, integration, and end-to-end tests
- **Performance Tests**: Add tests for workflow execution time and resource usage

## 6. **Monitoring & Observability**

### Structured Logging
- **Trace IDs**: Implement consistent trace ID propagation through workflows
- **Error Context**: Log sufficient context for debugging production issues
- **Performance Metrics**: Track workflow execution times and success rates

### Health Checks
- **Service Dependencies**: Monitor external service availability
- **Workflow Status**: Implement health endpoints for workflow components
- **Resource Monitoring**: Track memory and CPU usage during processing

## 7. **Code Architecture Improvements**

### Dependency Injection
- **Service Abstractions**: Use abstract base classes for services to improve testability
- **Configuration Management**: Centralize configuration with proper validation
- **Plugin Architecture**: Design extensible components for different CV formats

### Error Recovery Patterns
- **Circuit Breaker**: Implement circuit breaker pattern for external service calls
- **Retry Logic**: Add exponential backoff for transient failures
- **Graceful Degradation**: Provide fallback functionality when services are unavailable

## Implementation Priority

1. **High Priority**: Fix async mocking issues and implement proper error state management
2. **Medium Priority**: Add comprehensive integration tests and improve Streamlit error handling
3. **Low Priority**: Implement monitoring and observability features

These recommendations leverage the best practices from the documentation and address the specific challenges identified in your codebase, particularly around error handling, async testing, and workflow state management.
        