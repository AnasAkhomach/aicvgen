"""Streamlit Callback Handler for real-time UI updates during workflow execution.

This module provides the StreamlitCallbackHandler class that integrates with
LangGraph workflows to provide real-time token streaming and progress updates
in the Streamlit UI.
"""

import asyncio
import streamlit as st
from typing import Any, Dict, List, Optional

from src.config.logging_config import get_structured_logger

logger = get_structured_logger(__name__)


class StreamlitCallbackHandler:
    """Callback handler for streaming LLM responses and workflow updates to Streamlit UI.

    This handler provides real-time updates during CV generation workflows,
    including token streaming from LLM responses and agent step progress tracking.
    """

    def __init__(self, container_key: str = "workflow_output"):
        """Initialize the Streamlit callback handler.

        Args:
            container_key: Streamlit session state key for the output container
        """
        self.container_key = container_key
        self.logger = logger

        # Initialize Streamlit containers if not exists
        if self.container_key not in st.session_state:
            st.session_state[self.container_key] = st.empty()

        self.output_container = st.session_state[self.container_key]
        self.current_content = ""

        logger.info("StreamlitCallbackHandler initialized")

    async def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs
    ) -> None:
        """Called when LLM starts generating.

        Args:
            serialized: Serialized LLM configuration
            prompts: List of prompts being processed
            **kwargs: Additional keyword arguments
        """
        logger.debug("LLM generation started", extra={"prompts_count": len(prompts)})

        # Update UI to show LLM is starting
        with self.output_container.container():
            st.info("ğŸ¤– AI is thinking...")
            st.empty()  # Placeholder for streaming content

    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called when a new token is generated by the LLM.

        Args:
            token: The new token generated
            **kwargs: Additional keyword arguments
        """
        # Accumulate tokens
        self.current_content += token

        # Update the UI with streaming content
        with self.output_container.container():
            st.markdown(f"**AI Response:**\n\n{self.current_content}")

    async def on_llm_end(self, response: Any, **kwargs) -> None:
        """Called when LLM finishes generating.

        Args:
            response: The final LLM response
            **kwargs: Additional keyword arguments
        """
        logger.debug("LLM generation completed")

        # Finalize the content display
        with self.output_container.container():
            st.success("âœ… AI response completed")
            st.markdown(f"**Final Response:**\n\n{self.current_content}")

    async def on_llm_error(self, error: Exception, **kwargs) -> None:
        """Called when LLM encounters an error.

        Args:
            error: The error that occurred
            **kwargs: Additional keyword arguments
        """
        logger.error(f"LLM error occurred: {error}")

        with self.output_container.container():
            st.error(f"âŒ AI Error: {str(error)}")

    async def on_agent_start(self, agent_name: str, **kwargs) -> None:
        """Called when an agent starts execution.

        Args:
            agent_name: Name of the agent starting
            **kwargs: Additional keyword arguments
        """
        logger.info(f"Agent started: {agent_name}")

        with self.output_container.container():
            st.info(f"ğŸ”„ Starting {agent_name}...")

    async def on_agent_end(self, agent_name: str, result: Any, **kwargs) -> None:
        """Called when an agent completes execution.

        Args:
            agent_name: Name of the agent that completed
            result: The result from the agent
            **kwargs: Additional keyword arguments
        """
        logger.info(f"Agent completed: {agent_name}")

        with self.output_container.container():
            st.success(f"âœ… Completed {agent_name}")

    async def on_agent_error(self, agent_name: str, error: Exception, **kwargs) -> None:
        """Called when an agent encounters an error.

        Args:
            agent_name: Name of the agent that errored
            error: The error that occurred
            **kwargs: Additional keyword arguments
        """
        logger.error(f"Agent error in {agent_name}: {error}")

        with self.output_container.container():
            st.error(f"âŒ Error in {agent_name}: {str(error)}")

    async def on_workflow_update(self, state_update: Dict[str, Any]) -> None:
        """Called when workflow state is updated.

        Args:
            state_update: The updated workflow state
        """
        logger.debug(
            "Workflow state updated", extra={"keys": list(state_update.keys())}
        )

        # Extract relevant information from state update
        current_stage = state_update.get("current_stage", "Unknown")
        progress = state_update.get("progress", {})

        with self.output_container.container():
            st.info(f"ğŸ“Š Workflow Progress: {current_stage}")

            if progress:
                # Display progress information
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Current Stage", current_stage)
                with col2:
                    completion = progress.get("completion_percentage", 0)
                    st.metric("Progress", f"{completion}%")

                # Progress bar
                st.progress(completion / 100.0)

    async def on_workflow_error(self, error: str) -> None:
        """Called when workflow encounters an error.

        Args:
            error: The error message
        """
        logger.error(f"Workflow error: {error}")

        with self.output_container.container():
            st.error(f"âŒ Workflow Error: {error}")

    def clear_output(self) -> None:
        """Clear the current output display."""
        self.current_content = ""
        self.output_container.empty()
        logger.debug("Output cleared")

    def update_progress(self, stage: str, percentage: float, message: str = "") -> None:
        """Update progress display in the UI.

        Args:
            stage: Current workflow stage
            percentage: Completion percentage (0-100)
            message: Optional progress message
        """
        with self.output_container.container():
            st.subheader(f"Current Stage: {stage}")
            st.progress(percentage / 100.0)

            if message:
                st.info(message)

        logger.debug(
            "Progress updated",
            extra={"stage": stage, "percentage": percentage, "message": message},
        )
